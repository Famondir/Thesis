# Results

## Page identification

As described in \@ref(text-extraction-benchmark) open source libraries have been used to extract the text from the annual reports.

### Baseline: Regex {#regex-page-identification}

```{r page-identification-regex-data-loading, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, cache=TRUE, cache.extra = tools::md5sum('scripts/page_identification_regex.R')}
source("scripts/page_identification_regex.R")
```

Building a sound regular expression often is an iterative process. In a first approach a very simple one was implemented. 

Comparing the differences in the metrics based on the different text extraction libraries it can be said that the extracted text is very similar but not identical. Since the resukts are not depending on the used text extraction library the *exhaustive regex restricted* has only been run with the fast text extraction library *pdfium*. The results of the regex based page identification are presented in the following tables.

* look into details where they differ and if it is because of a line break or whitespace ?

Due to the imbalanced distribution of the classes the accuracy is not a good metric to compare the performance of the different methods. The number of pages of interest is much smaller than the number of irrelevant pages. Therefore, precision, recall and F1 score are presented as well.

The regular expressions can be found in the appendix (see \@ref(regex-page-identification)).

General bad precision. Increasing recall degrades precision even further. number of pages positive identified total; used as subset for table identification task

```{r display-metrics-regex-page-identification-aktiva, echo=FALSE, results="asis"}
metric_summaries["Aktiva"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Aktiva'", ref="display-metrics-regex-page-identification-aktiva")
```

```{r display-metrics-regex-page-identification-passiva, echo=FALSE, results="asis"}
metric_summaries["Passiva"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Passiva'", ref="display-metrics-regex-page-identification-passiva")
```

```{r display-metrics-regex-page-identification-guv, echo=FALSE, results="asis"}
metric_summaries["GuV"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Gewinn- und Verlustrechnung'", ref="display-metrics-regex-page-identification-guv")
```

```{r display-metrics-plot-regex-page-identification, echo=FALSE, figwidth=8, fig.height=6, out.width="100%"}
metrics %>% bind_rows() %>%
  pivot_longer(
    cols = -c(package, method, classification_type),
    names_to = "metric",
    values_to = "value"
  ) %>%
  filter(metric %in% c(
    # "acc", 
    "precision", "recall", "F1")) %>%
  ggplot() +
  geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0) +
  facet_grid(metric~classification_type) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme(
    legend.position = "bottom"
  )
```

Results by company?

### Advanced techniques

#### Table of Contents understanding

```{r page-identification-toc-data-loading, echo=FALSE}
source("scripts/page_identification_toc_data_loading.R")
```

An optional step for larger documents in @li_extracting_2023 framework is to identify the pages of interest based on the table of contents (TOC). This would be more efficent than processing the whole document with an LLM. The TOC in a PDF can be given explicit and machine readable or it can be presented in form of text on any page. Of course it can be missing completely as well.

* For a lot of short annual reports one can find the tables of interest within the first eight pages as well.

* calculate and add Qwen, Gemini or LLama results? <-- No time!

##### Text based

@li_extracting_2023 used the table of contents to identify the pages of interest. In their approach the table of contents is extracted from the text. Based on their observation, that the TOC that "ACFRs typically spans no more than the initial 165 lines of the converted document" (p. 20), they use the first 200 lines of text.

My expectation was to find the TOC within the first five pages. Often we find way less than 200 lines of text on the five first pages (see Figure \@ref(fig:page-identification-toc-histogram)). Some files are not machine readable without OCR and thus show zero lines in the first five pages as well.

```{r page-identification-toc-histogram, echo=FALSE, fig.width=8, fig.height=3, out.width="80%", fig.cap="Histogram of the number of lines in the first 5 pages of the annual reports"}
tibble(num_lines = num_lines) %>% ggplot() +
  geom_histogram(aes(x = num_lines), bins = 20) +
  labs(x = "Number of lines in the first 5 pages", y = "Count")
```

###### First five pages

A request to Mistral results in `r n_found_toc_5_pages` strings that should represent a table of contents among the first five pages [strings not checked in detail].

###### First 200 lines

A request to Mistral results in `r n_found_toc_200_lines` strings that should represent a table of contents among the first five pages [strings not checked in detail].



###### Machine readable TOC based

To limit the text and hopefully increase the quality of the input data one can work with the TOC representation embedded within the PDF files. From `r n_toc+n_no_toc` annual reports `r n_toc` files do have a machine readable separate table of contents and `r n_no_toc` do not have one.

One can see that correct predictions for the page range are more probable when the TOC has a medium number of entries. It is possible to drop PDFs with less than `r min_n_entries_max_correct` without loosing a single correct prediction. This means that for PDFs with TOC with less then `r min_n_entries_max_correct` entieres the LLM was not able to make a correct prediction. This is not surprising since neither *Bilanz* nor *Gewinn- und Verlustrechnung* are mentioned there.

Almost no influence if TOC is passed formated as markdown or json. With the json formated TOC it found two more correct page ranges (single test run). It was testes because the relation *page_number* heading and value might have been clearer in json for a linear working LLM.

```{r page-identification-toc-mr-degration, echo=FALSE, results="asis"}
df_toc_benchmark_mr_degration %>% ggplot() +
  geom_col(aes(x = n_entries, y = value, fill = correct)) +
  geom_line(aes(x = as.numeric(n_entries), y = 100*perc_correct, group = 1)) +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  facet_wrap(~type, ncol = 1)
```

##### Comparison of the different approaches

* toc analysis
* cleaned measures

The LLM performed best on the machine readable TOC. It resulted in highest ratio of correct page ranges as well as in highest absolute numbers even though there were least available TOC.

```{r page-identification-toc-analysis, echo=FALSE, results="asis", out.width="100%", fig.cap="Comparing number of fount TOC and amount of correct and incorrect predicted page ranges"}
df_toc_benchmark %>% ggplot() +
  geom_bar(aes(x = type, fill = in_range, colour = min_distance <= 1)) +
  geom_text(
    data = df_toc_benchmark %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1) # +
  # theme(
  #   legend.position = "bottom"
  # )
```

Values can be higher than 80, the total number of PDF files, since there can be multiple tables of interested for the same type in a single document or a table of interest can span two pages. Since the prompt for the LLM was not elaborated enough to cover cases, where there are multiple tables of interest for a single type that are not placed on concurrent pages, one could argue to drop those files from the analysis. This does not change the results significantly, since there are only few files with more than one table of interest per type.

```{r page-identification-toc-analysis-cleaned, echo=FALSE, results="asis"}
df_toc_benchmark_cleaned <- df_toc_benchmark %>% group_by(filepath, benchmark_type) %>%
  mutate(count = n()) %>% filter(count <= 4) %>% group_by(type, benchmark_type) %>% 
  mutate(
    perc_correct = sum(in_range)/n(),
  )

 df_toc_benchmark_cleaned %>% 
  ggplot() +
  geom_bar(aes(x = type, fill = in_range)) +
  geom_text(
    data = df_toc_benchmark_cleaned %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1)
```

Besides a single group that was predicted far off for the machine readable TOC approach the LLM reported higher confidence for the correct page ranges and got the ranges less far off. But it did not predict the smallest ranges.

```{r}
mean_ranges %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == min(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(`mean range`) %>%
  render_table()
```

```{r page-identification-toc-range_logprobs, echo=FALSE, results="asis", out.width="100%"}
df_toc_benchmark %>% group_by(filepath, benchmark_type) %>%
  distance_confidence_plot() +
  facet_wrap(~benchmark_type, nrow = 1) +
  theme(
    legend.position = "bottom"
  )
```

In general the LLM performed worst to identify the correct page range for *Passiva*. The median distance is one page bigger than for *Aktiva* and *Gewinn- und Verlustrechnung*. This makes sense for *Aktiva* because the *Passiva* is often on the next page but the predicted page range for *Aktiva* and *Passiva* are often identical. Furthermore the predicted page range for *Aktiva* is often only a single page wide. Thus the *Passiva* on the next page is not inside the predicted page range.

This problem was solved by explicitly mentioning that assets and liabilities are both part of the balance sheets for the five pages and 200 lines approach but not for the machine readable TOC one.

```{r page-identification-toc-analysis-balanced, echo=FALSE, results="asis", out.width="100%", fig.cap="Comparing number of fount TOC and amount of correct and incorrect predicted page ranges"}
balanced_df_toc_benchmark %>% ggplot() +
  geom_bar(aes(x = type, fill = in_range, colour = min_distance <= 1)) +
  geom_text(
    data = balanced_df_toc_benchmark %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1) # +
  # theme(
  #   legend.position = "bottom"
  # )
```

A pragmatic way would be to use the machine readable TOC approachs prediction for the *Aktiva* page range and add one to the end page to get the *Passiva* page range. Beside the problem to predict a correct page range for *Passiva* the machine readable TOC approach was very effective and is also pretty efficient if one counts in the effort the LLM driven TOC extraction takes.

```{r page-identification-toc-gpu-time-comparison, echo=FALSE, results="asis"}
gpu_time_per_document_page_range %>% mutate_if(
  is.numeric, 
  ~ifelse(
    . == min(., na.rm = TRUE),
    paste0("**", ., "**"),
    .
  )
) %>% 
  render_table(alignment="lrr", caption="Comparing GPU time for page range prediction and table of contents extraction", ref="page-identification-toc-gpu-time-comparison")
```

#### Classification with LLMs {#llm-page-identification}

```{r page-identification-llm-data-loading, echo=FALSE}
temp_list <- readRDS("data_storage/page_identification_llm.rds")
df_binary <- temp_list$df_binary
df_multi <- temp_list$df_multi

method_families <- c("zero_shot", "law_context", "top_n_rag_examples", "n_random_examples", 'n_rag_examples')
method_familiy_colors <- c(
  "zero_shot" = "#e41a1c", 
  "law_context" = "#377eb8", 
  "top_n_rag_examples" = "#4daf4a", 
  "n_random_examples" = "#984ea3", 
  'n_rag_examples' = "#ff7f00"
  )

df_binary <- df_binary %>% filter(!str_detect(model, "gemma") || str_detect(model, "0-9-1")) %>% 
  mutate(
  n_examples = as.numeric(n_examples),
  n_examples = if_else(method_family == "zero_shot", 0, n_examples),
  n_examples = if_else(method_family == "law_context", 1, n_examples),
  method_family = factor(method_family, levels = method_families)
)
```

```{r, echo=FALSE}
binary_task <- list()
binary_task$n_models <- df_binary$model %>% unique() %>% length()
binary_task$n_model_families <- df_binary$model_family %>% unique() %>% length()
binary_task$n_method_families <- df_binary$method_family %>% unique() %>% length()
```

structured outputs forcing to answer with a *yes* or *no* for binary task or with *Aktiva*, *Passiva*, *GuV* or *other* for multi classification task

##### Binary classification

Could be more efficient to predict "is any of interest" and then which type, because dataset is highly imbalanced.

`r  binary_task$n_models` models from `r binary_task$n_model_families` haven been benchmarked among `r binary_task$n_method_families` methods

Most models have been used up to 3 examples for the context.

The best combination of model and method for each method family is presented in the following table. It is clear that the Google Gemma models are performing worst.^[This is not due to a temporary technical problems caused by a bug in the transformers version shipped with the vllm 0-9-2 image. Those problems have been overcome. The performance stays bad.] Surprisingly Mistral 2410 is the best performing model for all three prediction tasks even though it only has 8B parameters.

```{r display-metrics-llm-page-identification-binary, echo=FALSE, results="asis"}
df_binary %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, classification_type) %>% 
  filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
  # mutate(
  #   n = n()
  # ) %>% filter(n > 1)
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, classification_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  ) %>% 
  render_table()
```

It is interesting that the predictions do not get better by providing more and more examples. Especially for the n-rag-example approach we find a significant drop in the F1 score if the examples pages come from different companies annual reports. This is caused by a  sever recall drop. But also for the n-ranom-example approach we see this for the prediction of class Passiva.

Recall better with examples from same company. Precision better without.

We can also see that the prediction performance is stable.^[Earlier experiments on a subset of the pages have been run five times indicating stable results. Running the experiments up to tree times in this very task indicate this as well.]

```{r, echo=FALSE, out.width="100%"}
df_binary %>% filter(model == "mistralai_Ministral-8B-Instruct-2410", loop < 3) %>% 
  filter(n_examples <= 5 | is.na(n_examples)) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  geom_text(aes(label = n_examples)) +
  scale_color_manual(values = method_familiy_colors) +
  facet_grid(model~classification_type) +
  # theme(legend.position = "bottom") +
  guides(
    color = guide_legend(ncol = 1, title.position = "top"),
    shape = guide_legend(ncol = 1, title.position = "top")
  )
```

* f1
* multiple models
* best model detail (different methods / settings)

The experiments for the best performing model, Ministral-8B-Instruct-2410, have been extended by methods with even more examples. Especially for the top-n-rag-example approach to get a better comparable picture based on the real number of examples / context length.

```{r, echo=FALSE, out.width="100%"}
df_binary %>% filter(model == "mistralai_Ministral-8B-Instruct-2410", loop < 2) %>% 
  filter(n_examples > 1 | is.na(n_examples)) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  scale_color_manual(values = method_familiy_colors) +
  geom_text(aes(label = n_examples)) +
  facet_grid(model~classification_type) +
  # theme(legend.position = "bottom") +
  guides(
    color = guide_legend(ncol = 1, title.position = "top"),
    shape = guide_legend(ncol = 1, title.position = "top")
  )
```

```{r, echo=FALSE}
read_csv("../benchmark_truth/real_example_count.csv") %>% kbl()
```

Predictions very accurate. Confidence not always 1. Wrong predictions with often with medium confidence. If Aktiva and Passiva on same page more often Aktiva predicted.

```{r, echo=FALSE, out.width="100%"}
df_filtered <- df_binary %>% filter(classification_type == "Aktiva") %>% 
  arrange(desc(f1_score))
df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
df_flipped_score <- df_temp %>% 
  mutate(
    confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
    is_aktiva = str_detect(type, "Aktiva")
  )
model_name_best_f1_aktiva <- df_filtered[1, "model"]
method__best_f1_aktiva <- df_filtered[1, "method"]

df_flipped_score %>% 
  ggplot() +
  geom_boxplot(aes(x = predicted_type, y = confidence_score)) +
  geom_jitter(aes(x = predicted_type, y = confidence_score, color = match), alpha = .3) +
  facet_wrap(~type) +
  labs(title = model_name_best_f1_aktiva,
       subtitle = method__best_f1_aktiva)
```

Qwen returns always high confidence even if it is wrong.

```{r, echo=FALSE, out.width="100%"}
df_filtered <- df_binary %>% filter(classification_type == "Aktiva", model_family=="Qwen") %>% 
  arrange(desc(f1_score))
df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
df_flipped_score <- df_temp %>% 
  mutate(
    confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
    is_aktiva = str_detect(type, "Aktiva")
  )
model_name_best_f1_aktiva <- df_filtered[1, "model"]
method__best_f1_aktiva <- df_filtered[1, "method"]

df_flipped_score %>% 
  ggplot() +
  geom_boxplot(aes(x = predicted_type, y = confidence_score)) +
  geom_jitter(aes(x = predicted_type, y = confidence_score, color = match), alpha = .3) +
  facet_wrap(~type) +
  labs(title = model_name_best_f1_aktiva,
       subtitle = method__best_f1_aktiva)
```

* IBB other law
* degewo only one where no ocr is needed

mistral: recall IBB and Netzholding big range
meta & mistral: very high precision for Amt f√ºr Statistik BBB <- lowest average pagecount (29.3) but IBB has more pages than berlinovo but better precision. No information about which company / report the page is from

```{r, echo=FALSE, out.width="100%"}
no_ocr_needed <- read_csv("../benchmark_truth/aktiva_passiva_guv_table_pages_no_ocr.csv") %>% select(filepath) %>% 
  unique() %>% mutate(filepath = str_replace(filepath, "..", "/pvc")) %>% .[[1]]

l <- list()

for (t in c('Aktiva', 'Passiva', 'GuV')) {
  df_filtered <- df_binary %>% filter(classification_type == t) %>% 
    arrange(desc(f1_score)) %>% select(model, method, predictions, model_family, method_family)
  df_temp <- df_filtered %>% unnest(predictions) %>% filter(filepath %in% no_ocr_needed)
  
  df_f1_by_company <- df_temp %>% group_by(company, predicted_type, match, model,model_family, method, method_family) %>% reframe(
    n = n()
  ) %>% complete(company, predicted_type, match, model,model_family, method, method_family,fill=list(n=0)) %>% 
    mutate(
      metric = if_else(predicted_type == t & match, "true_positive", ''),
      metric = if_else(predicted_type == t & !match, "false_positive", metric),
      metric = if_else(predicted_type != t & !match, "false_negative", metric),
      metric = if_else(predicted_type != t & match, "true_negative", metric),
    ) %>% select(-predicted_type, -match) %>% 
    pivot_wider(names_from = metric, values_from = n) %>% 
    mutate(
      precision = true_positive/(true_positive+false_positive),
      recall = true_positive/(true_positive+false_negative),
      f1_score = 2*precision*recall/(precision+recall),
      classification_type = t
    )
  
  l[t] <- list(df_f1_by_company)
}

df_f1_by_company <- bind_rows(l)

df_f1_by_company %>% ggplot() +
  geom_boxplot(aes(x = company, y = f1_score)) +
  # geom_jitter(aes(x = company, y = f1_score, color = model), alpha = .4) +
  facet_grid(classification_type~model_family) +
  scale_x_discrete(guide = guide_axis(angle = 30))
```

```{r}
n_reports_by_company_no_ocr <- df_temp %>% select(company, filepath) %>% unique() %>% group_by(company) %>% reframe(n = n())
n_reports_by_company <- df_filtered %>% unnest(predictions) %>% select(company, filepath) %>% unique() %>% group_by(company) %>% reframe(n = n())

n_reports_by_company_no_ocr %>% kbl()
n_reports_by_company %>% kbl()
```

##### Multi classification

Microsoft phi 4 and Falcon 3 only ran with one example because their context window is smaller.

* f1
* multiple models
* best model detail (different methods / settings)

#### Term frequency based classifier

* top 1
* top k

low precision llm linked to position of correct page? numeric frequency?

### Comparison

Multiclassification more effective than three times single classification

#### F1

#### Energy usage and runtime

## Table extraction

### Baseline: Regex

### Extraction with LLMs

* Google models again start underperforming without changes in task (neither data nor prompt?!) even though it was done with vllm already before

