---
editor_options: 
  markdown: 
    wrap: 72
---

# Results {#results}

<!-- \addthumb{Chapter~\thechapter.1}{\textbf{\Huge{\thechapter}\Large{.1}}}{white}{gray} -->
\ChapFrame

This chapter presents the results for the two main and our side research
questions of this thesis:

```{r, include=knitr::is_html_output(), results='asis', echo=FALSE}
cat("<ol class='rs-questions' style='--counter: 1;'><li>How can we use LLMs effectively to locate specific information in a financial report?</li><li>How can we use LLMs effectively to extract these information from the document?</li><li>Can we use additional information from the extraction process, to guide the user which values need to be checked and which can be trusted as they are?</li></ol>")
```

\begin{enumerate}[label={\textbf{Q\theenumi}}]
  \item How can we use LLMs effectively to locate specific information in a financial report?
  \item How can we use LLMs effectively to extract these information from the document?
  \item Can we use additional information from the extraction process, to guide the user which values need to be checked and which can be trusted as they are?
\end{enumerate}

Section \@ref(page-identification-introduction) presents the results for
the first research question. Section
\@ref(table-extraction-introduction) presents the results for the second
question. Section \@ref(error-rate-guidance-results) shows the results
for the side research question. Finally, we will summarize all results
in section \@ref(summary-results).

Each section will start with an overview about the specific sub tasks as
well about the models, methods and data used to investigate the research
question. We refer to the sections in the appendix that document our
investigation and the results of the sub tasks in detail.

## Page identification {#page-identification-introduction}

(ref:page-identification-intro-text) The first research question asks, how \acr{LLM}s can be used, to effectively locate specific information in a financial report. The task
for this thesis is identifying the pages where the balance sheet
(*Bilanz*) and the profit-and-loss-and-statement (*Gewinn- und
Verlustrechnung, GuV*) are located. The balance sheet is composed of two
tables showing the assets (*Aktiva*) and liabilities (*Passiva*) of a
company. Often these two tables are on separate pages. Hereafter, the
German terms **Aktiva**, **Passiva** and **GuV** will be used.

(ref:page-identification-intro-text)

@liExtractingFinancialData2023 describes two ways to identify the
relevant pages (see Figure \@ref(fig:extraction-framework-flow-chart)).
For longer documents they propose to use the \acr{TOC} to determine a
page range that includes the information of interest. In addition, they
develop target specific regular expressions and rules to filter out
irrelevant pages[^05_results-1]. The result of this "Page Range
Refinement" is then passed to the \acr{LLM} to extract information from.

[^05_results-1]: Personal opinion: Developing well performing regular
    expressions can be a very tedious and setting appropriate rules
    requires some domain knowledge. It can be worth the effort if there
    are a lot of documents with similar information to extract. For this
    thesis it took multiple months. At least, now there is kind of a
    pipeline one can reuse, exchanging the rules and key word lists.
    Thus the next similar task should be solved faster.

This section is presenting the results of four approaches to identify
the page[^05_results-2] of interest:

[^05_results-2]: In some cases the information of interest is spanning
    two pages. These rare cases are not covered from the approaches
    presented here, yet.

* Subsection \@ref(regex-page-identification) presents the findings
    of a page range refinement using a list of key words with a regular
    expression.
* Subsection \@ref(toc-understanding) presents the findings of a
    \acr{TOC} understanding approach
* Subsection \@ref(llm-page-identification) presents the findings
    of a text classification approach using \acr{LLM}s.
* Subsection \@ref(tf-classifier) presents the findings of a
    term-frequency approach.



In subsection \@ref(comparison-page-identification) the results get
compared and summarized. Subsection \@ref() proposes an efficient
combination of approaches to solve the task of this thesis and discusses
its limitations.

###  {.unlisted .unnumbered}

Woanders hin oder weg:

* Thus, we prompt the \acr{LLM} to classify if the text extract of a given
page
* for implementation: As described in \@ref(text-extraction-benchmark)
open source libraries have been used to extract the text from the annual
reports.

::: paragraph-start
##### Dataset describtion

For the page identification task companies (mainly) from the first row of Figure \@ref(fig:beteiligungsunternehmen) have been selected, to build the ground truth from. The idea is that the documents of a companies within a category are more similar to each other, than to documents of companies of other categories. For the chosen companies all available annual reports are selected. Since one of the companies mainly published documents that require \acr{OCR} preprocessing, we include the documents of a second company for this category.
:::

### Approaches

The page identification task is broken down to a classification task for all of the approaches presented in this section but the \acr{TOC} understanding approach. This subsection briefly describes our approaches. A detailed report can be found in chapter \@ref(page-identification-report).

::: paragraph-start
##### Regular expressions

We develop multiple sets of regular expressions and filter out all pages that do not fulfill all regular expressions of a given set. There are different sets for each target type, **Aktiva**, **Passiva** and **GuV**. The sets also differ in how versatile they can cope with additional white space introduced by a imperfect text extraction and how many different words for a given term are accepted. Figure \@ref(fig:regex-filter) shows an example for two sets of regular expressions to identify a **Aktiva** page.
:::

```{r regex-filter, fig.cap="Comparing the prediction of two different sets of regular expressions on dummy pages. The simple one has a lower recall, while the  expended one has a lower precision.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/regex_filtering.png")
```

::: paragraph-start
##### Table of Contents Understanding

We use a \acr{LLM} to extract the \acr{TOC} from the first pages from a document or use the embedded \acr{TOC} and prompt a \acr{LLM} to identify the pages where the **Aktiva**, **Passiva** and **GuV** are located. Figure \@ref(fig:toc-screenshot) shows a screenshot of a annual report with an embedded TOC and its TOC in text form.
:::

```{r toc-screenshot, fig.cap="Showing a screenshot of a annual report with an embedded TOC (left) and its TOC in text form (right). The embeded TOC is not listing all entries from the TOC in text form.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/toc.png")
```

::: paragraph-start
##### Large Language Model Classification

We use \acr{LLM}s to classify if the text extract of a given page is containing a **Aktiva**, **Passiva** or **GuV** table or something else. We test binary classification and a multi-classification approach. The reported confidence scores can be used to form a ranking, which text extract might be most similar to the target type.
:::

We test a wide range of open-weight models and compare different prompting techniques. Figure \@ref(fig:prompt-setup-classification) shows, how the prompts are composed for the different strategies. Besides a zero shot approach we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing the legal text instead of examples from a annual report. 

```{r prompt-setup-classification, fig.cap="Showing the basic structure of the prompts and which strategies are used to pass additional information to the LLM.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/promt_building_classification.png")
```

::: paragraph-start
##### Term frequency Ranking

We use normalized term frequencies and normalized float frequency to as features for a classification using a random forest. The predicted scores are used to build a ranking, which page most probably contains the target pages. Undersampling is used during training, to handle the unbalanced data. Figure \@ref(fig:tf-flowchart) visualizes, how the prediction works in this approach.
:::

```{r tf-flowchart, fig.cap="Visualizing, how term and float frequency get calculated and used to predict, if a page is of the target class.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/tf_flowchart.png")
```

### Comparison {#comparison-page-identification}

```{r load-page-identification-comapison-data, echo=echo_flag, warning=warning_flag, message=message_flag}
df_page_identification_comparing_performance <- readODS::read_ods("data_storage/page_identification_summary.ods", sheet = 1)
df_page_identification_comparing_efficiency <- readODS::read_ods("data_storage/page_identification_summary.ods", sheet = 2)

df_df_page_identification_comparing_performance_formatted <- df_page_identification_comparing_performance %>% 
  group_by(type) %>% 
  mutate(
    across(
      c(precision, recall, F1, `top 1 recall`),
      ~ifelse(      
        . == max(., na.rm = TRUE),
        paste0("**", round(., 3), "**"),
        round(., 3)
        )
      )) %>% 
  mutate(`k for full recall` = ifelse(      
        `k for full recall` == min(`k for full recall`, na.rm = TRUE),
        paste0("**", round(`k for full recall`, 3), "**"),
        round(`k for full recall`, 3)
        ))
```

This subsection presents the performance and efficiency for all
four presented approaches and compare it with the the results a human
achieves manually

::: paragraph-start
##### Prediction performance

Table \@ref(tab:page-identification-performance-overview-all) shows the
best performance achieved by the four presented approaches regarding precision, recall and F1 score.

```{r page-identification-performance-overview-all, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_df_page_identification_comparing_performance_formatted %>% 
  select(-c(`top 1 recall`, `k for full recall`)) %>% 
  arrange(type) %>% 
  render_table(
    alignment = "lllrrr",
    caption = "Comparing page identification perfromance among all four approaches.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

The best
F1 score is reached by Llama 4 Scout for the target types **Akiva** and
**Passiva** in the multi-class classification approach. For **GuV** the
best F1 score (0.985) is found with Ministral-8B-Instruct in the binary
classification approach. Llama 4 Scout reaches a F1 score of 0.971 for
target type **GuV** and multi-class classification.
:::

In the dataset preparation for the table extraction task (see section
\@ref(table-extraction-introduction) 107 **Aktiva** pages have been
selected. In this manual process we made two mistakes, accidently
selecting one **Passiva** and one **GuV** page. Thus the human baseline
to compete with is 0.981. Thus, Llama 4 Scout is more precise than us.

Furthermore, Llama 4 Scout reached a recall of 1.0 for all target types.
This means, the results can be used downstream, even though the
precision is not always perfect. The pages classified as target can be
double checked by a human, without missing any page.

The other approaches' performance is way worse.

Only the term-frequency approach's results could be used downstream, because we find a recall of 1.0. Table \@ref(tab:page-identification-performance-overview-top-k-recall) shows the results of the top k recall for the term-frequency and \acr{LLM}
approaches. The \acr{LLM}s always rate the correct **GuV** page highest.
With Llama Scout 4 we find all target pages within the first two ranked
pages. For the term-frequency approach a human sometimes has to check up
to five pages.

```{r page-identification-performance-overview-top-k-recall, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_df_page_identification_comparing_performance_formatted %>% 
  select(-c(precision, recall, F1)) %>% 
  filter(!is.na(`top 1 recall`)) %>% 
  arrange(type) %>% 
  render_table(
    alignment = "lllrr",
    caption = "Comparing the top k recall for the term-frequency and LLM approaches.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

::: paragraph-start
##### Energy usage and runtime

Table \@ref(tab:page-identification-efficiency-overview-all) shows the
runtime in seconds per document, estimated energy consumption in Joule
per document and costs in **CENTS per 1000 documents**. The runtime for
the \acr{LLM}s was normalized to seconds on a nvidia B200 and thus the
TDP of 700 W is used to calculate the energy consumption. For the other
approaches, running on my laptop (see section \@ref(local-machine)) a
TDP of 28 Watts is used. For manual work by a human additional 60 W are
added for the screen used. It is assumed that the \acr{LLM} is hosted
locally.
:::

```{r page-identification-efficiency-overview-all, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_page_identification_comparing_efficiency %>% filter(!drop == 1) %>% 
  select(approach, strategy, `runtime per document in s`, `energy in J`, `costs in CENTS per 1000 documents`) %>%
  mutate(
    across(
      where(is.numeric),
      ~ifelse(      
        . == min(., na.rm = TRUE),
        paste0("**", round(., 3), "**"),
        round(., 3)
        )
      )) %>% 
  render_table(
    alignment = "llrrr",
    caption = "Comparing page identification efficiency among all four approaches.",
    ref = opts_current$get("label"))
```

Table \@ref(tab:page-identification-efficiency-overview-all) shows, that
the regular expression approach is fastest and consumes least energy.
Nevertheless, since the results are not sufficient another approach has
to be chosen if the amount of manual labor should be reduced for the
human inn the loop.

Second place regarding all these criteria is the term-frequency
approach, which guarantees a perfect recall, while reducing the number
of pages to investigate to five per target type. This is similar to the
number of pages a human has to investigate to find the \acr{TOC} of the
document. And it is a reduction to 7.4 % of the average 67 pages the
documents in this dataset have. The costs are still negligible.

The \acr{LLM} approaches have the highest runtime and energy
consumption. This is the case, because they process every page with very
computational demanding algorithms. The fastest and least energy
consuming strategy is to use a small model as Ministral-8B-Instruct for
the multi-class approach. This is more effective than running three
binary classifications.

An alternative approach could be to binary
predict if the page is of any target type and then perform a
classification, which type exactly the page is of. But the results of
the multi-class strategy are good enough as well. In both strategies the
k required for perfect recall is three, using the Ministral-8B-Instruct
model[^05_results-23].

[^05_results-23]: Potentially smaller fine tuned models can solve the
    task even more efficient.

For the \acr{TOC} approach \acr{LLM}s are used as well, but they process
far less of the documents pages. This can be achieved for the good
performing \acr{LLM} classification strategy too, by combining the term-frequency
approach with the \acr{LLM} approach.

::: paragraph-start
###### Compare with manual page identification

The manual approach is the slowest. For the benchmark ten random
documents were processed by us. We used the \acr{TOC} and the search
function to find key words like **Aktiva** or **Bilanz**. Anyhow, its
almost as fast as the multi-classification using Llama 4 Scout, while
consuming eight times less energy. Comparing it to Ministral-8B-Instruct
it take three times longer but consumes less then half of the energy.
:::

Not taken into account fo this comparison are factors like

-   costs to buy and maintain hardware (i.e. a GPU cluster).

-   higher costs per runtime if the \acr{LLM} compute is purchased from
    cloud providers. CLOUD: price if LLM is in the cloud \<-- print
    tokens used

    -   four classes (3 random examples): 11 k tokens

    -   binary (3 random examples): 6.5 k tokens

-   payment and insurance to pay for a human (e.g . student coworker).

-   the training time and energy consumption for training either

    -   a \acr{LLM} (probably done by the \acr{LLM} provider).

    -   a human (growing up, getting educated).

-   the energy consumed with the food humans eat.

Since all approaches but the manual identification need the text
extract, this runtime and energy consumption are also not listed (but
low).

## Information extraction {#table-extraction-introduction}

<!-- \addthumb{Chapter~\thechapter.2}{\textbf{\Huge{\thechapter}\Large{.2}}}{white}{gray} -->

(ref:information-extraction-intro-text) The second research question asks, how \acr{LLM}s can be used, to
effectively extract specific information from a financial report. The
task for this thesis is to extract the numeric values for the assets
(*Aktiva*) table, which is part of the balance sheet (*Bilanz*).
Hereafter, the German term **Aktiva** will be used. We are limiting the
scope even further than in subsection
\@ref(page-identification-introduction), because it takes more time to
manually create the first reference dataset.

(ref:information-extraction-intro-text)

###  {.unlisted .unnumbered}

::: paragraph-start
##### Structured output

We are using a strict schema for the extraction process that is derived
from the legal text [@hgbHandelsgesetzbuchImBundesgesetzblatt2025,
§266]. Actually, there are three types of verbosity, that are defined in
the law. Smaller companies are permitted to create less detailed balance
sheets. Our schema is created based on the most detailed level. This is
the form most often found in the document base[^05_results-24].
:::

[^05_results-24]: Unfortunately, well known companies as BVG and BSR
    publish a less detailed form. Thus, their documents are not included
    in the document base for this task.

Using a strict schema has advantages for processing the
results in downstream tasks - i.e. for adding the results to a
relational database. It is also easier to compare the results with a
ground truth if the names of all rows and their order is fixed. The
schema is defined as \acr{ebnf}-grammar and passed as an argument to
\acr{vllm}.

::: paragraph-start
##### Gound truth dataset

For the information extraction task two datasets are used. First, a collection of 107 real **Aktiva** tables is created, going through two sampling iterations. In the first iteration a single report is selected for each company. In addition, all available reports from the first listed company are chosen, to test an in-company learning approach. In the second iteration more reports of the other companies are added, to increase the ground truth size and allow to test the in-company approach for all companies.
:::

Second, a dataset of 16_504 synthetic **Aktiva** tables is created. These tables are generated based on the extraction schema and filled with random numeric values. Different table characteristics are systematically combined, to investigate potential effects of these features on the extraction performance. The tables are created as \acr{PDF}, \acr{HTML} and Markdown files each. This dataset allows to estimate the extraction performance, if there are no unknown row identifiers present.

### Approaches

::: paragraph-start
##### Regular expressions

We use regular expressions to extract the numeric values for matching row identifiers. The regular expressions handle line breaks between words in the row identifiers, but not within a word. They can handle multiple signs of white space. Besides that, they try to fully match the labels from the legal text with the text extract, ignoring upper case. They extract numbers with "." as thousands separator.
:::

::: paragraph-start
##### Real tables

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation. The \acr{LLM} has to group row identifiers and corresponding numeric values and match the row identifier with the labels of the schema. If a row identifier is unknown, the values have to be discarded. If a label is not present among the row identifiers, the model predicts *null*. All values are extracted in one pass. We do not include any instruction, how to proceed with currency units, that might be given for certain columns.
:::

We test a wide range of open-weight models and compare different prompting techniques. Figure \@ref(fig:prompt-setup-extraction) shows, how the prompts are composed for the different strategies. Besides a zero shot approach we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing a synthetic **Aktiva** table as example. We test models from OpenAIs GPT family in addition to the open-weight models.

```{r prompt-setup-extraction, fig.cap="Showing the basic structure of the prompts and which strategies are used to pass additional information to the LLM for the information extraction task.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/promt_building_classification.png")
```

::: paragraph-start
##### Synthetic tables

We use \acr{LLM}s to extract the numeric values of synthetic **Aktiva** tables with restricted generation. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

::: paragraph-start
##### Hybrid approach

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation, providing examples from synthetic **Aktiva** tables. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

### Comparison {#comparing-table-extraction-methods}

This subsection compares the results for the table extraction tasks. It
will discuss the findings about performance and runtime and compare it
with the results a human may achieve wit manual labor.

::: paragraph-start
##### Performance

Table \@ref(tab:compare-extraction-performance) summarizes the mean
percentage of correct predictions total for all approaches and both
types of **Aktiva** tables. The highest baseline for the extraction
tasks is set by our own manual performance. We achieve 97.6 % correct
extracted values on the real **Aktiva** tables. The \acr{regex}
performance on the synthetic **Aktiva** tables comes close but on real
**Aktiva** tables it is far off.
:::

The mean performance of Qwen3-235B does not match our baseline on the
real **Akiva** tables. But its median performance already is 100 %. The
extraction performance may get higher, if the in-context learning
examples show how to deal with columns that have a currency unit. But
the strong performance observed right now is already based on implicit
numeric transformations, since 19.2 % of all numeric values have *T€* as
unit. Furthermore, both measures, percentage of correct numeric
predictions (98.0 %) and F1 score (98.1 %) are not perfect yet and could
be improved.

On synthetic tables its mean performance is almost perfect, if currency
units get respected. The 0.1 % incorrect prediction from the PDF
documents could be caused by faulty text extracts by *pdfium*. With HTML
documents we find 100 % correct predictions. With Markdown documents we
find 99.9 % correct predictions as well. Figure
\@ref(fig:comparing-table-extraction-performance-among-real-and-synth-aktiva-data)
shows, that the better performance on the synthetic tables is found for
almost all models.

Qwen3-8B performed best among the small models \acr{LLM}s but shows over
4 % more wrong predictions than Qwen3-235B. Using synthetic examples,
results in worse performance. But it can be used to show how to handle
currency units.

```{r compare-extraction-performance, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
tribble(
  ~approach, ~strategy, ~table_type, ~percentage_correct_total,
  "human", "manual", "real", 97.6,
  "regex", "", "real", 68.6,
  "llm", "Qwen3-235B, top_5_rag_examples", "real", 97.0,
  "llm", "Qwen3-8B, top_5_rag_examples", "real", 92.7,
  "llm", "Qwen3-235B, top_5_rag_examples, synth examples", "real", 91.8,
  "regex", "", "synth", 96.9,
  "llm", "Qwen3-235B, top_5_rag_examples, respect_units", "synth", 99.9,
  "llm", "Qwen3-8B, top_5_rag_examples", "synth", 94.6,
) %>% # arrange(table_type) %>% 
  group_by(table_type) %>% 
  mutate(
    across(
      c(percentage_correct_total),
      ~ifelse(      
        . == max(., na.rm = TRUE),
        paste0("**", round(., 3), "**"),
        round(., 3)
        )
      )) %>% 
  render_table(
    alignment = "lllr",
    caption = "Comparing the mean percentage of correct predictions total among all approaches and table types.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

::: paragraph-start
##### Runtime

Extracting the values from all 106 tables took Qwen3-235B around six
minutes. Thus, excluding the setup time for the LLM,
Qwen3-235B-A22B-Instruct is around 100 times faster than a human.
Checking the extracted values takes up to three minutes. This totals in
300 minutes prediction checking. Thus, selecting a smaller model that is
finishing after 2:30 minutes is not speeding up the process a lot. Once
we get a sufficient good performance with the big models the prediction
checking can be dropped. This would bring th real benefit.
:::

::: paragraph-start
##### Hypotheses

The predictor that showed a strong effect in all approaches is currency
unit. Reflecting this in the table extraction is a key factor to
optimize the performance. For the approaches that use \acr{LLM}s most of
the model and method related variables showed a strong effect. Using a
versatile model and providing good learning examples is mandatory.
:::

Especially for the approaches that use synthetic tables show that the
input format could also have a meaningful effect. It seems important to
prevent erroneous text extraction and converting the extracted text in
HTML might be helpful to eliminate last unclarities. But the question,
if a perfect text extract would be as good as HTML or Markdown, is not
answered yet.

## Error rate guidance {#error-rate-guidance-results}

(ref:error-rate-guidance-intro-text) The side research question asks, if it is possible to guide the users attention to predictions that have a higher empirical rate of errors. In this thesis we focus the confidence score reported with \acr{LLM}s responses.

(ref:error-rate-guidance-intro-text)

Subsection \@ref(page-identification-error-rate-results) presents the results found regarding our side research question for the page identification task. Subsection \@ref(information-extraction-error-rate-results) presents the results found regarding our side research question for the information extraction task. 

### Page identification {#page-identification-error-rate-results}

We find, that the confidence score can be used in the page identification task, to identify confidence intervals, that contain no or only a few error. The amount of predictions falling in these intervals varies among models and classification task and target class.

The 

### Information extraction {#information-extraction-error-rate-results}


Other possibilities narrow the selection down and get a more concrete error rate score for similar texts can be the type of information to extract, the company, ...

## Summary {#summary-results}
