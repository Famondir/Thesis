---
editor_options: 
  markdown: 
    wrap: 72
---

# Results

\ChapFrame

This chapter presents the results for the two research questions of this
thesis:

1.  How can we use \acr{LLM}s effectively to locate specific information
    in a financial report?
2.  How can we use \acr{LLM}s effectively to extract these information
    from the document?

Section \@ref(page-identification-introduction) presents the results for
the first research question. Section
\@ref(table-extraction-introduction) presents the results for the second
question.

Each section will start with an overview about the specific sub tasks as
well about the models, methods and data used to investigate the research
question. The subsections present the results of the sub tasks. At the
end of each section all results get compared and summarized.

## Page identification {#page-identification-introduction}

```{r count-benchmark-documents, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("../benchmark_truth/aktiva_passiva_guv_table_pages_no_ocr.csv", "./scripts/page_identification_preparations.R"))}
source("./scripts/page_identification_preparations.R")

num_multiple_tables_per_document <- df_targets_no_ocr %>%
  anti_join(consecutive_pages, by = c("filepath", "page", "type")) %>% 
  arrange(filepath, page) %>%
  group_by(filepath, type) %>% 
  summarise(count = n()) %>% 
  filter(count > 1) %>%
  group_by(filepath) %>% 
  summarise(count = n()) %>% 
  nrow()

multiple_tables_per_type_and_document <- data_unnested %>%
  anti_join(consecutive_pages, by = c("filepath", "page", "type")) %>% 
  arrange(filepath, page) %>%
  group_by(filepath, type) %>% 
  summarise(count = n()) %>% 
  filter(count > 1) %>% 
  group_by(type) %>% 
  summarise("multiple targets in document" = n())

df_special_targets <- multiple_tables_per_type_and_document %>% left_join(consecutive_pages %>% group_by(type) %>% summarise("target two pages long" = n())) %>% mutate_if(is.numeric,coalesce,0)
```

The first research question asks, how \acr{LLM}s can be used, to
effectively locate specific information in a financial report. The task
for this thesis is identifying the pages where the balance sheet
(*Bilanz*) and the profit-and-loss-and-statement (*Gewinn- und
Verlustrechnung, GuV*) are located. The balance sheet is composed of two
tables showing the assets (*Aktiva*) and liabilities (*Passiva*) of a
company. Often these two tables are on separate pages. Hereafter, the
German terms **Aktiva**, **Passiva** and \acr{GuV} will be used.

@li_extracting_2023 describes two ways to identify the relevant pages
(see Figure \@ref(fig:extraction-framework-flow-chart)). For longer
documents they propose to use the \acr{TOC} to determine a page range
that includes the information of interest. In addition, they develop
target specific regular expressions and rules to filter out irrelevant
pages[^05_results-1]. The result of this "Page Range Refinement" is then
passed to the \acr{LLM} to extract information from.

[^05_results-1]: Personal opinion: Developing well performing regular
    expressions can be a very tedious and setting appropriate rules
    requires some domain knowledge. It can be worth the effort if there
    are a lot of documents with similar information to extract. For this
    thesis it took multiple months. At least, now there is kind of a
    pipeline one can reuse, exchanging the rules and key word lists.
    Thus the next similar task should be solved faster.

This section is presenting four approaches to identify the
page[^05_results-2] of interest.

[^05_results-2]: In some cases the information of interest is spanning
    two pages. These rare cases are not covered from the approaches
    presented here, yet.

-   Subsection \@ref(regex-page-identification) presents the performance
    of a page range refinement using a list of key words with a regular
    expression.
-   Subsection \@ref(toc-understanding) presents the performance of a
    \acr{TOC} understanding approach
-   Subsection \@ref(llm-page-identification) presents the performance
    of a text classification using \acr{LLM}s.
-   Subsection \@ref(tf-classifier) presents the performance of a
    term-frequency approach.

In subsection \@ref(comparison-page-identification) the results get
compared and summarized. Subsection \@ref() proposes an efficient
combination of approaches to solve the task of this thesis and discusses
its limitations.

Figure \@ref(fig:document-base-page-identification) shows how the
document base for most of the tasks in this section is
composed[^05_results-3]. Overall `r num_documents` annual reports from
`r num_companies` companies are used. For this thesis the tables of
interest are those that show **Aktiva**, **Passiva** and **GuV**. Among
the `r total_pages_no_ocr` pages `r num_tables` tables have to be
identified on `r num_target_pages` pages. Figure
\@ref(fig:document-base-page-identification) also gives an impression on
how many pages the documents have. The documents of *IBB* tend to be
longer. The documents of *Amt für Statistik Berlin-Brandenburg* tend to
be shorter.

[^05_results-3]: I downloaded all publicly available annual reports for
    some of the companies shown in the first row of Figure
    \@ref(fig:beteiligungsunternehmen). I assumed that this will give a
    representative sample of document structures for the other companies
    of the same type. Realizing that the degewo AG reports would require
    ocr preprocessing I additionally downloaded reports for GESOBAU AG.
    This approach could have been more systematic. For the second task I
    downloaded reports for all companies available and tried to use a
    balanced amount of reports per company.

```{r, eval = knitr::is_html_output(), echo=FALSE, warning=warning_flag, message=message_flag, results='asis', class.chunk='pre-detail'}
cat("<p>You can have a look at the ground truth data unfolding the following details element.</p>")
```

```{r ground-truth-tables-of-interest, eval = knitr::is_html_output(), echo=FALSE, warning=warning_flag, message=message_flag, class.chunk='hideme', results='asis'}
df_targets_no_ocr %>% render_table(
  caption = "Showing all pages of interest with the target table type and filepath.",
  ref = opts_current$get("label")
  )
```

```{r document-base-page-identification, echo=echo_flag, warning=warning_flag, message=message_flag, fig.cap="Showing the number of pages (bar height) and number of documents (number above the bar) per company for the data used for the page identification task. Some documents would require ocr before being processed and were not used.", results='asis'}
df_pages %>% group_by(company) %>% mutate(
  docs_per_company = n(), pages_per_company = sum(pages)
  ) %>% 
  ggplot() +
  geom_col(aes(x = company, y = pages, fill = needs_ocr), color = "#00000033") +
  geom_text(
    data = . %>% group_by(company) %>% slice_head(n = 1),
    aes(x = company, y = pages_per_company, label = docs_per_company, group = company),
    stat = "identity",
    vjust = -0.5,
    size = 3
  ) +
  scale_x_discrete(guide = guide_axis(angle = 30))
```

Table \@ref(tab:display_multiple_tables_per_type_and_document) shows how
many documents have multiple target tables per type and how many target
tables span two pages. In total `r num_consecutive_pages` tables are
distributed on two pages. In `r num_multiple_tables_per_document`
documents there are multiple tables per type of interest. There are
`r num_two_tables_on_one_page` pages with two target tables (**Aktiva**
and **Passiva**) on it.

```{r display-multiple-tables-per-type-and-document, echo=FALSE, warning=warning_flag, message=message_flag, results='asis'}
df_special_targets %>% render_table(
  caption = "Showing the number of documents with multiple target tables per type and the number of target tables that span two pages.",
  ref = opts_current$get("label")
  )
```

This task is broken down to a classification task all of the approaches
presented in this section but the \acr{TOC} understanding approach.

Thus, we prompt the \acr{LLM} to classify if the text extract of a given
page

for implementation: As described in \@ref(text-extraction-benchmark)
open source libraries have been used to extract the text from the annual
reports.

### Baseline: Regex {#regex-page-identification}

```{r page-identification-regex-data-loading, echo=echo_flag, warning=warning_flag, message=message_flag, message=FALSE, cache=TRUE, cache.extra = tools::md5sum("data_storage/page_identification_regex.rds")}
data_page_identification_regex <- readRDS("data_storage/page_identification_regex.rds")

metrics <- data_page_identification_regex$metrics
metric_summaries <- data_page_identification_regex$metric_summaries
metrics_by_company_and_type <- data_page_identification_regex$metrics_by_company_and_type

metrics_plot_regex_page_identification <- metrics %>% bind_rows() %>%
  pivot_longer(
    cols = -c(package, method, classification_type),
    names_to = "metric",
    values_to = "value"
  ) %>%
  filter(metric %in% c(
    # "acc", 
    "precision", "recall", "F1")) %>%
  ggplot() +
  geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0) +
  facet_grid(metric~classification_type) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme(
    legend.position = "bottom"
  ) +
  coord_cartesian(ylim = c(0, 1))

# creating combi table
df_list <- list()

for (type in c("Aktiva", "Passiva", "GuV")) {
  df_temp <- metric_summaries[type][[1]] %>%
    mutate_if(
        is.numeric, 
        ~ifelse(
            . == max(., na.rm = TRUE),
            paste0("**", format(round(., 3), nsmall=3), "**"),
            format(round(., 3), nsmall=3)
        )
    ) %>% pivot_wider(names_from = stat, values_from = c(precision, recall, F1)) %>% mutate(
        precision = paste(precision_mean, "±", precision_sd),
        recall = paste(recall_mean, "±", recall_sd),
        F1 = paste(F1_mean, "±", F1_sd),
    ) %>% 
    select(-ends_with("_mean")) %>% select(-ends_with("_sd")) %>% mutate_all(~str_remove(., " ± NA")) %>% 
  arrange(desc(method)) %>% mutate(.before = 2, type = type)
  # rownames(df_temp) <- df_temp$method
  
  df_list[[type]] <- df_temp
}

df_combined <- bind_rows(df_list) %>% bold_value_in_table()
```

The first approach presented in this section is, to use a key word list
and \acrfull{regex} to filter out irrelevant pages. It is setting the
performance baseline for the following approaches. Building a sound
regular expression often is an iterative process. In a first approach a
very *simple regex* was implemented. To increase the recall to 1.0 the
regular expression was extended[^05_results-4]. This second regex is
called *exhaustive regex*. In a third attempt minor changes have been
made to the *exhaustive regex* to increase the precision without
decreasing the recall. This regular expression is called *exhausitve
regex restricted*. The regular expressions can be found in the appendix
(see section \@ref(regex-page-identification-code)).

[^05_results-4]: The idea is that the regular expression approach is
    computationally cheap. If we can rely on the fact, that it keeps all
    relevant pages we can use additional, computationally more expensive
    approaches to further refine the page range.

Table \@ref(tab:display-metrics-regex-page-identification-all) shows the
mean performance for precision, recall and F1 for the three regular
expressions for the three types of pages to identify[^05_results-5]. It
was possible to create a regular expression that has a high recall for
all target types. The precision is low for all tested regular
expressions and target types. Figure
\@ref(fig:display-metrics-plot-regex-page-identification-details) gives
insight into performance differences between the companies. There is
only one document from *Berlin Energie und Netzholding* where the
**GuV** is not identified except with the *exhausitve regex
restricted*[^05_results-6].

[^05_results-5]: See Figure
    \@ref(fig:display-metrics-plot-regex-page-identification) for a
    graphical representation.

[^05_results-6]: I don't understand why the restricted version is
    finding the page but the non-restricted regex is not.

The regular expressions have been tested on the texts extracted with
multiple Python libraries. The reported standard deviations are very
small. This means that there are no substantial differences in the
extracted texts on a word level[^05_results-7]. But table
\@ref(tab:display-data-text-extraction) in section
\@ref(text-extraction-benchmark) shows that there are differences in the
extraction speed.

[^05_results-7]: Since the results are not depending on the text
    extraction library, the *exhaustive regex restricted* ran only with
    the text extracted by the fastest extraction library: *pdfium*. This
    library is used for the most tasks in this thesis. Later faced
    issues with the text extracted by *pdfium* are discussed in \@ref().

Code can be found at
"benchmark_jobs/page_identification/page_identification_benchmark_regex.ipynb"

Todo: \* look into details where they differ and if it is because of a
line break or whitespace?

```{r display-metrics-regex-page-identification-all, echo=echo_flag, warning=warning_flag, message=message_flag, results=ifelse(knitr::is_html_output(), "asis", 'markup')}
if (knitr::is_html_output()) {
  # Set caption
  cat("<table>",paste0("<caption>", "(#tab:", opts_current$get("label"), ")", "Comparing page identification metrics for different regular expressions for each classification task by type of the target table.", "</caption>"),"</table>", sep ="\n")

  df_combined %>% datatable(
    escape = FALSE, options = list(
      pageLength = 10, scrollX = TRUE, dom = "t", rowGroup = list(dataSrc=c(2)), ordering=FALSE, 
      columnDefs = list(list(visible=FALSE, targets=c(2)))
    ), extensions = 'RowGroup'
  )
} else if (knitr::is_latex_output()) {
  df_combined %>% kbl(caption = "Comparing page identification metrics for different regular expressions for each classification task by type of the target table.", escape = FALSE) %>%
    pack_rows("Aktiva", 1, 3, label_row_css = "background-color: #666; color: #fff;") %>%
    pack_rows("Passiva", 4, 6, label_row_css = "background-color: #666; color: #fff;") %>%
    pack_rows("GuV", 7, 9, label_row_css = "background-color: #666; color: #fff;")
}
```

```{r display-metrics-plot-regex-page-identification-details, echo=echo_flag, warning=warning_flag, message=message_flag, figwidth=8, fig.height=8, out.width="100%", fig.cap="Comparing the performance among different companies."}
metrics_by_company_and_type %>% 
    select(company, n_files, method, precision, recall, classification_type) %>%
    pivot_longer(cols=c(precision, recall), names_to = "metric") %>% 
    ggplot() +
    geom_boxplot(aes(x = company, y = value, fill = n_files), alpha = 0.5) +
    geom_jitter(aes(x = company, y = value, color = method), alpha = 1, height = 0) +
    facet_grid(classification_type~metric) +
    scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme(
    legend.position = "bottom"
  )
```

### Table of Contents understanding {#toc-understanding}

```{r page-identification-toc-data-loading, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("scripts/page_identification_toc_data_loading.R", "../benchmark_truth/toc_data.json", "../Python/pdf_texts.json"))}
source("scripts/page_identification_toc_data_loading.R")
```

The second approach presented in this section leverages the \acr{TOC}
understanding capabilities of \acr{LLM}s. @li_extracting_2023 use this
approach with long documents as a first step to determine a page range
of interest. If the predicted page range is correct and narrow, this
approach is more efficient than processing the whole document with a
\acr{LLM} directly. The \acr{TOC} in a \acr{PDF} document can be
embedded in a standardized, machine readable format or be presented in
varying, human readable forms of text on any page. Of course there are
documents without any \acr{TOC}.

Thus, the task is investigated based on two different input data formats
In one case the \acr{LLM} is provided with text extracted from the
beginning of the document. In the other case the \acr{LLM} is provided
with the Markdown formatted version of the machine readable \acr{TOC}
embedded in the document. Subsection \@ref(text-based-toc-understanding)
shows the results for the text based approach. Subsection
\@ref(code-based-toc-understanding) shows the results for the approach,
using the embedded \acr{TOC}.

Additionally, each approach is performed three times with minor changes
in the prompt. The prompts used for both approaches can be found at
\@ref(toc-understanding-promts). The prompt was adjusted two times to
tackle shortcomings in the results. The first change adds the
information, that assets and liabilities are part of the balance sheet.
It is the balance sheet, that is listed in the \acr{TOC} - not the
assets or liabilities itself. The second change specifies the
information, that assets and liabilities are often on separated pages,
into, liabilities often are found on the page after the assets.

The code can be found in:

-   "benchmark_jobs/page_identification/toc_extraction_mistral.ipynb"

-   "benchmark_jobs/page_identification/toc_extraction_qwen.ipynb"

Discussion:

-   @li_extracting_2023 did not report any issues with this approach.
    They use few-shot learning and Chain-of-Thought techniques to help
    the \acr{LLM} to understand the task. They ask just for one
    information at a time.
-   ChatGPT 4 vs Mistral 2410 8B (huge parameter difference)
-   For a lot of short annual reports one can find the tables of
    interest within the first eight pages as well.

#### Details for the approaches {#text-based-toc-understanding}

##### Text based

@li_extracting_2023 used the \acr{TOC} to identify the pages of
interest. In their approach the table of contents is extracted from the
text. Based on their observation, that the \acr{TOC} in \acr{ACFR}s is
found within the initial 165 lines of the converted document
[@li_extracting_2023, p. 20], they use the first 200 lines of text.

My initial expectation was to find the \acr{TOC} within the first five
pages. Often there are way less than 200 lines of text on the five first
pages (see Figure \@ref(fig:page-identification-toc-histogram)). In my
approach the first step is to prompt the \acr{LLM} to identify and
extract the \acr{TOC} in a given text extract\^[The prompt can be found
in section \@ref(toc-understanding-promts)]. For the same documents
Mistral 2410 8B finds\^[The strings extracted in this step have not been
checked in detail.]

-   `r n_found_toc_5_pages` strings that should represent a table of
    contents among the first five pages.

-   `r n_found_toc_200_lines` strings that should represent a table of
    contents among the first 200 lines.

```{r page-identification-toc-histogram, echo=echo_flag, warning=warning_flag, message=message_flag, fig.width=8, fig.height=3, out.width="80%", fig.cap="Histogram of the number of lines in the first 5 pages of the annual reports"}
df_num_lines %>% 
  filter(filepath %in% df_targets_no_ocr$filepath) %>% # filter documents that would need ocr preprocessing
  ggplot() +
  geom_histogram(aes(x = num_lines), bins = 20) +
  labs(x = "Number of lines in the first 5 pages", y = "Count") +
  coord_cartesian(xlim = c(0, max(df_num_lines$num_lines)))
```

##### Machine readable TOC based {#code-based-toc-understanding}

I also tested to use the \acr{TOC} representation embedded within the
PDF files. First, this limits the text amount to process. Second, this
hopefully increases the quality of the data passed to the \acr{LLM}.
`r n_toc` of the `r n_toc+n_no_toc` annual reports have a machine
readable embedded \acr{TOC}. The embedded \acr{TOC} is converted into
markdown format before it gets passed to the \acr{LLM}. Here is an
example:

```{r page-identification-markdown-code, echo=FALSE, warning=warning_flag, message=message_flag}
cat(toc_data$files_with_toc$markdown_toc[[1]])
```

#### Results

##### Comparison of the different approaches: base prompt

Table \@ref(tab:page-identification-toc-analysis-table) shows that the
machine readable \acr{TOC} approach has the highest rate of correct page
ranges for all types with the base prompt. It also predicts the most
correct page ranges in absolute numbers for **Aktiva** and **GuV**.
Thus, it also has the highest rate of correct page ranges based on the
total number of page ranges to identify over all documents - no matter,
if there was a \acr{TOC} of any type in the document or not - for
**Aktiva** and **GuV** of around `r perc_correct_total_base` %.

```{r page-identification-toc-analysis-table, echo=FALSE, warning=warning_flag, message=message_flag, results='asis'}
df_toc_benchmark %>% group_by(benchmark_type, type) %>% 
  reframe(n_correct = sum(in_range), n = n()) %>% 
  left_join(data_unnested %>% group_by(type) %>% summarise(n_total = n())) %>% 
  mutate(
    perc_correct = n_correct/n*100,
    perc_correct_total = n_correct/n_total*100
    ) %>% 
  group_by(type) %>% 
  mutate_at(
    vars(perc_correct, n_correct, perc_correct_total), 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", format_floats(.), "**"),
      format_floats(.)
    )
  ) %>% 
  render_table(alignment="llrrrrr", caption="Comparing the number and percentage of correct identified page ranges among the approaches.", ref = opts_current$get("label"), dom="t")
```

Figure \@ref(fig:page-identification-toc-analysis) shows that the amount
of correct predicted page ranges for **Passiva** is lowest for all
approaches but can be improved by simply extending the predicted end
page number by one the most. This improvement would be best for the
machine readable \acr{TOC} approach. This approach is the only one,
where the number of correct page ranges **Aktiva** would not increase if
we extend its range by one. Table
\@ref(tab:page-identification-toc-analysis-same-end-page) shows that
this is the case, because the machine readable \acr{TOC} approach
predicts the same end page for **Passiva** as for **Aktiva** in
`r perc_equal_end_page_mr` % of the cases, even though the prompt for
all approaches included the information, that **Aktiva** and **Passiva**
are on separate pages.

```{r page-identification-toc-analysis, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Comparing number of found TOC and amount of correct and incorrect predicted page ranges"}
df_toc_benchmark %>% ggplot() +
  geom_bar(aes(x = type, fill = in_range, colour = min_distance <= 1)) +
  geom_text(
    data = . %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1)
```

```{r page-identification-toc-analysis-same-end-page, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_toc_benchmark %>%
  filter(type != "GuV") %>%
  group_by(benchmark_type, filepath) %>%
  summarise(all_equal = n_distinct(end_page) == 1) %>% 
  group_by(benchmark_type) %>% 
  reframe(equal_end_page = sum(all_equal), n = n()) %>% 
  mutate(perc_equal_end_page = round(equal_end_page/n*100,1)) %>% 
  render_table(alignment="lrrr", caption="Comparing the number and percentage end pages prediction for Aktiva and Passiva that are equal.", ref = opts_current$get("label"), dom="t")
```

##### Comparison of the different approaches: advanved prompts

As a first attempt, to increase the correct page range rate for
**Passiva** I tried to specify, that assets and liabilities are part of
the balance sheet. This did work for the text based approaches, but not
for the machine readable approach (see Figure
\@ref(fig:page-identification-toc-analysis-balanced)). Figure
\@ref(fig:page-identification-toc-analysis-next-page) shows that it is
more successful, to explicit tell the \acr{LLM} that the liabilities
table is often on the page, after the assets table.

```{r page-identification-toc-analysis-next-page, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Comparing number of fount TOC and amount of correct and incorrect predicted page ranges"}
next_page_df_toc_benchmark %>% ggplot() +
  geom_bar(aes(x = type, fill = in_range, colour = min_distance <= 1)) +
  geom_text(
    data = . %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1) # +
  # theme(
  #   legend.position = "bottom"
  # )
```

Table \@ref(tab:page-identification-toc-analysis-table-next-page) shows
the results from the final zero shot prompt. The machine readable
\acr{TOC} approach is now predicting best for all types. Nevertheless, a
correct page range prediction rate below `r worst_finaLcorrect_rate_toc`
% is still unsufficient to build downstream task on withou human
checkups. Table \@ref(tab:page-identification-toc-gpu-time-comparison)
shows, that the machine readable \acr{TOC} approach is the fastest as
well.

```{r page-identification-toc-analysis-table-next-page, echo=FALSE, warning=warning_flag, message=message_flag, results='asis'}
next_page_df_toc_benchmark %>% group_by(benchmark_type, type) %>% 
  reframe(n_correct = sum(in_range), n = n()) %>% 
  left_join(data_unnested %>% group_by(type) %>% summarise(n_total = n())) %>% 
  mutate(
    perc_correct = n_correct/n*100,
    perc_correct_total = n_correct/n_total*100
    ) %>% 
  group_by(type) %>% 
  mutate_at(
    vars(perc_correct, n_correct, perc_correct_total), 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", format_floats(.), "**"),
      format_floats(.)
    )
  ) %>% 
  render_table(alignment="llrrrrr", caption="Comparing the number and percentage of correct identified page ranges among the approaches.", ref = opts_current$get("label"), dom="t")
```

```{r page-identification-toc-gpu-time-comparison, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis"}
gpu_time_per_document_page_range %>% mutate_if(
  is.numeric, 
  ~ifelse(
    . == min(., na.rm = TRUE),
    paste0("**", ., "**"),
    .
  )
) %>% 
  render_table(alignment="lrr", caption="Comparing GPU time for page range prediction and table of contents extraction. Time in seconds per text processed.", ref = opts_current$get("label"))
```

Table \@ref(tab:page-range-prediction-performance-table) shows, that
this advantage of the machine readable \acr{TOC} approach is not coming
from wide predicted page ranges. It has the smallest median range size
among all approaches. Figure
\@ref(fig:page-identification-toc-range-plot) shows, that especially the
ranges for **GuV** are not normally distributed. Some far off lying
range sizes are shifting the mean off from the median.

```{r page-range-prediction-performance-table, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
mean_ranges %>%
  mutate_at(
    vars(median_range),
    ~ifelse(
      . == min(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>%
  render_table(alignment="lrrrr", caption="Comparing the mean and median page range sizes.", ref = opts_current$get("label"))
```

```{r page-identification-toc-range-plot, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Comparint the predicted page range sizes. The red vertical line shows the mean and the green one shows the median of these sizes."}
next_page_df_toc_benchmark %>% 
  # mutate(range = range-1) %>% 
  ggplot() +
  geom_histogram(aes(x = range), binwidth = 1) +
  geom_vline(data = mean_ranges, aes(xintercept = mean_range), color = "red") +
  geom_vline(data = mean_ranges, aes(xintercept = median_range), color = "green") +
  facet_grid(benchmark_type~type)
```

Figure \@ref(fig:page-identification-toc-range-logprobs) shows that the
confidence of the \acr{LLM}s responses is higher for the machine
readable \acr{TOC} approach as well. Besides a single group that was
predicted far off, the page ranges are closer to the correct pages too.
A linear regression of the correlation between minimal page distance and
logistic probability shows that is has a similar slope for all
approaches and target types.

```{r page-identification-toc-range-logprobs, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Showing the minimal distance of the predicted page range to the actual page number overthe logprobs of the models response confidence."}
next_page_df_toc_benchmark %>% group_by(filepath, benchmark_type) %>%
  distance_confidence_plot() +
  facet_grid(type~benchmark_type) +
  theme(
    legend.position = "bottom"
  )
```

##### Machine readable TOC approach specific results

Figure \@ref(fig:page-identification-toc-mr-degration) shows, that
correct predictions for the page range are more probable when the
embedded \acr{TOC} has a medium number of entries. It is possible to
drop documents with less than `r min_n_entries_max_correct` without
loosing a single correct prediction. This means that the \acr{LLM} was
not able to make a correct prediction for documents with \acr{TOC}, that
have less then `r min_n_entries_max_correct` entries. This is not
surprising since neither **Bilanz** nor **GuV** are mentioned there
explicit.

It has no big influence on the predictions, if the \acr{TOC} is passed
formatted as markdown or json. With the json formatted \acr{TOC} it
found two more correct page ranges[^05_results-8]. This was tested
because the relation between heading and value for the column
*page_number* might have been clearer[^05_results-9] in json for a
one-dimensional working \acr{LLM}.

[^05_results-8]: This result is based on a single test run.

[^05_results-9]: With json the key *page_number* gets repeated every
    line, while it is just mentiones once in the beginning of the
    markdown formatted tables.

```{r page-identification-toc-mr-degration, echo=echo_flag, warning=warning_flag, message=message_flag, fig.cap="Showing the amount of correct and incorrect predicted page ranges (bars) and the percentage of correct predictions (black line)."}
df_toc_benchmark_mr_degration_next_page %>% 
  ggplot() +
  geom_col(aes(x = n_entries, y = value, fill = correct)) +
  geom_line(aes(x = as.numeric(n_entries), y = 100*perc_correct, group = 1)) +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  facet_wrap(~type, ncol = 1)
```

Delete or place somewhere else?:

-   Thus it is safer to go with the 200 lines approach. But it also
    takes longer. \@ref(tab:page-identification-toc-gpu-time-comparison)

-   Values can be higher than 80, the total number of PDF files, since
    there can be multiple tables of interested for the same type in a
    single document or a table of interest can span two pages.

### Classification with LLMs {#llm-page-identification}

```{r page-identification-llm-data-loading, echo=echo_flag, warning=warning_flag, message=message_flag}
temp_list <- readRDS("data_storage/page_identification_llm.rds")
df_binary <- temp_list$df_binary
df_multi <- temp_list$df_multi

method_families <- c("zero_shot", "law_context", "top_n_rag_examples", "n_random_examples", 'n_rag_examples')
method_familiy_colors <- c(
  "zero_shot" = "#e41a1c", 
  "law_context" = "#377eb8", 
  "top_n_rag_examples" = "#4daf4a", 
  "n_random_examples" = "#984ea3", 
  'n_rag_examples' = "#ff7f00"
  )

df_binary <- df_binary %>% filter(!str_detect(model, "gemma") || str_detect(model, "0-9-1")) %>% 
  mutate(
  n_examples = as.numeric(n_examples),
  n_examples = if_else(method_family == "zero_shot", 0, n_examples),
  n_examples = if_else(method_family == "law_context", 1, n_examples),
  method_family = factor(method_family, levels = method_families)
)

binary_task <- list()
binary_task$n_models <- df_binary$model %>% unique() %>% length()
binary_task$n_model_families <- df_binary$model_family %>% unique() %>% length()
binary_task$n_method_families <- df_binary$method_family %>% unique() %>% length()
```

structured outputs forcing to answer with a *yes* or *no* for binary
task or with *Aktiva*, *Passiva*, *GuV* or *other* for multi
classification task

top n accuracy

out of company vs in compnay rag

#### Binary classification

Could be more efficient to predict "is any of interest" and then which
type, because dataset is highly imbalanced.

`r  binary_task$n_models` models from `r binary_task$n_model_families`
haven been benchmarked among `r binary_task$n_method_families` methods

Most models have been used up to 3 examples for the context.

The best combination of model and method for each method family is
presented in the following table. It is clear that the Google Gemma
models are performing worst.[^05_results-10] Surprisingly Mistral 2410
is the best performing model for all three prediction tasks even though
it only has 8B parameters.

[^05_results-10]: This is not due to a temporary technical problems
    caused by a bug in the transformers version shipped with the vllm
    0-9-2 image. Those problems have been overcome. The performance
    stays bad.

```{r table-metrics-llm-page-identification-binary, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis", class.chunk="big-table"}
df_binary %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, classification_type) %>% 
  filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
  # mutate(
  #   n = n()
  # ) %>% filter(n > 1)
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, classification_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  ) %>% 
  render_table()
```

It is interesting that the predictions do not get better by providing
more and more examples. Especially for the n-rag-example approach we
find a significant drop in the F1 score if the examples pages come from
different companies annual reports. This is caused by a sever recall
drop. But also for the n-ranom-example approach we see this for the
prediction of class Passiva.

Recall better with examples from same company. Precision better without.

We can also see that the prediction performance is
stable.[^05_results-11]

[^05_results-11]: Earlier experiments on a subset of the pages have been
    run five times indicating stable results. Running the experiments up
    to tree times in this very task indicate this as well.

```{r binary-classification-result-mistral-plot, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_binary %>% filter(model == "mistralai_Ministral-8B-Instruct-2410", loop < 3) %>% 
  filter(n_examples <= 5 | is.na(n_examples)) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  geom_text(aes(label = n_examples)) +
  scale_color_manual(values = method_familiy_colors) +
  facet_grid(model~classification_type) +
  # theme(legend.position = "bottom") +
  guides(
    color = guide_legend(ncol = 1, title.position = "top"),
    shape = guide_legend(ncol = 1, title.position = "top")
  )
```

-   f1
-   multiple models
-   best model detail (different methods / settings)

The experiments for the best performing model,
Ministral-8B-Instruct-2410, have been extended by methods with even more
examples. Especially for the top-n-rag-example approach to get a better
comparable picture based on the real number of examples / context
length.

```{r binary-classification-result-mistral-many-random-examples-plot, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_binary %>% filter(model == "mistralai_Ministral-8B-Instruct-2410", loop < 2) %>% 
  filter(n_examples > 1 | is.na(n_examples)) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  scale_color_manual(values = method_familiy_colors) +
  geom_text(aes(label = n_examples)) +
  facet_grid(model~classification_type) +
  # theme(legend.position = "bottom") +
  guides(
    color = guide_legend(ncol = 1, title.position = "top"),
    shape = guide_legend(ncol = 1, title.position = "top")
  )
```

```{r comparing-actual-number-of-in-context-examples, echo=echo_flag, warning=warning_flag, message=message_flag}
read_csv("../benchmark_truth/real_example_count.csv") %>% kbl()
```

Predictions very accurate. Confidence not always 1. Wrong predictions
with often with medium confidence. If Aktiva and Passiva on same page
more often Aktiva predicted. Confidence for no displayed as 1-confidence
to represent confidence for yes (binary classification).

```{r confidence-match-plot-mistral-binary, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_filtered <- df_binary %>% filter(classification_type == "Aktiva") %>% 
  arrange(desc(f1_score))
df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
df_flipped_score <- df_temp %>% 
  mutate(
    confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
    is_aktiva = str_detect(type, "Aktiva")
  )
model_name_best_f1_aktiva <- df_filtered[1, "model"]
method__best_f1_aktiva <- df_filtered[1, "method"]

df_flipped_score %>% 
  ggplot() +
  geom_boxplot(aes(x = predicted_type, y = confidence_score)) +
  geom_jitter(aes(x = predicted_type, y = confidence_score, color = match), alpha = .3, height = 0) +
  facet_wrap(~type) +
  labs(title = model_name_best_f1_aktiva,
       subtitle = method__best_f1_aktiva)
```

Qwen returns always high confidence even if it is wrong.

```{r confidence-match-plot-qwen-binary, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_filtered <- df_binary %>% filter(classification_type == "Aktiva", model_family=="Qwen") %>% 
  arrange(desc(f1_score))
df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
df_flipped_score <- df_temp %>% 
  mutate(
    confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
    is_aktiva = str_detect(type, "Aktiva")
  )
model_name_best_f1_aktiva <- df_filtered[1, "model"]
method__best_f1_aktiva <- df_filtered[1, "method"]

df_flipped_score %>% 
  ggplot() +
  geom_boxplot(aes(x = predicted_type, y = confidence_score)) +
  geom_jitter(aes(x = predicted_type, y = confidence_score, color = match), alpha = .3, height = 0) +
  facet_wrap(~type) +
  labs(title = model_name_best_f1_aktiva,
       subtitle = method__best_f1_aktiva)
```

-   IBB other law
-   degewo only one where no ocr is needed

mistral: recall IBB and Netzholding big range meta & mistral: very high
precision for Amt für Statistik BBB \<- lowest average pagecount (29.3)
but IBB has more pages than berlinovo but better precision. No
information about which company / report the page is from

```{r performance-based-on-company, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
no_ocr_needed <- read_csv("../benchmark_truth/aktiva_passiva_guv_table_pages_no_ocr.csv") %>% select(filepath) %>% 
  unique() %>% mutate(filepath = str_replace(filepath, "..", "/pvc")) %>% .[[1]]

l <- list()

for (t in c('Aktiva', 'Passiva', 'GuV')) {
  df_filtered <- df_binary %>% filter(classification_type == t) %>% 
    arrange(desc(f1_score)) %>% select(model, method, predictions, model_family, method_family)
  df_temp <- df_filtered %>% unnest(predictions) %>% filter(filepath %in% no_ocr_needed)
  
  df_f1_by_company <- df_temp %>% group_by(company, predicted_type, match, model,model_family, method, method_family) %>% reframe(
    n = n()
  ) %>% complete(company, predicted_type, match, model,model_family, method, method_family,fill=list(n=0)) %>% 
    mutate(
      metric = if_else(predicted_type == t & match, "true_positive", ''),
      metric = if_else(predicted_type == t & !match, "false_positive", metric),
      metric = if_else(predicted_type != t & !match, "false_negative", metric),
      metric = if_else(predicted_type != t & match, "true_negative", metric),
    ) %>% select(-predicted_type, -match) %>% 
    pivot_wider(names_from = metric, values_from = n) %>% 
    mutate(
      precision = true_positive/(true_positive+false_positive),
      recall = true_positive/(true_positive+false_negative),
      f1_score = 2*precision*recall/(precision+recall),
      classification_type = t
    )
  
  l[t] <- list(df_f1_by_company)
}

df_f1_by_company <- bind_rows(l)

df_f1_by_company %>% 
  filter(!model_family %in% c('microsoft', 'tiiuae')) %>% 
  ggplot() +
  geom_boxplot(aes(x = company, y = f1_score)) +
  # geom_jitter(aes(x = company, y = f1_score, color = model), alpha = .4) +
  facet_grid(classification_type~model_family) +
  scale_x_discrete(guide = guide_axis(angle = 30))
```

```{r report-count-by-company-tables, echo=echo_flag, warning=warning_flag, message=message_flag}
n_reports_by_company_no_ocr <- df_temp %>% select(company, filepath) %>% unique() %>% group_by(company) %>% reframe(n = n())
n_reports_by_company <- df_filtered %>% unnest(predictions) %>% select(company, filepath) %>% unique() %>% group_by(company) %>% reframe(n = n())

n_reports_by_company_no_ocr %>% kbl()
n_reports_by_company %>% kbl()
```

-   Performance makes a jump at a critical parameter number (3B) then
    slow increase (compare Qwen 2.5)
-   Changes unsystematic with new models (see Mistral, Qwen 3 old vs
    llama 4)

PR curves for all classes look very alike- showing micro average curve

```{r micro-pr-curve-llm-binary-1, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
model_rank <- 1
l_temp <- list()

for (target in c('Aktiva', 'GuV', 'Passiva')) {
  t <- "Aktiva"
  df_filtered <- df_binary %>% filter(classification_type == t,
      loop == 0) %>% 
    arrange(desc(f1_score))
  model_name_best_f1_aktiva <- df_filtered[model_rank, "model"]
  method__best_f1_aktiva <- df_filtered[model_rank, "method"]
  
  df_filtered <- df_binary %>% 
    filter(
      classification_type == target,
      model == model_name_best_f1_aktiva,
      method == method__best_f1_aktiva,
      loop == 0
    ) %>% 
    arrange(desc(f1_score))
  df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
  df_flipped_score <- df_temp %>% 
    mutate(
      confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
      target = target
    )
  l_temp[target] <- list(df_flipped_score)
}

df_temp2 <- bind_rows(l_temp) # %>% filter(target == "Passiva")
# plot(pr_obj, color = "orange", main = "Precision-Recall Curve")

pr_obj <- pr.curve(scores.class0 = df_temp2$confidence_score[df_temp2$match == 1],
                   scores.class1 = df_temp2$confidence_score[df_temp2$match == 0],
                   curve = TRUE)

# Precision-Recall Curve with ggplot2

pr_df <- tibble(
  recall = pr_obj$curve[, 1],
  precision = pr_obj$curve[, 2],
  threshold = pr_obj$curve[, 3]
) %>%
  mutate(f1 = 2 * precision * recall / (precision + recall))

pr_auc <- round(pr_obj$auc.integral, 3)

g1 <- pr_df %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_line(aes(color = threshold), size = 1.2) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = str_c("Precision-Recall Curve (AUC = ", pr_auc, ")"),
    subtitle = str_c(model_name_best_f1_aktiva, " with ", method__best_f1_aktiva),
    x = "Recall",
    y = "Precision"
  ) +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    legend.position = "bottom"
  )

g2 <- pr_df %>%
  ggplot(aes(x = recall, y = precision, color = f1)) +
  geom_line(size = 1.2) +
  scale_color_viridis_c(option = "viridis") +
  labs(
    # title = "Precision-Recall Curve colored by F1 score",
    x = "Recall",
    y = NULL,
    color = "F1 score"
  ) +
  coord_cartesian(ylim = c(0,1))+
  theme(
    legend.position = "bottom"
  )

g1 + g2
```

```{r micro-pr-curve-llm-binary-300, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
l_temp <- list()

for (target in c('Aktiva', 'GuV', 'Passiva')) {
  t <- "Aktiva"
  df_filtered <- df_binary %>% filter(classification_type == t,
      loop == 0) %>% 
    arrange(desc(f1_score))
  model_rank <- as.integer(nrow(df_filtered)*0.8)
  
  model_name_best_f1_aktiva <- df_filtered[model_rank, "model"]
  method__best_f1_aktiva <- df_filtered[model_rank, "method"]
  
  df_filtered <- df_binary %>% 
    filter(
      classification_type == target,
      model == model_name_best_f1_aktiva,
      method == method__best_f1_aktiva,
      loop == 0
    ) %>% 
    arrange(desc(f1_score))
  df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
  df_flipped_score <- df_temp %>% 
    mutate(
      confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
      target = target
    )
  l_temp[target] <- list(df_flipped_score)
}

df_temp2 <- bind_rows(l_temp) # %>% filter(target == "Passiva")
# plot(pr_obj, color = "orange", main = "Precision-Recall Curve")

pr_obj <- pr.curve(scores.class0 = df_temp2$confidence_score[df_temp2$match == 1],
                   scores.class1 = df_temp2$confidence_score[df_temp2$match == 0],
                   curve = TRUE)

# Precision-Recall Curve with ggplot2

pr_df <- tibble(
  recall = pr_obj$curve[, 1],
  precision = pr_obj$curve[, 2],
  threshold = pr_obj$curve[, 3]
) %>%
  mutate(f1 = 2 * precision * recall / (precision + recall))

pr_auc <- round(pr_obj$auc.integral, 3)

g1 <- pr_df %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_line(aes(color = threshold), size = 1.2) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = str_c("Precision-Recall Curve (AUC = ", pr_auc, ")"),
    subtitle = str_c(model_name_best_f1_aktiva, " with ", method__best_f1_aktiva),
    x = "Recall",
    y = "Precision"
  ) +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    legend.position = "bottom"
  )

g2 <- pr_df %>%
  ggplot(aes(x = recall, y = precision, color = f1)) +
  geom_line(size = 1.2) +
  scale_color_viridis_c(option = "viridis") +
  labs(
    # title = "Precision-Recall Curve colored by F1 score",
    x = "Recall",
    y = NULL,
    color = "F1 score"
  ) +
  coord_cartesian(ylim = c(0,1))+
  theme(
    legend.position = "bottom"
  )

g1 + g2
```

#### Multi classification

bigger models are better with the multi classification task
Llama-4-Scout almost perfect F1 for all classes

Llama-4-Scout runs fast but needs long to load because it has 109B in
total with 17B actives Gemma performs much better than with binary
classification

drop with Qwen-14B

```{r table-metrics-llm-page-identification-multi, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis", class.chunk="big-table"}
df_multi %>% 
  unnest(metrics) %>% 
  filter(metric_type %in% c("Aktiva", "Passiva", "GuV")) %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, metric_type) %>% 
  filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, metric_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  ) %>% 
  render_table()

# df_multi %>% 
#   unnest(metrics) %>% 
#   filter(metric_type %in% c("micro_minorities")) %>% 
#   filter(is.finite(f1_score), loop == 0) %>% 
#   filter(n_examples <= 3 | is.na(n_examples)) %>%
#   group_by(model_family, metric_type) %>% 
#   filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
#   arrange(desc(f1_score)) %>% # head(10) %>% 
#   select(model_family, model, metric_type, method_family, n_examples, f1_score, norm_runtime) %>%
#   mutate(
#     f1_score = round(f1_score, 2),
#     norm_runtime = round(norm_runtime, 0),
#   ) %>% rename(
#     "runtime in s" = norm_runtime,
#   ) 
```

Mistral-8B-2410 almost as good as Mistral-123B-2411 but much faster

```{r table-metrics-llm-page-identification-multi-small-models, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis", class.chunk="big-table"}
df_multi %>% 
  unnest(metrics) %>% 
  filter(metric_type %in% c("Aktiva", "Passiva", "GuV")) %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(parameter_count<15) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, metric_type) %>% 
  filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, metric_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  ) %>% 
  render_table()
```

Mistral-2410 reaches good performance already with few examples and can
work with law-context approach but more examples don't realy help any
further

```{r performance-over-runtime-multi-classification-mistral-vs-llama-scout, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_selected <- df_multi %>% unnest(metrics) %>% filter(metric_type == "Aktiva")

df_selected %>% 
  filter(model %in% c(
    "mistralai_Ministral-8B-Instruct-2410",
    # "mistralai_Mistral-Large-Instruct-2411",
    # "mistralai_Mistral-Small-3.1-24B-Instruct-2503",
    "meta-llama_Llama-4-Scout-17B-16E-Instruct"
    # "meta-llama_Llama-4-Maverick-17B-128E-Instruct-FP8"
  )) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  geom_text(aes(label = n_examples)) +
  facet_grid(model~metric_type) +
  scale_color_manual(values = method_familiy_colors) #+
  # theme(legend.position = "bottom") +
  # guides(
  #   color = guide_legend(ncol = 1, title.position = "top"),
  #   shape = guide_legend(ncol = 1, title.position = "top")
  # ) +
  # scale_x_discrete(guide = guide_axis(angle = 30))
```

Most of the time pretty confident most problems with class "other" If
Aktiva and Passiva on same page it predicts Aktiva. Also one Passiva
missclassified as Aktiva No flipped confidence [^05_results-12]

[^05_results-12]: classify framework in needs special models with
    pooling capability. Would have been interesting but time was limited
    and would have needed new special models in most cases

```{r confidence-match-plot-mistral-multi, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_filtered <- df_selected %>%
  arrange(desc(f1_score))
df_temp <- df_filtered[1,"predictions"][[1]][[1]] %>% as_tibble()
df_flipped_score <- df_temp %>% 
  mutate(
    confidence_score = if_else(predicted_type == "no", 1-confidence_score, confidence_score),
    is_aktiva = str_detect(type, "Aktiva")
  )
model_name_best_f1_aktiva <- df_filtered[1, "model"]
method__best_f1_aktiva <- df_filtered[1, "method"]

df_flipped_score %>% 
  ggplot() +
  geom_boxplot(aes(x = predicted_type, y = confidence_score)) +
  geom_jitter(aes(x = predicted_type, y = confidence_score, color = match), alpha = .3, height = 0) +
  facet_wrap(~type) +
  labs(title = model_name_best_f1_aktiva,
       subtitle = method__best_f1_aktiva)
```

Microsoft phi 4 and Falcon 3 only ran with one and two examples because
their context window is smaller.

-   f1
-   multiple models
-   best model detail (different methods / settings)

```{r micro-pr-curve-llm-multi-1, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
model_rank <- 1

df_selected <- df_multi %>% unnest(metrics)
df_filtered <- df_selected %>% filter(
  metric_type == "micro_minorities"
) %>% 
  arrange(desc(f1_score))
df_temp <- df_filtered[model_rank,"predictions"][[1]][[1]] %>% as_tibble()

model_name_best_f1_aktiva <- df_filtered[model_rank, "model"]
method__best_f1_aktiva <- df_filtered[model_rank, "method"]

pr_obj <- pr.curve(scores.class0 = df_temp$confidence_score[df_temp$match == 1],
                   scores.class1 = df_temp$confidence_score[df_temp$match == 0],
                   curve = TRUE)

# plot(pr_obj, color = "orange", main = "Precision-Recall Curve")

# Precision-Recall Curve with ggplot2

pr_df <- tibble(
  recall = pr_obj$curve[, 1],
  precision = pr_obj$curve[, 2],
  threshold = pr_obj$curve[, 3]
) %>%
  mutate(f1 = 2 * precision * recall / (precision + recall))

pr_auc <- round(pr_obj$auc.integral, 3)

g1 <- pr_df %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_line(aes(color = threshold), size = 1.2) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = str_c("Precision-Recall Curve (AUC = ", pr_auc, ")"),
    subtitle = str_c(model_name_best_f1_aktiva, " with ", method__best_f1_aktiva),
    x = "Recall",
    y = "Precision"
  ) +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    legend.position = "bottom"
  )

g2 <- pr_df %>%
  ggplot(aes(x = recall, y = precision, color = f1)) +
  geom_line(size = 1.2) +
  scale_color_viridis_c(option = "viridis") +
  labs(
    # title = "Precision-Recall Curve colored by F1 score",
    x = "Recall",
    y = NULL,
    color = "F1 score"
  ) +
  coord_cartesian(ylim = c(0,1))+
  theme(
    legend.position = "bottom"
  )

g1 + g2
```

```{r micro-pr-curve-llm-multi-300, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_selected <- df_multi %>% unnest(metrics)
df_filtered <- df_selected %>% filter(
  metric_type == "micro_minorities",
      loop == 0
) %>% 
  arrange(desc(f1_score))
model_rank <- as.integer(nrow(df_filtered)*0.8)

model_name_best_f1_aktiva <- df_filtered[model_rank, "model"]
method__best_f1_aktiva <- df_filtered[model_rank, "method"]

df_temp <- df_filtered[model_rank,"predictions"][[1]][[1]] %>% as_tibble()

pr_obj <- pr.curve(scores.class0 = df_temp$confidence_score[df_temp$match == 1],
                   scores.class1 = df_temp$confidence_score[df_temp$match == 0],
                   curve = TRUE)

# plot(pr_obj, color = "orange", main = "Precision-Recall Curve")

# Precision-Recall Curve with ggplot2

pr_df <- tibble(
  recall = pr_obj$curve[, 1],
  precision = pr_obj$curve[, 2],
  threshold = pr_obj$curve[, 3]
) %>%
  mutate(f1 = 2 * precision * recall / (precision + recall))

pr_auc <- round(pr_obj$auc.integral, 3)

g1 <- pr_df %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_line(aes(color = threshold), size = 1.2) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = str_c("Precision-Recall Curve (AUC = ", pr_auc, ")"),
    subtitle = str_c(model_name_best_f1_aktiva, " with ", method__best_f1_aktiva),
    x = "Recall",
    y = "Precision"
  ) +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    legend.position = "bottom"
  )

g2 <- pr_df %>%
  ggplot(aes(x = recall, y = precision, color = f1)) +
  geom_line(size = 1.2) +
  scale_color_viridis_c(option = "viridis") +
  labs(
    # title = "Precision-Recall Curve colored by F1 score",
    x = "Recall",
    y = NULL,
    color = "F1 score"
  ) +
  coord_cartesian(ylim = c(0,1))+
  theme(
    legend.position = "bottom"
  )

g1 + g2
```

### Term frequency based classifier {#tf-classifier}

RandomForest performs much better than a logistic regression Better
results with \* undersampling \* training on all types simultaniousely

#### Two predictors

Term frequency of nouns of the law about Aktiva Float freqency (floats
divided by word count)

#### Four predictors

Count of integers Count of dates

-   top 1
-   top k

low precision llm linked to position of correct page? numeric frequency?

```{python random-forest-2-predictors, include=TRUE, eval=FALSE, echo=echo_flag, warning=warning_flag, message=message_flag}
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df_train_us = pd.read_csv("../benchmark_results/page_identification/term_frequency_table.csv")

# Drop rows without ground truth
# df_train_us = df_word_counts.merge(df_truth, on=["filepath", "type"], how="left")
df_train_us["is_truth"] = (df_train_us["page"] == df_train_us["page_truth"]).astype(int)
df_train_us = df_train_us.dropna(subset=["page_truth"])

# Undersample the majority class (is_truth == 0)
df_true = df_train_us[df_train_us["is_truth"] == 1]
df_false = df_train_us[df_train_us["is_truth"] == 0]
df_false_undersampled = df_false.sample(n=len(df_true), random_state=42)
df_train_us_balanced = pd.concat([df_true, df_false_undersampled]).sample(frac=1, random_state=42).reset_index(drop=True)
# df_train_us_balanced

# Features and target
X = df_train_us_balanced[["term_frequency", "float_frequency"]].values
y = df_train_us_balanced["is_truth"].values

# Train-test split (70% train, 30% test)
X_train, X_test, y_train, y_test, df_train_split, df_test_split = train_test_split(
    X, y, df_train_us_balanced, test_size=0.3, random_state=42, stratify=y
)

# Train Random Forest model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
score = clf.score(X_train, y_train)
# print(f"Training accuracy: {score:.2%}")
score = clf.score(X_test, y_test)
# print(f"Test accuracy: {score:.2%}")

# Predict and rerank: get predicted probabilities for each page
df_train_split["score"] = clf.predict_proba(X_train)[:, 1]
df_test_split["score"] = clf.predict_proba(X_test)[:, 1]

# Add all not-chosen negatives from df_false to test split
df_false_unused = df_false.loc[~df_false.index.isin(df_false_undersampled.index)]
df_false_unused = df_false_unused.copy()
df_false_unused["score"] = clf.predict_proba(df_false_unused[["term_frequency", "float_frequency"]].values)[:, 1]
df_false_unused["rank"] = np.nan  # Not ranked yet

# Concatenate with test split
df_test_split = pd.concat([df_test_split, df_false_unused], ignore_index=True)

# For each group (filepath, type), sort by score descending
df_train_split["rank"] = df_train_split.groupby(["filepath", "type"])["score"].rank(ascending=False, method="first")
df_test_split["rank"] = df_test_split.groupby(["filepath", "type"])["score"].rank(ascending=False, method="first")
```

```{r top-n-accuracy-term-frequency, echo=echo_flag, warning=warning_flag, message=message_flag}
df_2_predictors_test <- read_csv("/home/simon/Documents/data_science/Thesis/benchmark_results/page_identification/term_frequency_results_2_predictors_test.csv") %>% 
  mutate(data_split = 'test', n_predictors = 2)
df_2_predictors_train <- read_csv("/home/simon/Documents/data_science/Thesis/benchmark_results/page_identification/term_frequency_results_2_predictors_train.csv") %>% 
  mutate(data_split = 'train', n_predictors = 2)
df_4_predictors_test <- read_csv("/home/simon/Documents/data_science/Thesis/benchmark_results/page_identification/term_frequency_results_4_predictors_test.csv") %>% 
  mutate(data_split = 'test', n_predictors = 4)
df_4_predictors_train <- read_csv("/home/simon/Documents/data_science/Thesis/benchmark_results/page_identification/term_frequency_results_4_predictors_train.csv") %>% 
  mutate(data_split = 'train', n_predictors = 4)

df_rf_results <- bind_rows(
  df_2_predictors_train, df_2_predictors_test,
  df_4_predictors_train, df_4_predictors_test
  )

max_rank = df_rf_results %>% filter(is_truth == 1) %>% pull(rank) %>% max()
results <- map_dfr(1:max_rank, function(i_rank) {
  df_rf_results %>% 
    filter(is_truth == 1) %>% 
    group_by(type, data_split, n_predictors) %>% 
    mutate(le = if_else(rank <= i_rank, 1, 0)) %>% 
    summarise(mean = mean(le), .groups = "drop") %>% 
    mutate(i_rank = i_rank)
})

results %>% ggplot() +
  geom_col(aes(x = i_rank, y = mean)) +
  facet_nested(type ~ data_split + n_predictors) +
  labs(
    x = "rank",
    y = "top n accuracy",
    # title = "Top n accuracy for different ranks, data splits and number of predictors"
  )

```

```{python forest-map-tf-2-predictors, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum('data_storage/rf-tf-2preds.sav'), results='hold'}
import shap
import pickle
import pandas as pd

import matplotlib.pyplot as plt
from sklearn.inspection import DecisionBoundaryDisplay
from matplotlib.colors import ListedColormap

predictors = [
    "term_frequency", 
    "float_frequency", 
    "date_count", 
    "integer_count"
]

clf = pickle.load(open('data_storage/rf-tf-2preds.sav', 'rb'))
df_test_split = pd.read_csv("data_storage/term_frequency_results_2_predictors_test.csv")


X = df_test_split[["term_frequency", "float_frequency"]].values

cm = plt.cm.RdBu
cm_bright = ListedColormap(["#FF0000", "#0000FF"])
x_min, x_max = X[:, 0].min() - 0.05, X[:, 0].max() + 0.05
y_min, y_max = X[:, 1].min() - 0.05, X[:, 1].max() + 0.05

plt.figure(figsize=(8, 6))
ax = plt.gca()
catch_this_message =DecisionBoundaryDisplay.from_estimator(
    clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.05
)

df_test_split_sorted = df_test_split.sort_values('is_truth')
X_test = df_test_split_sorted[["term_frequency", "float_frequency"]].values
y_test = df_test_split_sorted["is_truth"].values
score = clf.score(X_test, y_test)

# ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k", label="Train")
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k", label="Test")
catch_this_message =ax.set_xlim(x_min, x_max)
catch_this_message =ax.set_ylim(y_min, y_max)
ax.set_xlabel("term_frequency")
ax.set_ylabel("float_frequency")
ax.set_title(f"RandomForestClassifier (accuracy={score:.2f})")
plt.legend()
plt.show()
```

```{python random-forest-4-predictors, include=TRUE, eval=FALSE, echo=echo_flag, warning=warning_flag, message=message_flag}
import pandas as pd
import pickle
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df_train_us = pd.read_csv("../benchmark_results/page_identification/term_frequency_table.csv")

# Drop rows without ground truth
# df_train_us = df_word_counts.merge(df_truth, on=["filepath", "type"], how="left")
df_train_us["is_truth"] = (df_train_us["page"] == df_train_us["page_truth"]).astype(int)
df_train_us = df_train_us.dropna(subset=["page_truth"])

# Undersample the majority class (is_truth == 0)
df_true = df_train_us[df_train_us["is_truth"] == 1]
df_false = df_train_us[df_train_us["is_truth"] == 0]
df_false_undersampled = df_false.sample(n=len(df_true), random_state=42)
df_train_us_balanced = pd.concat([df_true, df_false_undersampled]).sample(frac=1, random_state=42).reset_index(drop=True)
# df_train_us_balanced

predictors = [
    "term_frequency", 
    "float_frequency", 
    "date_count", 
    "integer_count"
]

# Features and target
X = df_train_us_balanced[predictors].values # only better with date and integer counts; otherwise worse
y = df_train_us_balanced["is_truth"].values

# Train-test split (70% train, 30% test)
X_train, X_test, y_train, y_test, df_train_split, df_test_split = train_test_split(
    X, y, df_train_us_balanced, test_size=0.3, random_state=42, stratify=y
)

# Train Random Forest model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
catch_this_message =clf.fit(X_train, y_train)

# Predict and rerank: get predicted probabilities for each page
df_train_split["score"] = clf.predict_proba(X_train)[:, 1]
df_test_split["score"] = clf.predict_proba(X_test)[:, 1]

# Add all not-chosen negatives from df_false to test split
df_false_unused = df_false.loc[~df_false.index.isin(df_false_undersampled.index)]
df_false_unused = df_false_unused.copy()
df_false_unused["score"] = clf.predict_proba(df_false_unused[predictors].values)[:, 1]
df_false_unused["rank"] = np.nan  # Not ranked yet

# Concatenate with test split
df_test_split = pd.concat([df_test_split, df_false_unused], ignore_index=True)

# For each group (filepath, type), sort by score descending
df_train_split["rank"] = df_train_split.groupby(["filepath", "type"])["score"].rank(ascending=False, method="first")
df_test_split["rank"] = df_test_split.groupby(["filepath", "type"])["score"].rank(ascending=False, method="first")
```

```{python shap-tf-4-predictors, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum('data_storage/rf-tf-preds.sav')}
import shap
import pickle
import pandas as pd

predictors = [
    "term_frequency", 
    "float_frequency", 
    "date_count", 
    "integer_count"
]

clf = pickle.load(open('data_storage/rf-tf-4preds.sav', 'rb'))
df_test_split = pd.read_csv("data_storage/term_frequency_results_4_predictors_test.csv")

# Use the same predictors and trained RandomForestClassifier (clf) as in cell 20
explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(df_test_split[predictors].values)

# Plot summary for class 1 (is_truth == 1)
shap.summary_plot(shap_values[:,:,1], df_test_split[predictors].values, feature_names=predictors)
```

```{r micro-pr-curve-tf-4-predictors, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%"}
df_temp <- df_rf_results %>% filter(n_predictors == 4, data_split == "test")
pr_obj <- pr.curve(scores.class0 = df_temp$score[df_temp$is_truth == 1],
           scores.class1 = df_temp$score[df_temp$is_truth == 0],
           curve = TRUE)

# Precision-Recall Curve with ggplot2

pr_df <- tibble(
  recall = pr_obj$curve[, 1],
  precision = pr_obj$curve[, 2],
  threshold = pr_obj$curve[, 3]
) %>%
  mutate(f1 = 2 * precision * recall / (precision + recall))

pr_auc <- round(pr_obj$auc.integral, 3)

g1 <- pr_df %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_line(aes(color = threshold), size = 1.2) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = str_c("Precision-Recall Curve (AUC = ", pr_auc, ")"),
    x = "Recall",
    y = "Precision"
  ) +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    legend.position = "bottom"
  )

g2 <- pr_df %>%
  ggplot(aes(x = recall, y = precision, color = f1)) +
  geom_line(size = 1.2) +
  scale_color_viridis_c(option = "viridis") +
  labs(
    # title = "Precision-Recall Curve colored by F1 score",
    x = "Recall",
    # y = "Precision",
    color = "F1 score"
  ) +
  coord_cartesian(ylim = c(0,1))+
  theme(
    legend.position = "bottom"
  )

g1 + g2
```

### Comparison {#comparison-page-identification}

#### Prediction performance

F1 scores for llms are much higher

#### Energy usage and runtime

Multiclassification more effective than three times single
classification Combine term frequency with llm approach to limit page
range

## Table extraction {#table-extraction-introduction}

\ChapFrame

The second task to solve is: extract the data from the document.

Which tasks have there been? Which models have been used for which
ttask? What data has been used?

### Baseline: Regex

```{r table-extraction-regex-data-loading, echo=FALSE, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum('data_storage/table_extraction_regex.rds')}
df_table_extraction_regex <- readRDS("data_storage/table_extraction_regex.rds")

real_table_extraction_regex_total_performance_mean <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(percentage_correct_total) %>% mean()
real_table_extraction_regex_num_performance_mean <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(percentage_correct_numeric) %>% mean(na.rm = TRUE)
real_table_extraction_regex_NA_F1_mean <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(NA_F1) %>% mean()

synth_table_extraction_regex_total_performance_mean <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(percentage_correct_total) %>% mean()
synth_table_extraction_regex_num_performance_mean <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(percentage_correct_numeric) %>% mean(na.rm = TRUE)
synth_table_extraction_regex_NA_F1_mean <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(NA_F1) %>% mean(na.rm = TRUE)
```

The baseline for the table extraction task is set by an approach using
regular expressions on the text extract. The approach performs much
better[^05_results-13] on the synthetic dataset compared to the real
dataset (see Figure \@ref(fig:table-extraction-regex-performance)). Even
though, it does not perform perfectly and its performance is more
consistent on the text extracted with pymupdf compared to pdfium. Some
possible explanations are:

[^05_results-13]: A comparison of the numeric values over all methods
    can be found in section \@ref(comparing-table-extraction-methods).

-   a duplicated row name[^05_results-14]
-   numeric columns extracted separated from row names by extraction
    libraries
-   sums in the same row as the single values[^05_results-15]
-   with pdfium: missing white space[^05_results-16]
-   with pdfium: random line breaks[^05_results-17]

[^05_results-14]: The row *Geleistete Anzahlungen* can be found in two
    parts of the table and the simple approach just matches the numbers
    to the first found entry.

[^05_results-15]: In this case the \acr{regex} takes the sum as the
    value for the previous year.

[^05_results-16]: This can form unexpected numeric patterns or prevent
    the row names to be recognized.

[^05_results-17]: The approach takes care of line breaks between words,
    but not within. This leads to unrecognized row names as well.

You can find some examples for incorrect extracted texts in section
\@ref(regex-extraction-mistakes).

On the real dataset the approach shows a wider spread for the percentage
of correct extracted numeric values as well as a considerable number of
annual reports where the extraction did not work at all. Interestingly,
the used text extraction library has no noticeable influence on the real
dataset.

```{r table-extraction-regex-performance, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Performance overall and on numeric value extraction with regular expressions. Showing single scores for *percentage correct numeric* on real tables to explain wide boxes."}
df_table_extraction_regex %>%
  select(c(table_type, percentage_correct_numeric, percentage_correct_total, T_EUR, extraction_backend)) %>% 
  pivot_longer(cols = -c(table_type, T_EUR, extraction_backend)) %>% 
  ggplot() +
  geom_point(
    data= . %>% filter(table_type == "real_tables", name == "percentage_correct_numeric"),
    aes(x = table_type, y = value, alpha = extraction_backend), color = "#264DEB",
    shape = 4, position=position_jitterdodge(dodge.width=0.9, jitter.width = 0.2, jitter.height = 0.005)
  ) +
  geom_boxplot(aes(x = table_type, y = value, fill = extraction_backend), alpha = .3) +
  # geom_jitter(data= . %>% filter(table_type == "real_tables"), aes(x = table_type, y = value, color = T_EUR), alpha = .8, height = 0, shape = 4) +
  # facet_wrap(~name, ncol = 1) +
  scale_alpha_manual(values = c(1, 1), guide = "none") +
  scale_fill_manual(values = c("#94EB1F", "orange")) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_grid(~name)
```

The random line breaks result in some missed row names which is
reflected by the bigger spread for NA precision with pdfium on the
synthetic dataset (see Figure \@ref(fig:table-extraction-regex-NA)).
Nevertheless, the NA precision for the majority of the cases is perfect.
This is different with the real dataset. The NA precision is found to be
at only
`r df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(NA_precision) %>% mean() %>% round(2)`.

```{r table-extraction-regex-NA, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Performance on classification for missing values with regular expressions"}
# NAs for F1 are valid if there is not thinning
df_table_extraction_regex %>% select(c(table_type, NA_precision, NA_recall, NA_F1, T_EUR, extraction_backend)) %>% 
  pivot_longer(cols = -c(table_type, T_EUR, extraction_backend)) %>% 
  ggplot() +
  geom_boxplot(aes(x = table_type, y = value, fill = extraction_backend), alpha = .3) +
  scale_alpha_manual(values = c(1, 1)) +
scale_fill_manual(values = c("#94EB1F", "orange")) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_grid(~name)
```

##### Hypotheses

The formulated hypotheses have been evaluated visually using the
dependence and beeswarm plots from the shapviz library based on the
\acr{SHAP} values calculated with a random forest.

###### Real dataset

```{r real-table-extraction-regex-shap-loading, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", cache=TRUE, cache.extra = tools::md5sum('data_storage/h2o/real_table_extraction_regex_h2o_results_sample_50000_shap_2000.rds')}
results <- readRDS("data_storage/h2o/real_table_extraction_regex_h2o_results_sample_50000_shap_2000.rds")

shap_num <- results$perc_numeric$shap_values$rf %>% convert_shap_x()
p1a <- shap_num %>% sv_importance(show_numbers = TRUE) +
  labs(title = "% numeric correct") +
  coord_cartesian(xlim = c(0, 0.225))
p1b <- shap_num %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.35, 0.35))

shap_f1 <- results$NA_F1$shap_values$rf %>% convert_shap_x()
p2a <- shap_f1 %>% sv_importance(show_numbers = TRUE) +
  labs(title = "F1 NA") +
  coord_cartesian(xlim = c(0, 0.225))
p2b <- shap_f1 %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.35, 0.35))

shap_binom <- results$binomial$shap_values$rf %>% convert_shap_x()
p3a <- shap_binom %>% sv_importance(show_numbers = TRUE) +
  labs(title = "binomial") +
  coord_cartesian(xlim = c(0, 0.225))
p3b <- shap_binom %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.35, 0.35))

real_table_extraction_regex_shap_plot <- (p1a | p1b) /
  (p2a | p2b) /
  (p3a | p3b)
```

There are multiple hypotheses that don't get supported by the visual
results (see Figure \@ref(fig:real-table-extraction-regex-shap-plot)).
The pretty surprising results are:

1.  The visual separation of columns or rows has an effect on the text
    processing.
2.  It seems to have a positive effect on F1 and numeric correctness
    rate if the Passiva table is on the same page, even though it has no
    influence on the single predictions.

But one has to keep in mind that the number of data points on the
aggregated values for the test set of the real dataset is only
`r shap_num$X %>% nrow()`. So these findings are not strongly supporting
any interpretation at all. Furthermore, the found effects are not very
large - most below 5 %. Only the hypothesis for a positive influence of
a missing of a value for the binomial prediction gets solid support with
a mean absolute \acr{SHAP} value of over
`r floor((shap_binom %>% sv_importance(kind = "no") %>% .['missing'])*100)`
%. To get reliable results more tables have to be included which would
require additional manual encoding.

```{r hypotheses-and-results-real-table-extraction-regex-table, class.chunk="big-table", echo=echo_flag, warning=warning_flag, message=message_flag}
htmltools::includeHTML(
  textConnection(
    xml2::read_html("../benchmark_results/table_extraction/hypotheses_and_results_real_table_extraction_regex.html") %>%
      xml2::xml_find_first("//table") %>%
      as.character()
  )
)
```

###### Synthetic dataset

```{r synth-table-extraction-regex-shap-loading, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", cache=TRUE, cache.extra = tools::md5sum('data_storage/h2o/synth_table_extraction_regex_h2o_results_sample_50000_shap_2000.rds')}
results <- readRDS("data_storage/h2o/synth_table_extraction_regex_h2o_results_sample_50000_shap_2000.rds")

shap_num <- results$perc_numeric$shap_values$rf %>% convert_shap_x()
p1a <- shap_num %>% sv_importance(show_numbers = TRUE) +
  labs(title = "% numeric correct") +
  coord_cartesian(xlim = c(0, 0.15))
p1b <- shap_num %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.5, 0.5))

p1c <- shap_num %>% sv_dependence("extraction_backend") +
  labs(title = "dependence plot for extraction_backend")
p1d <- shap_num %>% sv_dependence("header_span", color_var = "extraction_backend") +
  labs(title = "dependence plot for header_span")


shap_f1 <- results$NA_F1$shap_values$rf %>% convert_shap_x()
p2a <- shap_f1 %>% sv_importance(show_numbers = TRUE) +
  labs(title = "F1 NA") +
  coord_cartesian(xlim = c(0, 0.15))
p2b <- shap_f1 %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.5, 0.5))

shap_binom <- results$binomial$shap_values$rf %>% convert_shap_x()
p3a <- shap_binom %>% sv_importance(show_numbers = TRUE) +
  labs(title = "binomial") +
  coord_cartesian(xlim = c(0, 0.15))
p3b <- shap_binom %>% sv_importance(kind = "beeswarm") +
  coord_cartesian(xlim = c(-0.5, 0.5))

synth_table_extraction_regex_shap_plot <- (p1a | p1b) /
  (p2a | p2b) /
  (p3a | p3b)

synth_table_extraction_regex_shap_num_details_plot <- p1c | p1d
```

Interpreting the visual results for the \acr{SHAP} analysis on the
synthetic dataset brought some interesting insides into the question
under which condition the two PDF extraction libraries perform
differently. These results can be treated as reliable since the model
has been trained with 50_000 rows and the \acr{SHAP} values have been
calculated on 2_000 rows each.

Very interestingly the number of columns is having an opposite effect
for the two libraries (see Figure
\@ref(fig:synth-table-extraction-regex-shap-num-details-plot) A).
Besides that often only pdfium struggled with some of the table
characteristics while pymupdf is not influenced by them (for an example
with header_span see Figure
\@ref(fig:synth-table-extraction-regex-shap-num-details-plot) B).

It might be worth noting that the row for *Anteile an verbundenen
Unternehmen* was rated to have a clear negative effect on the chance to
extract the correct value.

Since there has no synthetic data created where also the Passiva table
is present the result found with the real dataset can't be investigated
further. Also the question if visual separation is having an effect was
not studied, even though, creating such additional synthetic data would
be very easy with the current generation process and could be done in
future work. It would be interesting if the visual separation is cause
for the maleous text extractions of pdfium as well.

```{r hypotheses-and-results-synth-table-extraction-regex-table, class.chunk="big-table", echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
if (knitr::is_latex_output()) {
  # Render as kable for PDF
  # Extract table from HTML and convert to data.frame
  html_table <- xml2::read_html("../benchmark_results/table_extraction/hypotheses_and_results_synth_table_extraction_regex.html") %>%
    xml2::xml_find_first("//table")
  df <- rvest::html_table(html_table, fill = TRUE)
  kable(df, format = "latex", booktabs = TRUE, longtable = TRUE)
} else {
  # Render as HTML for HTML output
  htmltools::includeHTML(
    textConnection(
      xml2::read_html("../benchmark_results/table_extraction/hypotheses_and_results_synth_table_extraction_regex.html") %>%
        xml2::xml_find_first("//table") %>%
        as.character()
    )
  )
}
```

```{r synth-table-extraction-regex-shap-num-details-plot, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.width=8, fig.height=4, fig.cap="Showing the influence of the extraxtion library on the numeric text extraction task with synthetic data"}
synth_table_extraction_regex_shap_num_details_plot + 
  plot_annotation(tag_levels = 'A')
```

### Extraction with LLMs

```{r loading-table-extraction-datasets, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/real_table_extraction_llm.rds", "data_storage/real_table_extraction_synth.rds", "data_storage/real_table_extraction_azure.rds"))}
df_real_table_extraction <- readRDS("data_storage/real_table_extraction_llm.rds") %>% 
  filter(!model %in% c("deepseek-ai_DeepSeek-R1-Distill-Qwen-32B", 'google_gemma-3n-E4B-it')) %>% 
  mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_synth <- readRDS("data_storage/real_table_extraction_synth.rds") %>% 
  mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_azure <- readRDS("data_storage/real_table_extraction_azure.rds") %>% 
  mutate(model = gsub("^[^_]+_", "", model))

model_by_size <- c(
  'gemma-3-4b-it', #'gemma-3n-E4B-it', 
  "gemma-3-12b-it", "gemma-3-27b-it", 
  "Llama-3.1-8B-Instruct", "Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct",
  "Llama-4-Scout-17B-16E-Instruct", "Llama-4-Maverick-17B-128E-Instruct-FP8",
  "Mistral-8B-Instruct-2410", "Mistral-Small-3.1-24B-Instruct-2503",
  "Mistral-Large-Instruct-2411", "Qwen2.5-0.5B-Instruct",
  "Qwen2.5-1.5B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct",
  "Qwen2.5-14B-Instruct", "Qwen2.5-32B-Instruct", "Qwen2.5-72B-Instruct",
  "Qwen3-0.6B", "Qwen3-1.7B", "Qwen3-4B",
  "Qwen3-8B", "Qwen3-14B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-32B",
  "Qwen3-235B-A22B-Instruct-2507-FP8", "Qwen3-235B-A22B-Instruct-2507",
  # "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
  "Falcon3-10B-Instruct", "phi-4"
)

method_order <- c("top_n_rag_examples", "n_random_examples", "top_n_rag_examples_out_of_sample", "static_example", "zero_shot" )

norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors_real_table_extraction.csv") %>% 
  mutate(
    model_name = model_name %>% str_replace("/", "_")
  )
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml") | str_ends(filename, "vllm_batched.yaml")))

df_real_table_extraction <- df_real_table_extraction %>% left_join(
  norm_factors_few_examples %>% mutate(model_name = gsub("^[^_]+_", "", model_name)) %>% select(model_name, parameter_count, normalization_factor),
  by = c("model" = "model_name")
  )

df_overview <- bind_rows(df_real_table_extraction, df_real_table_extraction_azure) %>% 
  filter(out_of_company != TRUE | is.na(out_of_company), n_examples <= 3) %>% 
  filter(model %in% model_by_size) %>%
  mutate(
    model = factor(model, levels = model_by_size),
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples)))
    )

units_real_tables <- read_csv("../benchmark_truth/real_tables/table_characteristics.csv") %>% mutate(
  filepath = paste0('/pvc/benchmark_truth/real_tables/', company, '__', filename),
  T_EUR = (T_in_year + T_in_previous_year)>0,
  T_EUR_both = (T_in_year + T_in_previous_year)>1
) %>% select(filepath, T_EUR, T_EUR_both)

df_real_table_extraction_synth <- df_real_table_extraction_synth %>% left_join(units_real_tables)

# real

zero_shot_stars <- df_real_table_extraction %>% filter(method == "zero_shot") %>% group_by(model) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>real_table_extraction_regex_total_performance_mean, mean_num>real_table_extraction_regex_num_performance_mean, mean_F1>real_table_extraction_regex_NA_F1_mean)

static_example_stars <- df_real_table_extraction %>% filter(method == "static_example") %>% group_by(model) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>real_table_extraction_regex_total_performance_mean, mean_num>real_table_extraction_regex_num_performance_mean, mean_F1>real_table_extraction_regex_NA_F1_mean)

underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example')) %>% group_by(model, method) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(mean_total<real_table_extraction_regex_total_performance_mean, mean_num<real_table_extraction_regex_num_performance_mean, mean_F1<real_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)

super_underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example'), n_examples>1) %>% group_by(model, method) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(mean_total<real_table_extraction_regex_total_performance_mean, mean_num<real_table_extraction_regex_num_performance_mean, mean_F1<real_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)
```

-   confidence usable to head for user checks?
-   not handled new entries
-   five examples bring not much more, but a little
-   random forest / SHAP

#### Real tables only

For the table extraction task
`r length(unique(df_real_table_extraction$model))+2` open source models
have been benchmarked[^05_results-18]. The results are presented in
Figure \@ref(fig:table-extraction-llm-performance-total-overview),
\@ref(fig:table-extraction-llm-performance-numeric-overview) and
\@ref(fig:table-extraction-llm-f1-overview)).

[^05_results-18]: The models *deepseek-ai_DeepSeek-R1-Distill-Qwen-32B*
    and *google_gemma-3n-E4B-it* have been tested as well but don't get
    presented as they never performed anywhere beyond random guessing.

Most models need a context learning approach to beat the performance of
the regular expression approach at total and numeric correctness rate
and F1 score. Only `r nrow(zero_shot_stars)` models perform better
without any guidance[^05_results-19] (see Table
\@ref(tab:zero-and-synth-performing-models-tab)).
`r nrow(static_example_stars)` models achieved an performance better as
the regex baseline using the approach to learn with a fixed example from
the synthetic dataset.

[^05_results-19]: There is an external guidance through the provided
    xgrammar template but it is not communicated to the model in a
    promt.

In contrast: most of the models achieved a better performance than the
regex baseline when they were provided with one or more examples from
real *Aktiva* tables. Just `r nrow(super_underperformer)` don't achieve
a better value even with three or five realistic examples (see Table
\@ref(tab:super-underperforming-models-tab)). Here we find the smallest
models with less than 2B parameters which don't achieve a consistence
performance no matter how many examples they get. But we also find
models that start to perform bad if they get a too long context with too
many examples like the very recent and large model Llama 4 Maverick.

With one and three examples the performance within one model family is
positive correlated with the number of parameters the models have. Once
the 4B parameters are passed the improvements get less and less getting
closer to a perfect performance but never reaching it on all documents.
Table \@ref(tab:table-metrics-llm-real-table-extraction) shows the mean
performance for the best model-method approach for each model family.
Most of the top performing model-method combinations rely on the maximum
number of examples provided. Only the Llama-3 and Falcon3 model perform
best with three examples[^05_results-20].

[^05_results-20]: Phi4 also perfroms best with three examples. But this
    is the maximum it can process due to a limited context length.

```{r zero-and-synth-performing-models-tab, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
zero_shot_stars %>% full_join(static_example_stars, join_by(model), suffix = c("_zero_shot", "_static_example")) %>% select(model, starts_with("mean_total")) %>% 
mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", round(., 3), "**"),
      round(., 3)
    )
  ) %>% 
  render_table(
    alignment = "lrr",
    caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that perform well without or with little context learning",
    ref = opts_current$get("label"), dom="t")
```

```{r super-underperforming-models-tab, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
super_underperformer %>% select(model, method, mean_total) %>% ungroup() %>% # mutate(model = str_replace_all(model, "_", " ")) %>% 
mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", round(., 3), "**"),
      round(., 3)
    )
  ) %>% 
  render_table(
    alignment = "llr",
    caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that worse than the regex baselin with 3 or 5 examples for in-context learning",
    ref = opts_current$get("label"), dom="t")
```

```{r table-metrics-llm-real-table-extraction, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis"}
df_real_table_extraction %>% group_by(model, method) %>% mutate(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
  arrange(desc(mean_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>% 
  select(model_family, model, method_family, n_examples, mean_total) %>% arrange(desc(mean_total)) %>% 
  mutate(mean_total = round(mean_total, 3)) %>% 
  render_table(caption = "Comparing best mean table extraction performance with real 'Aktiva' dataset for each model family", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
```

```{r table-metrics-llm-real-table-extraction-small, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis"}
df_real_table_extraction %>% group_by(model, method) %>% filter(parameter_count<17) %>% mutate(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
  arrange(desc(mean_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>% 
  select(model_family, model, method_family, n_examples, mean_total) %>% arrange(desc(mean_total)) %>% 
  mutate(mean_total = round(mean_total, 3)) %>% 
  render_table(caption = "Comparing best mean table extraction performance with real 'Aktiva' dataset for each model family for models with less than 17B parameters", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
```

Based on a small sample of
`r df_overview %>% select(filepath) %>% filter(str_detect(filepath, "Statistik")) %>% unique() %>% nrow()`
documents by the *Amt für Statistik Berlin-Brandenburg* it seems that
there is support for the hypothesis, that providing Aktiva tables from
the same company in in-context learning, is improving the results. This
is especially noticeable for models with very few parameters and when
providing only a single example. This seems intuitive, since there the
potential for possibilities is much bigger. Figure
\@ref(fig:table-extraction-llm-performance-total-overview-out-of-company)
shows that on this limited sample

-   the improvement is bigger for Qwen 3 than for Qwen 2.5
-   Googles gemma 27b and GPT 4.1 mini could overcome an unnoticed issue
    with the extraction with just one example.
-   the effect of being overwhelmed by a too rich context with LLamas
    Maverick model could get reduced a bit.

To examine the question, if the reported confidence score of the
responses can be used, to flag the predicted values as potentially
wrong. Again, Figure \@ref(fig:table-extraction-llm-confidence) shows,
that Qwen 3 reports very high confidence values no matter if the results
are correct or not. With the Mistral model we find a wider range of
confidences given and for wrong results lower confidence is reported.

Figure \@ref(fig:table-extraction-llm-confidence-lm) shows, that the
chance to make an mistake by believing the prediction is rising with
lower confidence. The chance to make a mistake is higher for predictions
of numeric values than for believing a value is not present in the
table. The chance to make such a mistake is higher using the confidence
reported by Qwen 3.

```{r table-extraction-llm-confidence, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Comparing the reported confidence scores for the table extraction task on real dataset for the Mistral and Qwen 3 with 8B parameters.", cache=TRUE, cache.extra = tools::md5sum("data_storage/real_table_extraction_llm.rds")}
confidence_vs_truth <- df_real_table_extraction %>% 
  filter(model %in% c("Ministral-8B-Instruct-2410", "Qwen3-8B")) %>% 
  group_by(method, model) %>% mutate(
    mean_percentage_correct_total = mean(percentage_correct_total, na.rm=TRUE), .before = 1
    ) %>% group_by(model) %>% 
  arrange(desc(mean_percentage_correct_total)) %>% 
  slice_max(mean_percentage_correct_total, n = 1, with_ties = TRUE) %>% 
  mutate(predictions_processed = map(predictions, ~{
    .x %>% 
      select(-"_merge") %>% 
      mutate(
        match = (year_truth == year_result) | (is.na(year_truth) & is.na(year_result)),
        confidence = confidence_this_year,
        truth_NA = is.na(year_truth),
        predicted_NA = is.na(year_result),
        .before = 4
      ) %>% nest(
        tuple_year = c(match, confidence, truth_NA, predicted_NA)
      ) %>% 
      mutate(
        confidence = confidence_previous_year,
        match = (previous_year_truth == previous_year_result) | (is.na(previous_year_truth) & is.na(previous_year_result)),
        truth_NA = is.na(previous_year_truth),
        predicted_NA = is.na(previous_year_result),
        .before = 4
      ) %>% nest(
        tuple_previous_year = c(match, confidence, truth_NA, predicted_NA)
      ) %>% select(
        -c(year_truth, previous_year_truth, year_result, previous_year_result,
           confidence_this_year, confidence_previous_year)
      ) %>% 
      pivot_longer(-c("E1", "E2", "E3")) %>% 
      unnest(cols = value) %>% mutate(
        match = if_else(is.na(match), FALSE, match)
      )
  })) %>% 
  unnest(predictions_processed) %>% mutate(
    match = factor(match, levels = c(F, T)),
    truth_NA = factor(truth_NA, levels = c(F, T))
  )

confidence_vs_truth %>% ggplot() +
  geom_boxplot(
    aes(x = match, y = confidence, fill = truth_NA), 
    position = position_dodge2(preserve = "single")) +
  scale_fill_discrete(drop = FALSE) +
  scale_x_discrete(drop = FALSE) +
  facet_grid(~ model)
```

```{r table-extraction-llm-confidence-lm, echo=echo_flag, warning=warning_flag, message=message_flag, fig.height=3, out.width="100%", fig.cap="Estimating the relative frequency to find a wrong extraction result over different confidence intervals"}
confidence_vs_truth %>%
  mutate(
    conf_interval = cut(confidence, breaks = seq(0, 1, by = 0.05), include.lowest = TRUE),
    conf_center = as.numeric(sub("\\((.+),(.+)\\]", "\\1", levels(conf_interval))[conf_interval]) + 0.005
  ) %>%
  group_by(conf_center, predicted_NA, model) %>%
  summarize(
    n_true = sum(match == TRUE, na.rm = TRUE),
    n_false = sum(match == FALSE, na.rm = TRUE),
    total = n_true + n_false,
    chance_false = if_else(total > 0, n_false / total * 100, NA_real_),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = conf_center, y = chance_false, color = predicted_NA)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Confidence Interval Center", y = "Chance False (%)", color = "Predicted NA") +
  coord_cartesian(ylim = c(0, 100), xlim = c(0,1)) +
  facet_grid(~ model)
```

##### Hypotheses

The formulated hypotheses have been evaluated visually using the
dependence and beeswarm plots from the shapviz library based on the
\acr{SHAP} values calculated with a random forest.

```{r real-table-extraction-llm-shap-loading, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", cache=TRUE, cache.extra = tools::md5sum('data_storage/h2o/real_table_extraction_h2o_results_sample_50000_shap_2000.rds')}
results <- readRDS("data_storage/h2o/real_table_extraction_h2o_results_sample_50000_shap_2000.rds")

shap_num <- results$perc_numeric$shap_values$rf %>% convert_shap_x()
p1a <- shap_num %>% sv_importance(show_numbers = TRUE, max_display = 25) +
  labs(title = "% numeric correct") +
  coord_cartesian(xlim = c(0, 0.12))
p1b <- shap_num %>% sv_importance(kind = "beeswarm", max_display = 25) +
  coord_cartesian(xlim = c(-0.5, 0.35))
p1c <- shap_num %>% sv_dependence("n_examples")
p1d <- shap_num %>% sv_dependence("T_in_year")

shap_f1 <- results$NA_F1$shap_values$rf %>% convert_shap_x()
p2a <- shap_f1 %>% sv_importance(show_numbers = TRUE, max_display = 25) +
  labs(title = "F1 NA") +
  coord_cartesian(xlim = c(0, 0.12))
p2b <- shap_f1 %>% sv_importance(kind = "beeswarm", max_display = 25) +
  coord_cartesian(xlim = c(-0.5, 0.35))

shap_binom <- results$binomial$shap_values$rf %>% convert_shap_x()
p3a <- shap_binom %>% sv_importance(show_numbers = TRUE, max_display = 25) +
  labs(title = "binomial") +
  coord_cartesian(xlim = c(0, 0.12))
p3b <- shap_binom %>% sv_importance(kind = "beeswarm", max_display = 25) +
  coord_cartesian(xlim = c(-0.5, 0.35))

shap_binom <- results$confidence$shap_values$rf %>% convert_shap_x()
p4a <- shap_binom %>% sv_importance(show_numbers = TRUE, max_display = 25) +
  labs(title = "confidence") +
  coord_cartesian(xlim = c(0, 0.12))
p4b <- shap_binom %>% sv_importance(kind = "beeswarm", max_display = 25) +
  coord_cartesian(xlim = c(-0.5, 0.35))

real_table_extraction_llm_shap_plot <- (p1a | p1b) /
  (p2a | p2b) /
  (p3a | p3b) /
  (p4a | p4b)

real_table_extraction_llm_shap_num_details_plot <- p1c | p1d
```

Even though the samples size of Aktiva tables did not increase, the
available training, test and \acr{SHAP} sample size is much larger,
because the experiment has been repeated with different models and
methods. Thus, the interpretations based on the visual evaluation (see
Figure \@ref(fig:real-table-extraction-llm-shap-plot))) are more
reliable for model and method specific predictors. Since there is one
Aktiva example for every company files were found for they might even be
generalizable for this population. But one has to keep in mind that
there have been more Aktiva tables for *Amt Stat BBB* which might nudge
the results a bit.

The results assign much more influence on model and method specific
attributes than on the table specific attributes. The importance of the
table attributes are as low as found with the regular expresion
approach. Only for the binomial prediction we find the predictor
*missing* to get assigned more importance than to all model and method
specific attributes. Same is true for the *label* that is having the
highest influence on the reported confidence. Nevertheless, in the case
of the binomial prediction there is half of the predictors *missing* and
*label* importance shifted to model and method specific predictors.

Again, multiple hypotheses don't get supported by the visual results.
The surprising results are:

1.  In general more examples are helpful except for Llamas Maverick
    model that performs poorly with five examples. But this effect is
    only noticeable with the aggregated metrics nor for the case wise
    binomial evaluation.
2.  The number of columns has a negative effect on the performance but
    no effect on the reported confidence.
3.  There was no negative effect found if the *Passiva* table is on the
    same page as the Aktiva table.
4.  Larger models start to report less confidence again. This is not
    unexpected for the Mistral model but was surprising for the largest
    Qwen 3 model. (Discussion: New Generation? Aktive paramaters count?
    Irrelevant because not well distinguishing?)
5.  It not only influences the the performance to extract the correct
    numeric value from a row where there are additional sums present but
    also the F1 score.

Two interesting details found while inspecting the dependence plots for
the metric *percentage_numeric_correct* are (see Figure
\@ref(fig:real-table-extraction-llm-shap-num-details-plot) A) that the
bad performance of LLamas Maverick with five examples is easily
spottable and that the negative effect of *T_in_year* might be caused by
an interaction with *vis_separated_rows* completely (see Figure
\@ref(fig:real-table-extraction-llm-shap-num-details-plot) B). To
investigate the second finding one would need tables where the uni is
present in the year column and having no visual separation of the rows
at the same time. Synthetic data potentiall could help to answer such
questions.

```{r hypotheses-and-results-real-table-extraction-llm-table, class.chunk="big-table", echo=echo_flag, warning=warning_flag, message=message_flag}
htmltools::includeHTML(
  textConnection(
    xml2::read_html("../benchmark_results/table_extraction/hypotheses_and_results_real_table_extraction_llm.html") %>%
      xml2::xml_find_first("//table") %>%
      as.character()
  )
)
```

```{r real-table-extraction-llm-shap-num-details-plot, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.width=8, fig.height=4, fig.cap="Showing the influence of many examples on Llama 4 Maverick (A) and interaction between *T in year* and *vis separated rows* (B)"}
real_table_extraction_llm_shap_num_details_plot + 
  plot_annotation(tag_levels = 'A')
```

##### GPT

```{r, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/real_table_extraction_llm.rds", "data_storage/real_table_extraction_azure.rds"))}
model_by_size_gpt <- c(
  "Qwen3-0.6B", "Qwen3-8B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-235B-A22B-Instruct-2507",
 "gpt-4.1-nano", "gpt-4.1-mini", "gpt-5-mini",
 "gpt-oss-20b", "gpt-oss-120b", "gpt-4.1"
)

df_overview_gpt <- bind_rows(df_real_table_extraction, df_real_table_extraction_azure) %>% 
  filter(out_of_company == TRUE | is.na(out_of_company), n_examples <= 3) %>% 
  filter(model %in% model_by_size_gpt) %>%
  mutate(
    model = factor(model, levels = model_by_size_gpt),
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples)))
    ) %>% mutate(
      n_predicted_values = NA_true_positive+NA_false_positive+NA_false_negative+NA_true_negative
    )

perc_wrong_prediction_count <- round((df_overview_gpt %>% filter(!str_detect(model, "Qwen"), n_predicted_values != 58) %>% nrow()/df_overview_gpt %>% filter(!str_detect(model, "Qwen")) %>% nrow()) * 100, 1)
```

Even though a lot of documents to process at \acr{RHvB} will not be
public and thus must not be processed on public cloud infrastructure,
the performance of models like OpenAI's GPT or Google's Gemini are
interesting benchmark references within this thesis and for comparing
these findings with other papers results. Therefore for this thesis the
public available versions of annual reports have been used instead of
the ones used internally or for public administration purposes. Those
public available reports often are visually more appealing and more
heterogeneous in their structure.

As a reference to compare the performance of OpenAI's models with the
results of four Qwen 3 models are shown as well (see Figure
\@ref(fig:table-extraction-llm-performance-total-gpt),
\@ref(fig:table-extraction-llm-performance-numeric-gpt) and
\@ref(fig:table-extraction-llm-f1-gpt)). Surprisingly gpt-5-mini is
almost performing as good as the top Qwen 3 model and gpt-4.1. But
besides gpt-4.1-nano and Qwen3-0.6B all models perform pretty well with
the random or top-rag example methods. The ranking for the best
model-method combination can be found in Table
\@ref(tab.table-extraction-llm-performance-total-gpt-ranking). Since
gpt-4.1 costs five times more than gpt-4.1-mini (see Table
\@ref{tab:costs-azure}) it seems reasonable to prefer the smaller model
for this specific task.

Costs for gpt-5-mini not shown in Azure yet. :(

The author was not able to get OpenAI's models to stick to the provided
\acr{json} schema strictly. Passing the \acr{ebnf} grammar did not work
at all. This means that with gpt-4.1-nano there have been
`r df_overview_gpt %>% filter(n_predicted_values == 0) %>% nrow()`
predictions that have been completely empty. Overall there have been
`r perc_wrong_prediction_count` % of the responses of OpenAI's models
that were compatible with the schema but had a wrong number of rows
predicted (see Figure
\@red(fig:table-extraction-llm-prediction-count-gpt)).

Using gpt-5-nano and gpt-5-chat for the table extraction task was not
working. With gpt-5-nano the answers were not respecting the provided
grammar. Running gpt-5-chat resulted in the error informing that a
*json_schema* can't be used with this model. With gpt-5-mini the very
approach worked flawless. Running gpt-oss-20b with the vllm offline
inference framework was possible and the new harmony output format could
be processed after minor code changes for most
approaches[^05_results-21]. With a gpt-oss-120b instance hosted on Azure
the guided decoding worked flawless.

[^05_results-21]: With the static example approach there have been 24
    files where the resonse could not get parsed into valid \acr{json}.
    With the other approaches there are one to four unparsable
    responses.

```{r table-extraction-llm-performance-total-gpt-ranking, echo=echo_flag, warning=warning_flag, message=message_flag}
df_overview_gpt %>% group_by(model, method) %>% summarise(mean_percentage_correct_total = mean(percentage_correct_total)) %>% arrange(desc(mean_percentage_correct_total)) %>% slice_head(n = 1) %>% ungroup() %>% arrange(desc(mean_percentage_correct_total)) %>% mutate(mean_percentage_correct_total = round(mean_percentage_correct_total, 2)) %>% rename("mean correct total" = mean_percentage_correct_total) %>% kbl()
```

```{r table-extraction-llm-performance-total-gpt, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", out.height="95%", out.extra='keepaspectratio', fig.height=10, fig.cap="Comparing the percentage of correct predictions overall for OpenAi's LLMs with some Qwen 3 models"}
df_overview_gpt %>% 
  ggplot() +
  geom_hline(yintercept = real_table_extraction_regex_total_performance_mean, linetype = "dashed") +
  geom_boxplot(aes(x = model, y = percentage_correct_total, fill = model_family)) +
  # geom_jitter(aes(x = model, y = percentage_correct_total)) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_nested(method_family + n_examples ~ .) +
  theme(
    axis.title.x = element_blank()
  )
```

```{r table-extraction-llm-performance-numeric-gpt, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", out.height="95%", out.extra='keepaspectratio', fig.height=10, fig.cap="Comparing the percentage of correct numeric predictions for OpenAi's LLMs with some Qwen 3 models"}
df_overview_gpt %>% 
  ggplot() +
  geom_hline(yintercept = real_table_extraction_regex_num_performance_mean, linetype = "dashed") +
  geom_boxplot(aes(x = model, y = percentage_correct_numeric, fill = model_family)) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_nested(method_family + n_examples ~ .) +
  theme(
    axis.title.x = element_blank()
  )
```

```{r table-extraction-llm-f1-gpt, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", out.height="90%", out.extra='keepaspectratio', fig.height=10, , fig.cap="Comparing the F1 score for predicting the missingness of a value for OpenAi's LLMs with some Qwen 3 models. The green crosses indicate results where a model has predicted only numeric values even though there have been missing values."}
df_overview_gpt %>% 
  mutate(NA_F1 = if_else(is.na(NA_F1), 0, NA_F1)) %>% 
  ggplot() +
  geom_hline(yintercept = real_table_extraction_regex_NA_F1_mean, linetype = "dashed") +
  geom_boxplot(aes(x = model, y = NA_F1, fill = model_family)) +
  geom_jitter(data = df_overview_gpt %>% filter(is.na(NA_F1)), aes(x = model, y = 0), height = 0, color = "green", alpha = .5, shape = 4) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_nested(method_family + n_examples ~ .) +
  theme(
    axis.title.x = element_blank()
  )
```

```{r table-extraction-llm-prediction-count-gpt, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Showing the number of predictions OpenAI's models made."}
df_overview_gpt %>% filter(n_predicted_values != 58) %>% 
  mutate(color_code = if_else(n_predicted_values==0, "zero", if_else(n_predicted_values > 58, "too many", "too little"))) %>% 
  ggplot() +
  geom_histogram(aes(x = n_predicted_values, fill = color_code)) + facet_grid(model~.)
```

```{r costs-azure, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/real_table_extraction_llm.rds", "data_storage/real_table_extraction_azure.rds"))}
costs_azure <- read_csv("../CostManagement_master-thesis_2025.csv")

token_prop <- df_real_table_extraction %>% group_by(model, method, n_examples) %>% summarize(
    request_tokens_total = sum(request_tokens[[1]])) %>% 
    group_by(method, n_examples) %>% 
    summarize(mean = mean(request_tokens_total, na.rm = TRUE)) %>% mutate(five_examples = n_examples == 5) %>% group_by(five_examples) %>% summarise(sum = sum(mean))

five_ex_tokens <- token_prop %>% filter(five_examples == TRUE) %>% pull(sum)
other_tokens <- token_prop %>% filter(five_examples == FALSE) %>% pull(sum)

costs_azure %>% mutate(
  Cost_all_tasks = Cost,
  Cost_all_tasks = if_else(Meter == "gpt 4.1 Inp glbl Tokens", Cost_all_tasks+Cost_all_tasks*five_ex_tokens/other_tokens, Cost_all_tasks),
  Cost_all_tasks = if_else(Meter == "gpt 4.1 Outp glbl Tokens", Cost_all_tasks+Cost_all_tasks*3/11, Cost_all_tasks)
  ) %>% mutate(
    Cost = round(Cost, 2),
    Cost_all_tasks = round(Cost_all_tasks, 2)
  ) %>% rename(
    Model = Meter
  ) %>% select(
    Model, Cost, Cost_all_tasks, Currency
  ) %>% kbl()
```

#### Synthetic tables only

```{r loading-synth-table-extraction-datasets, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/synth_table_extraction_llm.rds")), cache.lazy = FALSE}
df_synth_table_extraction <- readRDS("data_storage/synth_table_extraction_llm.rds") %>% 
  filter(!model %in% c("deepseek-ai_DeepSeek-R1-Distill-Qwen-32B", 'google_gemma-3n-E4B-it')) %>% 
  mutate(model = gsub("^[^_]+_", "", model)) %>% sample_frac(size = .1)

norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors_synth_table_extraction.csv") %>% 
  mutate(
    model_name = model_name %>% str_replace("/", "_")
  )
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml") | str_ends(filename, "vllm_batched.yaml")))

df_synth_table_extraction <- df_synth_table_extraction %>% left_join(
  norm_factors_few_examples %>% mutate(model_name = gsub("^[^_]+_", "", model_name)) %>% select(model_name, parameter_count, normalization_factor),
  by = c("model" = "model_name")
  )

# model_by_size <- c(
#   'gemma-3-4b-it', #'gemma-3n-E4B-it', 
#   "gemma-3-12b-it", "gemma-3-27b-it", 
#   "Llama-3.1-8B-Instruct", "Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct",
#   "Llama-4-Scout-17B-16E-Instruct", "Llama-4-Maverick-17B-128E-Instruct-FP8",
#   "Mistral-8B-Instruct-2410", "Mistral-Small-3.1-24B-Instruct-2503",
#   "Mistral-Large-Instruct-2411", "Qwen2.5-0.5B-Instruct",
#   "Qwen2.5-1.5B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct",
#   "Qwen2.5-14B-Instruct", "Qwen2.5-32B-Instruct", "Qwen2.5-72B-Instruct",
#   "Qwen3-0.6B", "Qwen3-1.7B", "Qwen3-4B",
#   "Qwen3-8B", "Qwen3-14B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-32B", "Qwen3-235B-A22B-Instruct-2507",
#   # "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
#   "Falcon3-10B-Instruct", "phi-4"
# )
# 
# method_order <- c("top_n_rag_examples", "n_random_examples", "top_n_rag_examples_out_of_sample", "static_example", "zero_shot" )

df_overview_synth <- df_synth_table_extraction %>% 
  mutate(
    model = factor(model, levels = model_by_size),
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples)))
  )

# synth

zero_shot_stars_synth <- df_synth_table_extraction %>% filter(method == "zero_shot") %>% group_by(model) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>synth_table_extraction_regex_total_performance_mean, mean_num>synth_table_extraction_regex_num_performance_mean, mean_F1>synth_table_extraction_regex_NA_F1_mean)

static_example_stars_synth <- df_synth_table_extraction %>% filter(method == "static_example") %>% group_by(model) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>synth_table_extraction_regex_total_performance_mean, mean_num>synth_table_extraction_regex_num_performance_mean, mean_F1>synth_table_extraction_regex_NA_F1_mean)

underperformer_synth <- df_synth_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example')) %>% group_by(model, method) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(any(mean_total<synth_table_extraction_regex_total_performance_mean, mean_num<synth_table_extraction_regex_num_performance_mean, mean_F1<synth_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)

super_underperformer_synth <- df_synth_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example'), n_examples>1) %>% group_by(model, method) %>% 
  reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(mean_total<synth_table_extraction_regex_total_performance_mean, mean_num<synth_table_extraction_regex_num_performance_mean, mean_F1<synth_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)

df_synth_top_performance <- df_synth_table_extraction %>% group_by(model, method) %>% mutate(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
  arrange(desc(mean_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>% 
  select(model_family, model, method_family, n_examples, mean_total) %>% arrange(desc(mean_total)) %>% 
  mutate(mean_total = round(mean_total, 3))

df_synth_top_performance_small <- df_synth_table_extraction %>% group_by(model, method) %>% filter(parameter_count < 17) %>% mutate(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
  arrange(desc(mean_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>% 
  select(model_family, model, method_family, n_examples, mean_total) %>% arrange(desc(mean_total)) %>% 
  mutate(mean_total = round(mean_total, 3))

n_better_as_regex <- df_synth_table_extraction %>% group_by(model, method) %>% summarise(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
  arrange(desc(mean_total))  %>% 
  mutate(
    better_than_regex =  mean_total>synth_table_extraction_regex_total_performance_mean
  ) %>% group_by(better_than_regex) %>% summarise(n())

n_better_as_regex_families <- df_synth_top_performance %>% mutate(
    better_than_regex =  mean_total>synth_table_extraction_regex_total_performance_mean
) %>% group_by(better_than_regex) %>% summarise(n())
```

Table \@ref(tab:table-metrics-llm-real-table-extraction-synth-context)
shows that for `r deframe(n_better_as_regex_families)['TRUE']` from
`r nrow(n_better_as_regex_families)` model families there is at least
one model-method combination that performed better than the regex
baseline. For the synthetic table extraction task the baseline is
`r synth_table_extraction_regex_total_performance_mean`.

Only `r deframe(n_better_as_regex)['TRUE']` from
`r nrow(n_better_as_regex)` model-method combinations performed better
than this baseline. There has been no model that performed better than
this baseline with the zero or static example approach.

span argument was not implemented correct in html tables and md :/

already just using 10 % of documents generated; and then 10 % of that
sum of all experiment results (factor 14) with random forest?

\@ref(fig:synth-table-extraction-llm-performance-total-overview)

```{r table-metrics-llm-real-table-extraction-synth-context, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis"}
df_synth_top_performance %>% 
  mutate(
    mean_total = if_else(
      mean_total>synth_table_extraction_regex_total_performance_mean,
      paste0("**", mean_total, "**"),
      as.character(mean_total)
      )
  ) %>% 
  render_table(caption = "Comparing best mean table extraction performance with synthetic 'Aktiva' dataset for each model family", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
```

```{r table-metrics-llm-real-table-extraction-synth-context-small, echo=echo_flag, warning=warning_flag, message=message_flag, results="asis"}
df_synth_top_performance_small %>% 
  mutate(
    mean_total = if_else(
      mean_total>synth_table_extraction_regex_total_performance_mean,
      paste0("**", mean_total, "**"),
      as.character(mean_total)
      )
  ) %>% 
  render_table(caption = "Comparing best mean table extraction performance with synthetic 'Aktiva' dataset for each model family for models with less than 17B parameters", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
```

###### Hypotheses

HTML and Markdown better but expected interaction effects mostly not
found - except: - columns help pdf - thinning least bad for pdf - pdf
worst with numbers that have currency units (short numbers, maybe no
1000er delimiter) - enumeration positive for pdf (and interaction with
log10 mult)

line breaks are no problem

zero shot gets confused by text around

Markdown might be even better than HTML

respecting units was bad - except for: Top n rag finds examples with
same currency units (shorter numbers more important than currnency in
header?)

log10 multiplier has many interaction effects

LLama 4 Maverick again problem with five examples

Positive column count effect (different for real data)

#### Extract from real tables with synthetic content

```{r preparing-synth-context-learning-data, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/real_table_extraction_llm.rds", "data_storage/real_table_extraction_synth.rds"))}
best_models_with_examples <- df_real_table_extraction_synth %>% filter(str_detect(method_family, "n_")) %>% group_by(model, method) %>% summarise(mean_synth = mean(percentage_correct_total)) %>% arrange(desc(mean_synth)) %>% slice_head(n = 1) %>% ungroup() %>% arrange(desc(mean_synth)) %>% left_join(
  df_real_table_extraction %>% group_by(model, method) %>% summarise(mean_real = mean(percentage_correct_total))) %>% 
  left_join(
    df_real_table_extraction %>% group_by(model, method) %>% summarise(mean_zero_shot = mean(percentage_correct_total)) %>% filter(method == "zero_shot") %>% select(-method)
  ) %>% 
  mutate_if(is.numeric, ~round(., 3))

best_mistral <- df_real_table_extraction_synth %>% filter(model_family == "mistralai") %>% group_by(model, method) %>% summarise(mean_synth = mean(percentage_correct_total)) %>% arrange(desc(mean_synth)) %>% slice_head(n = 1) %>% pull(mean_synth)

df_real_table_extraction_synth_respect_units <- df_real_table_extraction_synth %>% 
  mutate(n_col_T_EUR = T_EUR_both + T_EUR) %>% 
  mutate(
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples))),
    respect_units = !ignore_units
  )

confidence_vs_truth <- df_real_table_extraction_synth %>% 
  filter(method_family %in% c("top_n_rag_examples", "n_random_examples")) %>% 
  filter(model %in% c("Ministral-8B-Instruct-2410", "Qwen3-8B")) %>% 
  group_by(method, model) %>% mutate(
    mean_percentage_correct_total = mean(percentage_correct_total, na.rm=TRUE), .before = 1,
    respect_units = !ignore_units
  ) %>% group_by(respect_units, model) %>% 
  arrange(desc(mean_percentage_correct_total)) %>% 
  slice_max(mean_percentage_correct_total, n = 1, with_ties = TRUE) %>% 
  mutate(predictions_processed = map(predictions, ~{
    .x %>% 
      select(-"_merge") %>% 
      mutate(
        match = (year_truth == year_result) | (is.na(year_truth) & is.na(year_result)),
        confidence = confidence_this_year,
        truth_NA = is.na(year_truth),
        predicted_NA = is.na(year_result),
        .before = 4
      ) %>% nest(
        tuple_year = c(match, confidence, truth_NA, predicted_NA)
      ) %>% 
      mutate(
        confidence = confidence_previous_year,
        match = (previous_year_truth == previous_year_result) | (is.na(previous_year_truth) & is.na(previous_year_result)),
        truth_NA = is.na(previous_year_truth),
        predicted_NA = is.na(previous_year_result),
        .before = 4
      ) %>% nest(
        tuple_previous_year = c(match, confidence, truth_NA, predicted_NA)
      ) %>% select(
        -c(year_truth, previous_year_truth, year_result, previous_year_result,
           confidence_this_year, confidence_previous_year)
      ) %>% 
      pivot_longer(-c("E1", "E2", "E3")) %>% 
      unnest(cols = value) %>% mutate(
        match = if_else(is.na(match), FALSE, match)
      )
  })) %>% 
  unnest(predictions_processed) %>% mutate(
    match = factor(match, levels = c(F, T)),
    truth_NA = factor(truth_NA, levels = c(F, T))
  )

confidence_vs_truth_linear <- confidence_vs_truth %>%
  mutate(
    conf_interval = cut(confidence, breaks = seq(0, 1, by = 0.05), include.lowest = TRUE),
    conf_center = as.numeric(sub("\\((.+),(.+)\\]", "\\1", levels(conf_interval))[conf_interval]) + 0.005
  ) %>%
  group_by(conf_center, predicted_NA, respect_units, model) %>%
  summarize(
    n_true = sum(match == TRUE, na.rm = TRUE),
    n_false = sum(match == FALSE, na.rm = TRUE),
    total = n_true + n_false,
    chance_false = if_else(total > 0, n_false / total * 100, NA_real_),
    .groups = "drop"
  )
```

Table
\@ref(tab:comparing-extraction-performance-real-and-synth-context-learning)
shows that using real examples for in-context- learning is better than
using the created synthetic data. Nevertheless, it is improving the
overall performance for the table extraction task by almost 20 % for all
models but Google's gemma-3-12b-it. However, the performance difference
with and without in-context learning is the smallest for the
gemma-3-12b-it model as well. The spread of the performance is bigger
using synthetic in-context learning data for all models but Llama 3 8B
Instruct.

Any pattern?

```{r comparing-extraction-performance-real-and-synth-context-learning, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
best_models_with_examples %>% mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% 
  render_table(
    alignment = "llrrr",
    caption = "Comparing extraction performance for real Aktiva extraction task with synthetic and real examples for in-context learning with a zero shot approach",
    ref = opts_current$get("label"), dom="t")
```

```{r compare-real-table-extraction-by-context-type, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.height=10, fig.cap="Comparing table extraction perfromance for real Aktiva extraction task with synthetic and real examples for in-contect learning"}
bind_rows(
  df_real_table_extraction_synth %>% mutate(context = "synth"), 
  df_real_table_extraction %>% mutate(context = "real")
  ) %>% 
  filter(model %in% best_models_with_examples$model) %>% 
  filter(method_family %in% c("top_n_rag_examples", "n_random_examples")) %>% 
  mutate(
    # model = factor(model, levels = model_by_size),
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples)))
  ) %>% 
  ggplot() +
  geom_boxplot(aes(fill=context, y = percentage_correct_total), alpha = .5) +
  # scale_fill_manual(values = c("blue", "orange")) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  facet_nested(method_family+n_examples~model)
```

Table
\@ref(tab:comparing-extraction-performance-synth-context-learning-respecting-units-table)
shows, that synthetic data can be used for in-context learning in a task
where the currency units given for a table or specific
columns[^05_results-22]. Except for Llama 3.1 all models achieved much
better numeric extraction results for tables where currencies are given
for all columns. The model also achieved better for cases where there
was only a single column with units, even though there have been no
examples with units only for one column in the synthetic data. On the
other hand, being prompted to respect currency units decreased the
performance for tables where no units are given for all models but Llama
4 Schout. This decrease was highest for Qwen 3 and also higher than 10 %
for Mistral Large.

[^05_results-22]: Synthetic data is used here because the
    characterization, which real *Aktiva* table has units in which
    column, was created too late.

\@ref(fig:compare-effect-of-respecting-units-on-overall-performance),
\@ref(fig:compare-effect-of-respecting-units-on-numeric-performance) and
\@ref(fig:compare-effect-of-respecting-units-on-NA-F1)

Thus, synthetic data can be used to solve new tasks and substitute
missing data for rare classes.

Confidence with both tasks (respect or ignore units) the same.

```{r comparing-extraction-performance-synth-context-learning-respecting-units-table, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_real_table_extraction_synth_respect_units %>% filter(method_family %in% c("top_n_rag_examples", "n_random_examples")) %>% group_by(model, method, method_family, respect_units, n_col_T_EUR) %>% reframe(mean_num = mean(percentage_correct_numeric)) %>% pivot_wider(names_from = respect_units, values_from = mean_num, names_prefix = "units_") %>% mutate(delta = units_FALSE - units_TRUE) %>% select(-c(units_FALSE, units_TRUE)) %>% pivot_wider(names_from = n_col_T_EUR, values_from = delta, names_prefix = "n_cols_T_EUR_") %>% group_by(model) %>% summarise_if(is.numeric, mean) %>% mutate(across(is.numeric, ~round(., 2))) %>% mutate_if(
    is.numeric, 
    ~ifelse(
      . == min(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% 
  render_table(
    alignment = "lrrr",
    caption = "Comparing extraction performance for real Aktiva extraction task dependent on the promt addition to respect currency units",
    ref = opts_current$get("label"), dom="t")
```

```{r comparing-extraction-performance-synth-context-learning-respecting-units-histogram, echo=echo_flag, warning=warning_flag, message=message_flag}
# df_real_table_extraction_synth_respect_units %>% group_by(model, method, respect_units, n_col_T_EUR) %>% reframe(mean_num = mean(percentage_correct_numeric)) %>% pivot_wider(names_from = respect_units, values_from = mean_num, names_prefix = "units_") %>% mutate(delta = units_FALSE - units_TRUE) %>% select(-c(units_FALSE, units_TRUE)) %>% ggplot() + geom_histogram(aes(x = delta)) + facet_grid(model~n_col_T_EUR)
```

```{r table-extraction-llm-confidence-synth-context, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Comparing confidence for reported values in the table extraction perfromance for real Aktiva extraction task with synthetic examples for in-context learning for two models."}
# confidence_vs_truth %>% ggplot() +
#   geom_boxplot(
#     aes(x = match, y = confidence, fill = truth_NA), 
#     position = position_dodge2(preserve = "single")) +
#   scale_fill_discrete(drop = FALSE) +
#   scale_x_discrete(drop = FALSE) +
#   facet_nested(~model+respect_units)
```

```{r table-extraction-llm-confidence-synth-context-table, results='asis'}
confidence_vs_truth %>% group_by(model, method, method_family, respect_units, predicted_NA) %>% reframe(mean_conf = mean(confidence)) %>% pivot_wider(names_from = respect_units, values_from = mean_conf, names_prefix = "mean_conf_units_") %>% # mutate(delta = units_FALSE - units_TRUE) %>% select(-c(units_FALSE, units_TRUE)) %>% pivot_wider(names_from = n_col_T_EUR, values_from = delta, names_prefix = "n_cols_T_EUR_") %>% group_by(model) %>% summarise_if(is.numeric, mean) %>% 
  mutate(across(is.numeric, ~round(., 2))) %>% 
  render_table(
    alignment = "lllrr",
    caption = "Comparing extraction confidence for real Aktiva extraction task dependent on the promt addition to respect currency units. No difference in confidence appearent.",
    ref = opts_current$get("label"), dom="t")
```

Risk for false NAs less with synth data for Mistral but greater for
numeric values (both).

```{r table-extraction-llm-confidence-lm-synth-context, echo=echo_flag, warning=warning_flag, message=message_flag, out.width="100%", fig.cap="Estimating the relative frequency to find a wrong extraction result over different confidence intervals for predictions based on synthetic examples for in-context learning.", fig.height=3}
confidence_vs_truth_linear %>%
  filter(!respect_units) %>% 
  ggplot(aes(x = conf_center, y = chance_false, color = predicted_NA)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Confidence Interval Center", y = "Chance False (%)", color = "Predicted NA") +
  coord_cartesian(ylim = c(0, 100), xlim = c(0,1)) +
  facet_nested(~model#+paste("respect units: ", respect_units)
               )
```

##### Hypotheses

### Comparison {#comparing-table-extraction-methods}

Most models performe better on synth tables once they have enough
in-context examples. (Needing more für random examples thanwith
top-n-rad approach). Especially Llama 3 models show wider performance
spread even with three examples
\@ref(fig:comparing-table-extraction-performance-among-real-and-synth-aktiva-data)
