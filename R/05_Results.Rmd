# Results

## Page identification

As described in \@ref(text-extraction-benchmark) open source libraries have been used to extract the text from the annual reports.

### Baseline: Regex {#regex-page-identification}

```{r page-identification-regex-data-loading, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, cache=TRUE, cache.extra = tools::md5sum('scripts/page_identification_regex.R')}
source("scripts/page_identification_regex.R")
```

Building a sound regular expression often is an iterative process. In a first approach a very simple one was implemented. 

Comparing the differences in the metrics based on the different text extraction libraries it can be said that the extracted text is very similar but not identical. Since the resukts are not depending on the used text extraction library the *exhaustive regex restricted* has only been run with the fast text extraction library *pdfium*. The results of the regex based page identification are presented in the following tables.

* look into details where they differ and if it is because of a line break or whitespace ?

Due to the imbalanced distribution of the classes the accuracy is not a good metric to compare the performance of the different methods. The number of pages of interest is much smaller than the number of irrelevant pages. Therefore, precision, recall and F1 score are presented as well.

The regular expressions can be found in the appendix (see \@ref(regex-page-identification)).

General bad precision. Increasing recall degrades precision even further. number of pages positive identified total; used as subset for table identification task

```{r display-metrics-regex-page-identification-aktiva, echo=FALSE, results="asis"}
metric_summaries["Aktiva"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Aktiva'", ref="display-metrics-regex-page-identification-aktiva")
```

```{r display-metrics-regex-page-identification-passiva, echo=FALSE, results="asis"}
metric_summaries["Passiva"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Passiva'", ref="display-metrics-regex-page-identification-passiva")
```

```{r display-metrics-regex-page-identification-guv, echo=FALSE, results="asis"}
metric_summaries["GuV"][[1]] %>%
  mutate_if(
    is.numeric, 
    ~ifelse(
      . == max(., na.rm = TRUE),
      paste0("**", ., "**"),
      .
    )
  ) %>% arrange(desc(method)) %>% 
  render_table(alignment="llrrr", caption="Comparing page identification metrics for different regular expressions for classification task 'Gewinn- und Verlustrechnung'", ref="display-metrics-regex-page-identification-guv")
```

```{r display-metrics-plot-regex-page-identification, echo=FALSE, figwidth=8, fig.height=6, out.width="100%"}
metrics %>% bind_rows() %>%
  pivot_longer(
    cols = -c(package, method, classification_type),
    names_to = "metric",
    values_to = "value"
  ) %>%
  filter(metric %in% c("acc", "precision", "recall", "F1")) %>%
  ggplot() +
  geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0) +
  facet_grid(metric~classification_type) +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme(
    legend.position = "bottom"
  )
```

### Advanced techniques

#### Table of Contents understanding

```{r page-identification-toc-data-loading, echo=FALSE}
source("scripts/page_identification_toc_data_loading.R")
```

* calculate and add Qwen, Gemini or LLama results?

##### Text based

@li_extracting_2023 used the table of contents to identify the pages of interest. In their approach the table of contents is extracted from the text. Based on their observation, that the TOC that "ACFRs typically spans no more than the initial 165 lines of the converted document" (p. 20), they use the first 200 lines of text.

My expectation was to find the TOC within the first five pages. Often we find way less than 200 lines of text on the five first pages (see Figure \@ref(fig:page-identification-toc-histogram)). Some files are not machine readable without OCR and thus show zero lines in the first five pages as well.

```{r page-identification-toc-histogram, echo=FALSE, fig.width=8, fig.height=3, out.width="80%", fig.cap="Histogram of the number of lines in the first 5 pages of the annual reports"}
tibble(num_lines = num_lines) %>% ggplot() +
  geom_histogram(aes(x = num_lines), bins = 20) +
  labs(x = "Number of lines in the first 5 pages", y = "Count")
```

###### First five pages

A request to Mistral results in `r n_found_toc_5_pages` strings that should represent a table of contents among the first five pages [strings not checked in detail].

###### First 200 lines

A request to Mistral results in `r n_found_toc_200_lines` strings that should represent a table of contents among the first five pages [strings not checked in detail].



###### Machine readable TOC based

To limit the text and hopefully increase the quality of the input data one can work with the TOC representation embedded within the PDF files. From `r n_toc+n_no_toc` annual reports `r n_toc` files do have a machine readable separate table of contents and `r n_no_toc` do not have one.

One can see that correct predictions for the page range are more probable when the TOC has a medium number of entries. It is possible to drop PDFs with less than `r min_n_entries_max_correct` without loosing a single correct prediction. This means that for PDFs with TOC with less then `r min_n_entries_max_correct` entieres the LLM was not able to make a correct prediction. This is not surprising since neither *Bilanz* nor *Gewinn- und Verlustrechnung* are mentioned there.

Almost no influence if TOC is passed formated as markdown or json. With the json formated TOC it found two more correct page ranges (single test run). It was testes because the relation *page_number* heading and value might have been clearer in json for a linear working LLM.

```{r page-identification-toc-mr-degration, echo=FALSE, results="asis"}
df_toc_benchmark_mr_degration %>% ggplot() +
  geom_col(aes(x = n_entries, y = value, fill = correct)) +
  geom_line(aes(x = as.numeric(n_entries), y = 100*perc_correct, group = 1)) +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  facet_wrap(~type, ncol = 1)
```

##### Comparison of the different approaches

* toc analysis
* cleaned measures

```{r page-identification-toc-analysis, echo=FALSE, results="asis", out.width="100%", fig.cap="Comparing number of fount TOC and amount of correct and incorrect predicted page ranges"}
df_toc_benchmark %>% ggplot() +
  geom_bar(aes(x = type, fill = in_range)) +
  geom_text(
    data = df_toc_benchmark %>% filter(in_range == TRUE),
    aes(x = type, label = paste0(round(perc_correct, 2), "")),
    stat = "count",
    vjust = 1.2,
    color = "white"
  ) +
  geom_text(
    aes(x = type, label = paste0(round(1-perc_correct, 2), "")),
    stat = "count",
    vjust = 1.5,
    color = "white"
  ) +
  facet_wrap(~benchmark_type, nrow = 1) # +
  # theme(
  #   legend.position = "bottom"
  # )
```

```{r page-identification-toc-range_logprobs, echo=FALSE, results="asis"}
df_toc_benchmark %>% 
  rowwise() %>% 
  mutate(
    min_confidence = min(confidence_start_page, confidence_end_page),
    range = abs(end_page - start_page) + 1,
  ) %>% group_by(filepath, benchmark_type) %>%
  distance_confidence_plot() +
  facet_wrap(~benchmark_type, ncol = 1)
```

#### Classification with LLMs {#llm-page-identification}

```{r page-identification-llm-data-loading, echo=FALSE}
temp_list <- readRDS("data_storage/page_identification_llm.rds")
df_binary <- temp_list$df_binary
df_multi <- temp_list$df_multi
```

```{r}
binary_task <- list()
binary_task$n_models <- df_binary$model %>% unique() %>% length()
binary_task$n_model_families <- df_binary$model_family %>% unique() %>% length()
binary_task$n_method_families <- df_binary$method_family %>% unique() %>% length()
```

structured outputs forcing to answer with a *yes* or *no* for binary task or with *Aktiva*, *Passiva*, *GuV* or *other* for multi classification task

##### Binary classification

`r  binary_task$n_models` models from `r binary_task$n_model_families` haven been benchmarked among `r binary_task$n_method_families` methods

Most models have have been used till up to 3 examples for the context

[Probably gonna drop because models will have rerun in some hours] Some models only ran for the *Aktiva* classification task and crashed because of a conflict trying to access the vector database. If their scores were not promising they have not been run for the other classification tasks at all.

The best combination of model and method for each method family is presented in the following table. It is clear that the Google Gemma models are performing worst. This is surprising since they did a decent job in a similar task as described in section \@ref(#llm-table-detection).

```{r display-metrics-llm-page-identification-binary, echo=FALSE, results="asis"}
df_binary %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, classification_type) %>% 
  filter(f1_score == max(f1_score, na.rm = TRUE)) %>% 
  # mutate(
  #   n = n()
  # ) %>% filter(n > 1)
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, classification_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  ) %>% 
  render_table()
```

* f1
* multiple models
* best model detail (different methods / settings)

The experiments for best performing model, Ministral-8B-Instruct-2410, have been extended by methods with even more examples in the context.

```{r}
df_binary %>% filter(model == "mistralai_Ministral-8B-Instruct-2410", loop < 1) %>% 
  ggplot(aes(x = norm_runtime, y = f1_score)) +
  geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
  scale_shape(na.value = 15, guide = "legend") +
  geom_text(aes(label = n_examples)) +
  facet_grid(model~classification_type) +
  theme(legend.position = "bottom") +
  guides(
    color = guide_legend(ncol = 1, title.position = "top"),
    shape = guide_legend(ncol = 1, title.position = "top")
  )
```

Some models have run for multiple times to check if the results are stable
Earlier examples with supset have been run five times indicating stable results. Running the experiments up to tree times in this very task indicate this as well.

##### Multi classification

* f1
* multiple models
* best model detail (different methods / settings)

#### Term frequency based classifier

* top 1
* top k

### Comparison

Multiclassification more effective than three times single classification

#### F1

#### Energy usage and runtime

## Table extraction