---
editor_options: 
  markdown: 
    wrap: none
---

# Results {#results}

\ChapFrame[Results][bhtteal]

This chapter presents the main results for the two main and our side research questions of this thesis:

```{r, include=knitr::is_html_output(), results='asis', echo=FALSE}
cat("<ol class='rs-questions' style='--counter: 1;'><li>How can we use LLMs effectively to locate specific information in a financial report?</li><li>How can we use LLMs effectively to extract this information from the document?</li><li>Can we use additional information from the extraction process to guide the user on which values need to be checked and which can be trusted as they are?</li></ol>")
```

\begin{enumerate}[label={\textbf{Q\theenumi}}]
  \item How can we use LLMs effectively to locate specific information in a financial report?
  \item How can we use LLMs effectively to extract this information from the document?
  \item Can we use additional information from the extraction process to guide the user on which values need to be checked and which can be trusted as they are?
\end{enumerate}

Section \@ref(page-identification-introduction) presents the results for the first research question. Section \@ref(table-extraction-introduction) presents the results for the second question. Section \@ref(error-rate-guidance-results) shows the results for the side research question. Finally, we will summarize all results in Section \@ref(summary-results).

Each section will start with an overview about the specific sub tasks as well as about the models, methods and data used to investigate the research question.

This chapter focuses on the presentation of results, that answer the question, if an approach can be used to solve a task. We discuss additional findings in Chapter \@ref(discussion). The presentation of those additional results can be found in the appendix. We refer to the chapters of the appendix, that document our investigation and the results of the sub tasks in detail, at the beginning of each of the following sections.

## Page identification {#page-identification-introduction}

```{r page-identification-llm-data-loading, echo=echo_flag, warning=warning_flag, message=message_flag}
temp_list <- readRDS("data_storage/page_identification_llm.rds")
df_binary <- temp_list$df_binary %>% mutate(
        # model_family = sub("_.*", "", model),
        model_family = if_else(str_detect(model, "Qwen2"), "Qwen 2.5", model_family),
        model_family = if_else(str_detect(model, "Qwen3"), "Qwen 3", model_family),
        model_family = if_else(str_detect(model, "Llama-3"), "Llama-3", model_family),
        model_family = if_else(str_detect(model, "Llama-4"), "Llama-4", model_family),
        model = str_remove(model, "-0-9-1")
      )
df_multi <- temp_list$df_multi %>% mutate(
        # model_family = sub("_.*", "", model),
        model_family = if_else(str_detect(model, "Qwen2"), "Qwen 2.5", model_family),
        model_family = if_else(str_detect(model, "Qwen3"), "Qwen 3", model_family),
        model_family = if_else(str_detect(model, "Llama-3"), "Llama-3", model_family),
        model_family = if_else(str_detect(model, "Llama-4"), "Llama-4", model_family),
        model = str_remove(model, "-0-9-1")
      )

method_families <- c("zero_shot", "law_context", "top_n_rag_examples", "n_random_examples", 'n_rag_examples')

method_familiy_colors <- c(
  "zero_shot" = "#e41a1c", 
  "law_context" = "#377eb8", 
  "top_n_rag_examples" = "#4daf4a", 
  "n_random_examples" = "#984ea3", 
  'n_rag_examples' = "#ff7f00"
  )

model_by_size_classification <- c('google_gemma-3-4b-it', 'google_gemma-3n-E4B-it', "google_gemma-3-12b-it",
  "google_gemma-3-27b-it", "meta-llama_Llama-3.1-8B-Instruct", 
  "meta-llama_Llama-3.1-70B-Instruct", "meta-llama_Llama-3.3-70B-Instruct",
  "meta-llama_Llama-4-Scout-17B-16E-Instruct", "meta-llama_Llama-4-Maverick-17B-128E-Instruct-FP8",
  "mistralai_Ministral-8B-Instruct-2410", "mistralai_Mistral-Small-3.1-24B-Instruct-2503",
  "mistralai_Mistral-Large-Instruct-2411", "Qwen_Qwen2.5-0.5B-Instruct",
  "Qwen_Qwen2.5-1.5B-Instruct", "Qwen_Qwen2.5-3B-Instruct", "Qwen_Qwen2.5-7B-Instruct",
  "Qwen_Qwen2.5-14B-Instruct", "Qwen_Qwen2.5-32B-Instruct", "Qwen_Qwen2.5-72B-Instruct",
  "Qwen_Qwen3-8B", "Qwen_Qwen3-30B-A3B-Instruct-2507", "Qwen_Qwen3-32B", "Qwen_Qwen3-235B-A22B-Instruct-2507",
  "tiiuae_Falcon3-10B-Instruct", "microsoft_phi-4"
  ) %>% gsub("^[^_]+_", "", .)

df_binary <- df_binary %>% 
  mutate(
  n_examples = as.numeric(n_examples),
  n_examples = if_else(method_family == "zero_shot", 0, n_examples),
  n_examples = if_else(method_family == "law_context", 1, n_examples),
  method_family = factor(method_family, levels = method_families)
) %>% mutate(model = gsub("^[^_]+_", "", model)) %>% 
  filter(model %in% model_by_size_classification) %>% 
  ungroup() # %>% 
  # mutate(
  #   model = str_replace(model, "_vllm", ""),
  #   model_family = sub("_.*", "", model),
  #   model_family = if_else(str_detect(model, "Qwen2"), "Qwen 2.5", model_family),
  #   model_family = if_else(str_detect(model, "Qwen3"), "Qwen 3", model_family),
  #   model_family = if_else(str_detect(model, "Llama-3"), "Llama-3", model_family),
  #   model_family = if_else(str_detect(model, "Llama-4"), "Llama-4", model_family)
  # )

binary_task <- list()
binary_task$n_models <- df_binary$model %>% unique() %>% length()
binary_task$n_model_families <- df_binary$model_family %>% unique() %>% length()
binary_task$n_method_families <- df_binary$method_family %>% unique() %>% length()

top_performer_binary <- df_binary %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, classification_type) %>% 
  slice_max(n = 1, f1_score) %>% 
  arrange(desc(f1_score)) %>% 
  select(model_family, model, classification_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  )

top_performer_binary_median <- top_performer_binary %>% group_by(classification_type) %>% summarise(median = median(f1_score))
top_performer_binary_median_guv <- top_performer_binary_median %>% filter(classification_type == "GuV") %>% pull(median)
top_performer_binary_median_aktiva <- top_performer_binary_median %>% filter(classification_type == "Aktiva") %>% pull(median)
top_performer_binary_median_passiva <- top_performer_binary_median %>% filter(classification_type == "Passiva") %>% pull(median)

df_multi <- df_multi %>% 
  mutate(
  n_examples = as.numeric(n_examples),
  n_examples = if_else(method_family == "zero_shot", 0, n_examples),
  n_examples = if_else(method_family == "law_context", 1, n_examples),
  method_family = factor(method_family, levels = method_families)
) %>% mutate(model = gsub("^[^_]+_", "", model)) %>% 
  filter(model %in% model_by_size_classification) %>% 
  ungroup() #%>%
  # mutate(
  #   model = str_replace(model, "_vllm", ""),
  #   model_family = sub("_.*", "", model),
  #   model_family = if_else(str_detect(model, "Qwen2"), "Qwen 2.5", model_family),
  #   model_family = if_else(str_detect(model, "Qwen3"), "Qwen 3", model_family),
  #   model_family = if_else(str_detect(model, "Llama-3"), "Llama-3", model_family),
  #   model_family = if_else(str_detect(model, "Llama-4"), "Llama-4", model_family)
  # )

top_performer_multu <- df_multi %>% 
  unnest(metrics) %>% 
  filter(metric_type %in% c("Aktiva", "Passiva", "GuV")) %>% 
  filter(is.finite(f1_score), loop == 0) %>% 
  filter(n_examples <= 3 | is.na(n_examples)) %>%
  group_by(model_family, metric_type) %>% 
  slice_max(n = 1, f1_score) %>% 
  arrange(desc(f1_score)) %>% # head(10) %>% 
  select(model_family, model, metric_type, method_family, n_examples, f1_score, norm_runtime) %>%
  mutate(
    f1_score = round(f1_score, 2),
    norm_runtime = round(norm_runtime, 0),
  ) %>% rename(
    "runtime in s" = norm_runtime,
  )
```

(ref:page-identification-intro-text) The first research question asks, how \acr{LLM}s can be used, to effectively locate specific information in a financial report. The task for this thesis is to identify the pages where the balance sheet (*Bilanz*) and the profit-and-loss-and-statement (*Gewinn- und Verlustrechnung, GuV*) are located. The balance sheet is composed of two tables showing the assets (*Aktiva*) and liabilities (*Passiva*) of a company. Often, these two tables are on separate pages. Hereafter, the German terms **Aktiva**, **Passiva** and **GuV** will be used.

(ref:page-identification-intro-text)

@liExtractingFinancialData2023 describes two ways to identify the relevant pages (see Figure \@ref(fig:extraction-framework-flow-chart)). For longer documents they propose to use the \acr{TOC} to determine a page range that includes the information of interest. In addition, they develop target specific regular expressions and rules to filter out irrelevant pages[^05_results-1]. The result of this "Page Range Refinement" is then passed to the \acr{LLM} to extract information from.

[^05_results-1]: Our personal opinion: Developing well performing regular expressions can be a very tedious and setting appropriate rules requires some domain knowledge. It can be worth the effort if there are a lot of documents with similar information to extract. For this thesis it took multiple months. At least, now there is kind of a pipeline one can reuse, exchanging the rules and key word lists. Thus the next similar task should be solved faster.

This section presents the results of four approaches to identify the page[^05_results-2] of interest. The detailed report on all experiments conducted and their results can be found in the appendix:

[^05_results-2]: In some cases the information of interest is spanning two pages. These rare cases are not covered from the approaches presented here, yet.

-   Subsection \@ref(regex-page-identification) presents the findings of a page range refinement using a list of key words with a regular expression.
-   Subsection \@ref(toc-understanding) presents the findings of a \acr{TOC} understanding approach
-   Subsection \@ref(llm-page-identification) presents the findings of a text classification approach using \acr{LLM}s.
-   Subsection \@ref(tf-classifier) presents the findings of a term-frequency approach.

We compare and summarize the results in Subsection \@ref(comparison-page-identification). Subsection \@ref(efficency-discussion) proposes an efficient combination of approaches to solve the task of this thesis and discusses its limitations.

###  {.unlisted .unnumbered}

::: paragraph-start
##### Dataset describtion

For the page identification task, we selected companies (mainly) from the first row of Figure \@ref(fig:beteiligungsunternehmen), to build the ground truth from. The main motivation is that the documents of a companies within a category are more similar to each other, than to documents of companies of other categories. For the chosen companies all available annual reports are selected. Since one of the companies mainly published documents that require \acr{OCR} preprocessing, we include the documents of a second company for this category.
:::

### Approaches

For all approaches presented in this section except the \acr{TOC} understanding approach, the page identification task is formulated as a classification problem. This subsection briefly describes our approaches. A detailed report can be found in teh Appendix \@ref(page-identification-report).

::: paragraph-start
##### Regular expressions

We develop multiple sets of regular expressions and filter out all pages that do not fulfill all regular expressions of a given set. There are different sets for each target type, **Aktiva**, **Passiva** and **GuV**. The sets also differ in how versatile they can cope with additional white space introduced by a imperfect text extraction and how many different words for a given term are accepted.
:::

::: paragraph-start
##### Table of Contents Understanding

We use a \acr{LLM} to extract the \acr{TOC} from the first pages from a document or use the embedded \acr{TOC} and prompt a \acr{LLM} to identify the pages where the **Aktiva**, **Passiva** and **GuV** are located.
:::

::: paragraph-start
##### Large Language Model Classification

We use \acr{LLM}s to classify if the text extract of a given page contains a **Aktiva**, **Passiva** or **GuV** table or something else. We test binary classification and a multi-classification approach. The reported confidence scores can be used to form a ranking, which text extract might be most similar to the target type.
:::

We test a wide range of open-weight models and compare different prompting techniques. Besides a zero shot approach, we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing the legal text instead of examples from a annual report.

::: paragraph-start
##### Term Frequency Ranking

We use normalized term frequencies and normalized float frequency as features for a classification with a random forest. The predicted scores are used to build a ranking, which page most probably contains the target pages. Undersampling is used during training, to handle the unbalanced data.
:::

### Comparison {#comparison-page-identification}

```{r load-page-identification-comapison-data, echo=echo_flag, warning=warning_flag, message=message_flag}
df_page_identification_comparing_performance <- readODS::read_ods("data_storage/page_identification_summary.ods", sheet = 1)
df_page_identification_comparing_efficiency <- readODS::read_ods("data_storage/page_identification_summary.ods", sheet = 2)

df_df_page_identification_comparing_performance_formatted <- df_page_identification_comparing_performance %>% 
  group_by(type) %>% 
  mutate(
    across(
      c(precision, recall, F1, `top 1 recall`),
      ~ifelse(      
        . == max(., na.rm = TRUE),
        paste0("**", round(., 3), "**"),
        round(., 3)
        )
      )) %>% 
  mutate(`k for full recall` = ifelse(      
        `k for full recall` == min(`k for full recall`, na.rm = TRUE),
        paste0("**", round(`k for full recall`, 3), "**"),
        round(`k for full recall`, 3)
        ))
```

This subsection presents the performance and efficiency for all four presented approaches and compares it with the the results a human achieves manually

::: paragraph-start
##### Prediction performance

Table \@ref(tab:page-identification-performance-overview-all) shows the best performance achieved by the four presented approaches regarding precision, recall and F1 score.

```{r page-identification-performance-overview-all, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_df_page_identification_comparing_performance_formatted %>% 
  select(-c(`top 1 recall`, `k for full recall`)) %>% 
  arrange(type) %>% 
  render_table(
    alignment = "lllrrr",
    caption = "Comparing page identification perfromance among all four approaches.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

Llama 4 Scout reaches the best F1 score for the target types **Akiva** and **Passiva** in the multi-class classification approach. For **GuV** the best F1 score (0.985) is found with Ministral-8B-Instruct in the binary classification approach. Llama 4 Scout reaches a F1 score of 0.971 for target type **GuV** and multi-class classification.
:::

In the dataset preparation for the table extraction task (see section \@ref(information-extraction-data-sampling)) 107 **Aktiva** pages have been selected. In this manual process we made two mistakes, accidently selecting one **Passiva** and one **GuV** page. Thus the human baseline to compete with is 0.981. Thus, Llama 4 Scout is more precise than us.

Furthermore, Llama 4 Scout reaches a recall of 1.0 for all target types. This means, the results can be used downstream, even though the precision is not always perfect. The pages classified as target can be double checked by a human, without missing any page.

The performance of approaches that do not utilize \acr{LLM}s is substantially lower. Only the term-frequency approach yields results that are suitable for downstream use, as it achieves a recall of 1.0. Table \@ref(tab:page-identification-performance-overview-top-k-recall) shows the results of the top k recall for the term-frequency and \acr{LLM} approaches. For the term-frequency approach the ranking is based on the scores of the random forest classifier. For the \acr{LLM}s the ranking is based on the calculated confidence scores.

The \acr{LLM}s always rate the correct **GuV** page highest. With Llama Scout 4 we find all target pages within the first two ranked pages. For the term-frequency approach a human sometimes has to check up to five pages.

```{r page-identification-performance-overview-top-k-recall, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
df_df_page_identification_comparing_performance_formatted %>% 
  select(-c(precision, recall, F1)) %>% 
  filter(!is.na(`top 1 recall`)) %>% 
  arrange(type) %>% 
  render_table(
    alignment = "lllrr",
    caption = "Comparing the top k recall for the term-frequency and LLM approaches.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

::: paragraph-start
##### Energy usage and runtime

Table \@ref(tab:page-identification-efficiency-overview-all) shows the runtime in seconds per document, estimated energy consumption in Joule per document and costs in **CENTS per 1000 documents**. The runtime for the \acr{LLM}s was normalized to seconds on a NVIDIA B200 and thus the \acr{TDP} of 700 W is used to calculate the energy consumption. For the other approaches, running on a laptop (see section \@ref(local-machine)) a \acr{TDP} of 28 Watts is used. For manual work by a human additional 60 W are added for the screen used. It is assumed that the \acr{LLM} is hosted locally.
:::

```{r page-identification-efficiency-overview-all, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
table_energy <- df_page_identification_comparing_efficiency %>% filter(!drop == 1) %>% 
  select(
    approach, strategy, 
    `runtime per document in s`, `energy in J`, 
    `costs in CENTS per 1000 documents`
    ) %>%
  mutate(
    across(
      where(is.numeric),
      ~ifelse(      
        . == min(., na.rm = TRUE),
        paste0("**", format(round(., 3), nsmall=3), "**"),
        format(round(., 3), nsmall=3)
        )
      )) %>% 
  render_table(
    alignment = "llrrr",
    caption = "Comparing page identification efficiency among all four approaches.",
    ref = opts_current$get("label")
    )
if(knitr::is_latex_output()) {
  table_energy <- table_energy %>% 
    column_spec(2, width = "3.7cm") %>% 
    column_spec(3, width = "2.3cm") %>% 
    column_spec(5, width = "3cm")
}
table_energy
```

Table \@ref(tab:page-identification-efficiency-overview-all) shows, that the regular expression approach is fastest and consumes least energy. Nevertheless, since the results are not sufficient, another approach has to be chosen, if the amount of manual labor should be reduced for the human inn the loop.

Second place, regarding all these criteria, is the term-frequency approach, which guarantees a perfect recall, while reducing the number of pages to investigate to five per target type. This is similar to the number of pages a human has to investigate to find the \acr{TOC} of the document. And it is a reduction to 7.4 % of the average 67 pages the documents in this dataset have. The costs are still negligible.

The \acr{LLM} approaches have the highest runtime and energy consumption. This is the case, because they process every page with very computational demanding algorithms. For the \acr{TOC} approach \acr{LLM}s are used as well, but they process far less of the documents pages. Thus, their energy consumption is lower.

Since all approaches but the manual identification need the text extract, this runtime and energy consumption are also not listed (but low).

## Information extraction {#table-extraction-introduction}

```{r loading-table-extraction-datasets, echo=echo_flag, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum(c("data_storage/real_table_extraction_extended_llm.rds", "data_storage/real_table_extraction_extended_synth.rds", "data_storage/real_table_extraction_extended_azure.rds"))}
df_real_table_extraction <- readRDS("data_storage/real_table_extraction_extended_llm.rds") %>% 
  filter(!model %in% c("deepseek-ai_DeepSeek-R1-Distill-Qwen-32B", 'google_gemma-3n-E4B-it')) %>% 
  mutate(
    model = gsub("^[^_]+_", "", model),
    company = map_chr(filepath, ~str_split(str_split(., "/")[[1]][5], "__")[[1]][1])
    ) %>% filter(company != "MEAB GmbH")
df_real_table_extraction_synth <- readRDS("data_storage/real_table_extraction_extended_synth.rds") %>% 
  mutate(
    model = gsub("^[^_]+_", "", model),
    company = map_chr(filepath, ~str_split(str_split(., "/")[[1]][5], "__")[[1]][1])
    ) %>% filter(company != "MEAB GmbH")
df_real_table_extraction_azure <- readRDS("data_storage/real_table_extraction_extended_azure.rds") %>% 
  mutate(
    model = gsub("^[^_]+_", "", model),
    company = map_chr(filepath, ~str_split(str_split(., "/")[[1]][5], "__")[[1]][1])
    ) %>% filter(company != "MEAB GmbH")

model_by_size <- c(
  'gemma-3-4b-it', #'gemma-3n-E4B-it', 
  "gemma-3-12b-it", "gemma-3-27b-it",
  "gpt-oss-20b", "gpt-oss-120b",
  "Llama-3.1-8B-Instruct", "Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct",
  "Llama-4-Scout-17B-16E-Instruct", "Llama-4-Maverick-17B-128E-Instruct-FP8",
  "Ministral-8B-Instruct-2410", "Mistral-Small-3.1-24B-Instruct-2503",
  "Mistral-Large-Instruct-2411", "Qwen2.5-0.5B-Instruct",
  "Qwen2.5-1.5B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct",
  "Qwen2.5-14B-Instruct", "Qwen2.5-32B-Instruct", "Qwen2.5-72B-Instruct",
  "Qwen3-0.6B", "Qwen3-1.7B", "Qwen3-4B",
  "Qwen3-8B", "Qwen3-14B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-32B",
  "Qwen3-235B-A22B-Instruct-2507-FP8", "Qwen3-235B-A22B-Instruct-2507",
  # "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
  "Falcon3-10B-Instruct", "phi-4"
)

method_order <- c("top_n_rag_examples", "n_random_examples", "top_n_rag_examples_out_of_sample", "static_example", "zero_shot" )

norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors_real_table_extraction.csv") %>% 
  mutate(
    model_name = model_name %>% str_replace("/", "_")
  )
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml") | str_ends(filename, "vllm_batched.yaml")))

df_real_table_extraction <- df_real_table_extraction %>% left_join(
  norm_factors_few_examples %>% mutate(model_name = gsub("^[^_]+_", "", model_name)) %>% select(model_name, parameter_count, normalization_factor),
  by = c("model" = "model_name")
  ) %>% mutate(normalized_runtime = round(runtime*normalization_factor, 0))

df_overview <- bind_rows(df_real_table_extraction, df_real_table_extraction_azure) %>% 
  filter(out_of_company != TRUE | is.na(out_of_company), n_examples <= 5) %>% 
  filter(model %in% model_by_size) %>%
  filter(n_examples != 2) %>% 
  mutate(
    model = factor(model, levels = model_by_size),
    method_family = factor(method_family, levels = method_order),
    n_examples = fct_rev(ordered(paste("n =", n_examples)))
    )

units_real_tables <- read_csv("../benchmark_truth/real_tables_extended/table_characteristics_more_examples.csv") %>% mutate(
  filepath = paste0('/pvc/benchmark_truth/real_tables_extended/', company, '__', filename),
  T_EUR = (T_in_year + T_in_previous_year)>0,
  T_EUR_both = (T_in_year + T_in_previous_year)>1,
  n_T_EUR = T_in_year + T_in_previous_year
) %>% select(filepath, T_EUR, T_EUR_both, n_T_EUR)

df_real_table_extraction <- df_real_table_extraction %>% left_join(units_real_tables) %>% 
  mutate(
    .before = 1, 
    company = map_chr(filepath, ~str_split(str_split(., "/")[[1]][5], "__")[[1]][1]),
    same_company = !out_of_company
    )
df_real_table_extraction_synth <- df_real_table_extraction_synth %>% 
  left_join(units_real_tables) %>% 
  mutate(
    .before = 1, 
    company = map_chr(filepath, ~str_split(str_split(., "/")[[1]][5], "__")[[1]][1]),
    same_company = !out_of_company
    )
```

(ref:information-extraction-intro-text) The second research question asks, how \acr{LLM}s can be used, to effectively extract specific information from a financial report. The task for this thesis is to extract the numeric values for the assets (*Aktiva*) table, which is part of the balance sheet (*Bilanz*). Hereafter, the German term **Aktiva** will be used. We are limiting the scope even further than in Subsection \@ref(page-identification-introduction), because it takes more time to manually create the first reference dataset.

(ref:information-extraction-intro-text)

###  {.unlisted .unnumbered}

::: paragraph-start
##### Structured output

We use a strict schema for the extraction process that is derived from the legal text [@hgbHandelsgesetzbuchImBundesgesetzblatt2025, §266]. Actually, there are three types of verbosity, that are defined in the law. Smaller companies are permitted to create less detailed balance sheets. Our schema is created based on the most detailed level. This is the form most often found in the document base[^05_results-3].
:::

[^05_results-3]: Unfortunately, well known companies as BVG and BSR publish a less detailed form. Thus, their documents are not included in the document base for this task.

Using a strict schema has advantages for processing the results in downstream tasks - i.e. for adding the results to a relational database. It is also easier to compare the results with a ground truth, if the names of all rows and their order is fixed. The schema is defined as \acr{ebnf} grammar and passed as an argument to \acr{vllm}.

::: paragraph-start
##### Gound truth dataset

For the information extraction we use two datasets. First, a collection of 107 real **Aktiva** tables is created, going through two sampling iterations. In the first iteration a single report is selected for each company. In addition, all available reports from the first listed company are chosen, to test an in-company learning approach. In the second iteration more reports of the other companies are added, to increase the ground truth size and to allow testing the in-company approach for all companies.
:::

Second, a dataset of 16_504 synthetic **Aktiva** tables is created. We generate these tables based on the extraction schema and fill them with random numeric values. Different table characteristics are systematically combined, to investigate potential effects of these features on the extraction performance. The tables are created as \acr{PDF}, \acr{HTML} and Markdown files each. This dataset allows to estimate the extraction performance, if there are no unknown row identifiers present.

### Approaches

::: paragraph-start
##### Regular expressions

We use regular expressions to extract the numeric values for matching row identifiers. The regular expressions handle line breaks between words in the row identifiers, but not within a word. They can handle multiple signs of white space. Besides that, they try to fully match the labels from the legal text with the text extract, ignoring upper case. They extract numbers with "." as thousands separator.
:::

::: paragraph-start
##### Real tables

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation. The \acr{LLM} has to group row identifiers and corresponding numeric values and match the row identifier with the labels of the schema. If a row identifier is unknown, the values have to be discarded. If a label is not present among the row identifiers, the model predicts *null*. All values are extracted in one pass. We do not include any instruction, how to proceed with currency units, that might be given for certain columns.
:::

We test a wide range of open-weight models and compare different prompting techniques. Besides a zero shot approach we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing a synthetic **Aktiva** table as example. We test models from OpenAIs GPT family in addition to the open-weight models.

::: paragraph-start
##### Synthetic tables

We use \acr{LLM}s to extract the numeric values of synthetic **Aktiva** tables with restricted generation. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

::: paragraph-start
##### Hybrid approach

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation, providing examples from synthetic **Aktiva** tables. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

### Comparison {#comparing-table-extraction-methods}

This subsection compares the results for the table extraction tasks. It will discuss the findings about performance and runtime and compare it with the results a human may achieve wit manual labor. The detailed report on all experiments conducted in their results can be found in the Appendix \@ref(information-extraction-report). The evaluation of feature importance can be found in Appendix \@ref(feature-effect-analysis).

::: paragraph-start
##### Performance

Table \@ref(tab:compare-extraction-performance) summarizes the mean percentage of correct predictions total for all approaches and both types of **Aktiva** tables. The highest baseline for the extraction tasks is set by our own manual performance. We achieve 97.6 % correct extracted values on the real **Aktiva** tables. The \acr{regex} performance on the synthetic **Aktiva** tables comes close but on real **Aktiva** tables it is far off.
:::

The mean performance of Qwen3-235B does not match our baseline on the real **Akiva** tables. But its median performance already is 100 %.

On synthetic tables its mean performance is almost perfect, if currency units get respected. With HTML documents we find 100 % correct predictions. With Markdown documents we find 99.9 % correct predictions as well. Figure \@ref(fig:comparing-table-extraction-performance-among-real-and-synth-aktiva-data) shows, that the better performance on the synthetic tables is found for almost all models.

Qwen3-8B performed best among the small models \acr{LLM}s but shows over 4 % more wrong predictions than Qwen3-235B.

Using synthetic examples results in worse performance. But it can be used to show how to handle currency units.

```{r compare-extraction-performance, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
tribble(
  ~approach, ~strategy, ~table_type, ~percentage_correct_total,
  "human", "manual", "real", 97.6,
  "regex", "", "real", 68.6,
  "llm", "Qwen3-235B, top_5_rag_examples", "real", 97.0,
  "llm", "Qwen3-8B, top_5_rag_examples", "real", 92.7,
  "llm", "Qwen3-235B, top_5_rag_examples, synth examples", "real", 91.8,
  "regex", "", "synth", 96.9,
  "llm", "Qwen3-235B, top_5_rag_examples, respect_units", "synth", 99.9,
  "llm", "Qwen3-8B, top_5_rag_examples", "synth", 94.6,
) %>% # arrange(table_type) %>% 
  group_by(table_type) %>% 
  mutate(
    across(
      c(percentage_correct_total),
      ~ifelse(      
        . == max(., na.rm = TRUE),
        paste0("**", round(., 3), "**"),
        round(., 3)
        )
      )) %>% 
  rename(mean_percentage_correct_total = percentage_correct_total) %>% 
  render_table(
    alignment = "lllr",
    caption = "Comparing the mean percentage of correct predictions total among all approaches and table types.",
    ref = opts_current$get("label"),
    row_group_col = 3
    )
```

::: paragraph-start
##### Runtime

Extracting the values from all 106 tables took Qwen3-235B around six minutes. Thus, excluding the setup time for the LLM, Qwen3-235B-A22B-Instruct is around 100 times faster than a human.
:::

::: paragraph-start
##### Hypotheses

The predictor that shows a strong effect in all approaches is currency unit. Reflecting this in the table extraction is a key factor to optimize the performance. For the approaches that use \acr{LLM}s most of the model and method related variables showed a strong effect. Using a versatile model and providing good learning examples is mandatory.
:::

The findings from approaches utilizing synthetic tables suggest that the input format can have a substantial influence on extraction outcomes. It appears important to prevent errors during text extraction, and converting the extracted text to HTML may help resolve remaining ambiguities. However, it remains unclear whether a perfect text extract would perform as well as HTML or Markdown formats.

## Error rate guidance {#error-rate-guidance-results}

(ref:error-rate-guidance-intro-text) The side research question asks, if it is possible to guide the users attention to predictions that have a higher empirical rate of errors. In this thesis we focus on the confidence score calculated from the reported log probabilities.

(ref:error-rate-guidance-intro-text)

Subsection \@ref(page-identification-error-rate-results) presents the results found regarding our side research question for the page identification task. Subsection \@ref(information-extraction-error-rate-results) presents the results found regarding our side research question for the information extraction task. The detailed report can be found in the Appendix \@ref(error-guidance-report).

### Page identification {#page-identification-error-rate-results}

We find, that the confidence score can be used in the page identification task, to identify confidence intervals, that contain no or only a few errors. The amount of predictions in these intervals varies among models, classification task and target class.

::: paragraph-start
##### Binary classification

Distinguishing correct and wrong classifications based on the confidence score is working well for the responses of Ministral-8B-Instruct. But for other models, e.g. from the Qwen family, it works worse. This is possible, because Ministral-8B-Instruct reports confidence scores over a wide range, while models from the Qwen 2.5 family report always high confidence.
:::

Thus, it is possible to define a wide range of confidence intervals, where we find a empirical error rate of zero for Ministral. For Qwen 2.5 32B we find the highest confidence interval containing some mistakes, But it is still good with less 1 %. Most predictions are in the highest confidence score interval for both models.

::: paragraph-start
##### Multi-class classification

Distinguishing correct and wrong classifications based on the confidence score is still working well for the responses of Ministral-8B-Instruct, predicting **Aktiva** and **GuV**. For **Passiva** we find a single wrong prediciton in the highest confidence interval. For Llama 4 Scout it is working for the target classes **Aktiva** and **Passiva**. For Qwen 2.5 32B it works worst among those three - still well performing - models.
:::

For Ministral we find a empirical error rate of zero for the highest confidence interval, except for **Passiva**. For Llama we find an error rate lesst than 1 % for all classes. For Qwen 2.5 32B only **Passiva** has a high confidence interval with less than 1 % error rate. Most predictions are in the highest confidence score interval for all models again.

### Information extraction {#information-extraction-error-rate-results}

We find, that the confidence score can not be used alone in the information extraction task, to identify confidence intervals, that contain no or only a few errors. There are only few exceptions, where we achieve an error rate of under 1 % over all annual reports. We just report the results of the best performing model here.

::: paragraph-start
##### Real tables

For the best predicting model Qwen3-235B-A22B-Instruct, we find an empirical error rate of 1.3 % for predicting a missing value and 3.3 % for predicting numeric values. Almost all predictions fall in the highest confidence interval. Making the interval width smaller, does not result in intervals with lower error rate.
:::

::: paragraph-start
##### Synthetic tables

For the best predicting model Qwen3-235B-A22B-Instruct, we find an empirical error rate below 1 % for predicting a missing value and numeric values, if we explicitly instruct the model to handle currency units. If the input format is not a text extracted from a \acr{PDF} file, but perfect \acr{HTML} code the error rate gets 0 %. With perfect Markdown code the error rate is above 0 % but below 1 %.
:::

::: paragraph-start
##### Hybrid approach

For the best predicting model Qwen3-235B-A22B-Instruct, we find an empirical error rate below 4 % for predicting a missing value and around 20 % to 26 % for predicting numeric values. The values is lower, if we explicitly instruct the model to handle currency units.
:::

## Summary {#summary-results}

In summary, the results presented in this chapter demonstrate the effectiveness of \acr{LLM}s for both page identification and information extraction tasks in financial reports. \acr{LLM}-based approaches consistently outperform traditional methods, achieving high recall and precision, and enabling efficient automation of information extraction. The experiments further highlight the importance of input format and model selection, as well as the potential for guiding user attention through confidence scores.

These findings provide a solid foundation for the subsequent discussion, where we interpret the results in the context of our research questions and hypotheses, analyze sources of error, and explore practical implications for future applications.
