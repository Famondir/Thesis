
@misc{nassar_tableformer_2022,
	title = {{TableFormer}: {Table} {Structure} {Understanding} with {Transformers}},
	shorttitle = {{TableFormer}},
	url = {http://arxiv.org/abs/2203.01017},
	doi = {10.48550/arXiv.2203.01017},
	abstract = {Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91\% to 98.5\% on simple tables and from 88.7\% to 95\% on complex tables.},
	urldate = {2025-04-07},
	publisher = {arXiv},
	author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01017 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/MHEENV38/Nassar et al. - 2022 - TableFormer Table Structure Understanding with Transformers.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/VXX5JEGY/2203.html:text/html},
}

@misc{auer_docling_2024,
	title = {Docling {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.09869},
	doi = {10.48550/arXiv.2408.09869},
	abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
	urldate = {2025-04-07},
	publisher = {arXiv},
	author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and Lima, Rafael Teixeira de and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
	month = dec,
	year = {2024},
	note = {arXiv:2408.09869 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/CB5G2B4S/Auer et al. - 2024 - Docling Technical Report.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/I7GPQFC3/2408.html:text/html},
}

@misc{bmi_referat_o2_minikommentar_2013,
	title = {Minikommentar zum {Gesetz} zur {Förderung} der elektroni- schen {Verwaltung} sowie zur Änderung weiterer {Vor}- schriften},
	editor = {{BMI, Referat O2}},
	month = jul,
	year = {2013},
	file = {Minikommentar zum Gesetz zur Förderung der elektroni- schen Verwaltung sowie zur Änderung weiterer Vor- schriften:/home/simon/snap/zotero-snap/common/Zotero/storage/MXWPI7DC/e-government-gesetz-minikommentar.pdf:application/pdf},
}

@misc{noauthor__nodate,
	title = {§ 12 {EGovG} - {Einzelnorm}},
	url = {https://www.gesetze-im-internet.de/egovg/__12.html},
	urldate = {2025-04-07},
	file = {§ 12 EGovG - Einzelnorm:/home/simon/snap/zotero-snap/common/Zotero/storage/5U8LSTMF/__12.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How {Much} {Does} it {Cost} to {Make} a {Commercial} {Ad}? ({Video} {Price})},
	shorttitle = {How {Much} {Does} it {Cost} to {Make} a {Commercial} {Ad}?},
	url = {https://www.startmotionmedia.com/commercial-video-production/commercial-video-cost-how-much-to-pay-for-commercial-video/},
	abstract = {Start Motion Media Video Services: How much to pay for video ads. Cost factor insights. Celebrating \$1 Billion Client Sales},
	language = {en-US},
	urldate = {2025-01-21},
	journal = {Start Motion Media Video Company},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/FQJFC7NI/commercial-video-cost-how-much-to-pay-for-commercial-video.html:text/html},
}

@misc{rico_radio_2023,
	title = {Radio {Advertising} {Costs}: {A} {Simple} {Guide} to {Ad} {Spend}},
	shorttitle = {Radio {Advertising} {Costs}},
	url = {https://fitsmallbusiness.com/radio-advertising-costs/},
	abstract = {How much does a radio ad cost? Find out all the radio advertising costs you need to know to get your small business started with radio ads.},
	language = {en-US},
	urldate = {2025-01-21},
	journal = {Fit Small Business},
	author = {Rico, Audrey Rawnie},
	month = jul,
	year = {2023},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/CE6PQT6N/radio-advertising-costs.html:text/html},
}

@misc{noauthor_billboard_nodate,
	title = {Billboard {Design} {Pricing} (\$249 {Onwards})},
	url = {https://designbro.com/pricing/},
	abstract = {World-class creative designers. ✭ Hand-selected \& trusted graphic designers. ✭ 98\% satisfaction rate. ✭ Agency quality creative designing for a freelance price.✭},
	language = {en},
	urldate = {2025-01-21},
	journal = {DesignBro},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/QKSZ4T9H/billboard.html:text/html},
}

@misc{chad_j_treadway_digital_2022,
	title = {Digital vs. {Traditional} {Advertising}: {A} {Fair} {Comparison}},
	url = {https://cubecreative.design/blog/small-business-marketing/traditional-vs-digital-advertising-comparison#billboard-and-out-of-home-media-advertising},
	urldate = {2025-01-20},
	author = {{Chad J. Treadway}},
	year = {2022},
	file = {Digital vs. Traditional Advertising\: A Fair Comparison:/home/simon/snap/zotero-snap/common/Zotero/storage/6BDQPSCI/traditional-vs-digital-advertising-comparison.html:text/html},
}

@misc{noauthor_was_nodate,
	title = {Was sind die häufigsten {Fehler} bei der {Segmentierung} {Ihres} {Modepublikums}?},
	url = {https://www.linkedin.com/advice/0/what-most-common-mistakes-when-segmenting-ujzoe},
	abstract = {Segmenting your fashion audience is a crucial step to understand your customers, tailor your products and marketing strategies, and increase your sales and loyalty. However, many fashion designers make some common mistakes when segmenting their audience, which can lead to ineffective or even harmful},
	language = {de},
	urldate = {2025-01-20},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/J4ZBMCH8/what-most-common-mistakes-when-segmenting-ujzoe.html:text/html},
}

@misc{noauthor_median_nodate,
	title = {Median income or consumption per day},
	url = {https://ourworldindata.org/grapher/daily-median-income},
	abstract = {This data is adjusted for inflation and for differences in the cost of living between countries.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Our World in Data},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/VY5LYCMB/daily-median-income.html:text/html},
}

@misc{evgenia_koptyug_e-commerce_2023,
	title = {E-commerce: online shoppers by age group {Germany}},
	shorttitle = {E-commerce},
	url = {https://www.statista.com/statistics/506181/e-commerce-online-shoppers-by-age-group-germany/?__sso_cookie_checker=failed},
	abstract = {In 2023, over 78 percent of 25- to 45-year-olds in Germany had ordered and purchased products online in the past three months.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Statista},
	author = {{Evgenia Koptyug}},
	year = {2023},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/8B76YXUC/e-commerce-online-shoppers-by-age-group-germany.html:text/html},
}

@misc{americas_market_intelligence_global_2023,
	title = {Global: e-commerce spend per capita by region 2023},
	shorttitle = {Global},
	url = {https://www.statista.com/statistics/518729/annual-b2c-e-commerce-spending-by-region/?__sso_cookie_checker=failed},
	abstract = {In 2023, the United States witnessed an estimated annual per capita spending on e-commerce of 3,370 U.S.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Statista},
	author = {{Americas Market Intelligence}},
	year = {2023},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/QK5BHZRQ/annual-b2c-e-commerce-spending-by-region.html:text/html},
}

@misc{stephanie_chevalier_online_2023,
	title = {Online shoppers willing to wait for discounts by age 2022},
	url = {https://www.statista.com/statistics/1395897/shoppers-waiting-for-discounts-before-buying-online-age/},
	abstract = {Younger consumers, specifically Gen Z and millennials, exhibit a higher propensity than older consumers to wait for discounts before making online purchases.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Statista},
	author = {{Stephanie Chevalier}},
	year = {2023},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/5BBPQ2LA/shoppers-waiting-for-discounts-before-buying-online-age.html:text/html},
}

@misc{bundesanzeiger_jahresabschluss_nodate,
	title = {Jahresabschluss zum {Geschäftsjahr} vom 01.01.2008 bis zum 31.12.2008},
	url = {https://www.bundesanzeiger.de/pub/de/suchergebnis?7},
	urldate = {2025-01-10},
	author = {{Bundesanzeiger}},
	file = {Suchergebnis – Bundesanzeiger:/home/simon/snap/zotero-snap/common/Zotero/storage/G3DTIWF3/suchergebnis.html:text/html},
}

@misc{grandini_metrics_2020,
	title = {Metrics for {Multi}-{Class} {Classification}: an {Overview}},
	shorttitle = {Metrics for {Multi}-{Class} {Classification}},
	url = {http://arxiv.org/abs/2008.05756},
	doi = {10.48550/arXiv.2008.05756},
	abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
	month = aug,
	year = {2020},
	note = {arXiv:2008.05756 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/WX2FF8RW/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overview.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/XVQUAGR6/2008.html:text/html},
}

@misc{sheng_pdftable_2024,
	title = {{PdfTable}: {A} {Unified} {Toolkit} for {Deep} {Learning}-{Based} {Table} {Extraction}},
	shorttitle = {{PdfTable}},
	url = {http://arxiv.org/abs/2409.05125},
	doi = {10.48550/arXiv.2409.05125},
	abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Sheng, Lei and Xu, Shuai-Shuai},
	month = sep,
	year = {2024},
	note = {arXiv:2409.05125 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/Y833Q63R/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/74UPJXVR/2409.html:text/html},
}

@inproceedings{chen_tablevlm_2023,
	address = {Toronto, Canada},
	title = {{TableVLM}: {Multi}-modal {Pre}-training for {Table} {Structure} {Recognition}},
	shorttitle = {{TableVLM}},
	url = {https://aclanthology.org/2023.acl-long.137/},
	doi = {10.18653/v1/2023.acl-long.137},
	abstract = {Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97\% in tree-editing-distance-score on ComplexTable.},
	urldate = {2025-01-10},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Leiyuan and Huang, Chengsong and Zheng, Xiaoqing and Lin, Jinshu and Huang, Xuanjing},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2437--2449},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/7LWJ5DED/Chen et al. - 2023 - TableVLM Multi-modal Pre-training for Table Structure Recognition.pdf:application/pdf},
}

@misc{li_extracting_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Extracting {Financial} {Data} from {Unstructured} {Sources}: {Leveraging} {Large} {Language} {Models}},
	shorttitle = {Extracting {Financial} {Data} from {Unstructured} {Sources}},
	url = {https://papers.ssrn.com/abstract=4567607},
	doi = {10.2139/ssrn.4567607},
	abstract = {This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from PDF-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate ESG reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5\% accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96\%. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {Social Science Research Network},
	author = {Li, Huaxia and Gao, Haoyun (Harry) and Wu, Chengzhang and Vasarhelyi, Miklos A.},
	month = sep,
	year = {2023},
	keywords = {Accounting information systems Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models, ChatGPT, Data extraction, Design Science, Information processing, Large Language Model (LLM), PDF Reports, Unstructured data},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/UEUDUT7Y/Li et al. - 2023 - Extracting Financial Data from Unstructured Sources Leveraging Large Language Models.pdf:application/pdf},
}

@misc{pythonology_extract_2023,
	title = {Extract text, links, images, tables from {Pdf} with {Python} {\textbar} {PyMuPDF}, {PyPdf}, {PdfPlumber} tutorial},
	url = {https://www.youtube.com/watch?v=G0PApj7YPBo},
	abstract = {Use these Python libraries to convert a Pdf into an image, extract text, images, links, and tables from pdfs using the 3 popular Python libraries PyMuPDF, PyPdf, PdfPlumber. Here is source code and article I have written:
https://pythonology.eu/what-is-the-be...
 -- Support Pythonology --
https://www.buymeacoffee.com/pythonology
-- Best Online Resource for Python --
Datacamp: The best online resource to learn Python, Web Scraping, Data analysis, and Data Science (Affiliate link)
https://datacamp.pxf.io/pythonology},
	urldate = {2025-01-10},
	author = {{Pythonology}},
	month = jan,
	year = {2023},
}

@misc{bbb_infrastruktur-verwaltungs_gmbh_geschaftsbericht_2024,
	title = {Geschäftsbericht 2023},
	url = {https://www.berlinerbaeder.de/fileadmin/BBB/Dateien/Unternehmen/Geschaeftsberichte/Geschaeftsberichte_der_BBB_Infrastruktur-Verwaltungs_GmbH___BBB_Infrastruktur_GmbH___Co._KG/GB_BBB_Infra_2023.pdf},
	urldate = {2025-01-10},
	author = {{BBB Infrastruktur-Verwaltungs GmbH} and {BBB Infrastruktur GmbH \& Co. KG}},
	month = oct,
	year = {2024},
	file = {GB_BBB_Infra_2023.pdf:/home/simon/snap/zotero-snap/common/Zotero/storage/NN95EQA8/GB_BBB_Infra_2023.pdf:application/pdf},
}

@misc{senatsverwaltung_fur_finanzen_berlin_beteiligungsbericht_2024,
	title = {Beteiligungsbericht},
	url = {https://www.berlin.de/sen/finanzen/vermoegen/downloads/beteiligungsbericht/artikel.941274.php},
	abstract = {Beteiligungsberichte},
	language = {de},
	urldate = {2025-01-10},
	author = {{Senatsverwaltung für Finanzen Berlin}},
	month = nov,
	year = {2024},
	file = {beteiligungsbericht_2024_gesamt:/home/simon/snap/zotero-snap/common/Zotero/storage/E2VJKHW4/beteiligungsbericht_2024_gesamt.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/YIZUVU22/artikel.941274.html:text/html},
}

@misc{lu_large_2024,
	title = {Large {Language} {Model} for {Table} {Processing}: {A} {Survey}},
	shorttitle = {Large {Language} {Model} for {Table} {Processing}},
	url = {http://arxiv.org/abs/2402.05121},
	doi = {10.1007/s11704-024-40763-6},
	abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.},
	urldate = {2025-01-10},
	author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
	month = oct,
	year = {2024},
	note = {arXiv:2402.05121 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/85WXEHCW/Lu et al. - 2024 - Large Language Model for Table Processing A Survey.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/UXHNNW2A/2402.html:text/html},
}

@article{textor_robust_2016,
	title = {Robust causal inference using directed acyclic graphs: the {R} package ‘dagitty’},
	volume = {45},
	issn = {0300-5771},
	shorttitle = {Robust causal inference using directed acyclic graphs},
	url = {https://doi.org/10.1093/ije/dyw341},
	doi = {10.1093/ije/dyw341},
	abstract = {Directed acyclic graphs (DAGs), which offer systematic representations of causal relationships, have become an established framework for the analysis of causal inference in epidemiology, often being used to determine covariate adjustment sets for minimizing confounding bias. DAGitty is a popular web application for drawing and analysing DAGs. Here we introduce the R package ‘dagitty’, which provides access to all of the capabilities of the DAGitty web application within the R platform for statistical computing, and also offers several new functions. We describe how the R package ‘dagitty’ can be used to: evaluate whether a DAG is consistent with the dataset it is intended to represent; enumerate ‘statistically equivalent’ but causally different DAGs; and identify exposure-outcome adjustment sets that are valid for causally different but statistically equivalent DAGs. This functionality enables epidemiologists to detect causal misspecifications in DAGs and make robust inferences that remain valid for a range of different DAGs. The R package ‘dagitty’ is available through the comprehensive R archive network (CRAN) at [https://cran.r-project.org/web/packages/dagitty/]. The source code is available on github at [https://github.com/jtextor/dagitty]. The web application ‘DAGitty’ is free software, licensed under the GNU general public licence (GPL) version 2 and is available at [http://dagitty.net/].},
	number = {6},
	urldate = {2024-07-27},
	journal = {International Journal of Epidemiology},
	author = {Textor, Johannes and van der Zander, Benito and Gilthorpe, Mark S and Liśkiewicz, Maciej and Ellison, George TH},
	month = dec,
	year = {2016},
	pages = {1887--1894},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/KPKSMQRL/Textor et al. - 2016 - Robust causal inference using directed acyclic gra.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/6YLDXHXD/2907796.html:text/html},
}

@misc{noauthor_wohnlagen_2024,
	title = {Wohnlagen},
	url = {https://www.berlin.de/sen/wohnen/service/mietspiegel/erlaeuterungen-zum-mietspiegel/wohnlagen/},
	language = {de},
	urldate = {2024-07-21},
	month = jun,
	year = {2024},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/D6YQTS5J/wohnlagen.html:text/html},
}

@misc{corona_get_nodate,
	title = {Get the {Data}},
	url = {https://insideairbnb.com/get-the-data/},
	abstract = {Adding data to the debate},
	language = {en},
	urldate = {2024-07-20},
	journal = {Inside Airbnb},
	collaborator = {Corona, Alice and Colomb, Clarice and Phibbs, Peter and Veracruz, Sito and Wieditz, Thorben},
	file = {Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/9A52ITSB/get-the-data.html:text/html},
}

@book{mcelreath_statistical_2020,
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-0-429-02960-8},
	url = {http://dx.doi.org/10.1201/9780429029608},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = mar,
	year = {2020},
	doi = {10.1201/9780429029608},
}

@misc{da_vision_2023,
	title = {Vision {Grid} {Transformer} for {Document} {Layout} {Analysis}},
	url = {http://arxiv.org/abs/2308.14978},
	doi = {10.48550/arXiv.2308.14978},
	abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14978 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/G5Y444YW/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analysis.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/LV6XRTVL/2308.html:text/html},
}

@misc{zhong_publaynet_2019,
	title = {{PubLayNet}: largest dataset ever for document layout analysis},
	shorttitle = {{PubLayNet}},
	url = {http://arxiv.org/abs/1908.07836},
	doi = {10.48550/arXiv.1908.07836},
	abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
	month = aug,
	year = {2019},
	note = {arXiv:1908.07836 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/4REPKJNM/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layout analysis.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/TKQLXWYB/1908.html:text/html},
}

@inproceedings{xu_layoutlm_2020,
	title = {{LayoutLM}: {Pre}-training of {Text} and {Layout} for {Document} {Image} {Understanding}},
	shorttitle = {{LayoutLM}},
	url = {http://arxiv.org/abs/1912.13318},
	doi = {10.1145/3394486.3403172},
	abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
	urldate = {2025-04-27},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	month = aug,
	year = {2020},
	note = {arXiv:1912.13318 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1192--1200},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/IADVHXM6/Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Document Image Understanding.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/LZU6Z3JF/1912.html:text/html},
}

@misc{geng_generating_2025,
	title = {Generating {Structured} {Outputs} from {Language} {Models}: {Benchmark} and {Studies}},
	shorttitle = {Generating {Structured} {Outputs} from {Language} {Models}},
	url = {http://arxiv.org/abs/2501.10868},
	doi = {10.48550/arXiv.2501.10868},
	abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
	urldate = {2025-07-06},
	publisher = {arXiv},
	author = {Geng, Saibo and Cooper, Hudson and Moskal, Michał and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
	month = jan,
	year = {2025},
	note = {arXiv:2501.10868 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/N8GNB3MS/Geng et al. - 2025 - Generating Structured Outputs from Language Models Benchmark and Studies.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/FMFPCEXK/2501.html:text/html},
}

@article{gougherty_testing_2024,
	title = {Testing the reliability of an {AI}-based large language model to extract ecological information from the scientific literature},
	volume = {3},
	copyright = {2024 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
	issn = {2731-4243},
	url = {https://www.nature.com/articles/s44185-024-00043-9},
	doi = {10.1038/s44185-024-00043-9},
	abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy ({\textgreater}90\%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
	language = {en},
	number = {1},
	urldate = {2025-07-09},
	journal = {npj Biodiversity},
	author = {Gougherty, Andrew V. and Clipp, Hannah L.},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Data mining, Invasive species, Macroecology},
	pages = {13},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/END5Q8YY/Gougherty and Clipp - 2024 - Testing the reliability of an AI-based large language model to extract ecological information from t.pdf:application/pdf},
}

@phdthesis{ambacher_designing_2024,
	title = {Designing a user-friendly and optimized version of a user interface for a large language model ({LLM})},
	copyright = {https://rightsstatements.org/page/InC/1.0/?language=de},
	url = {https://opus4.kobv.de/opus4-haw/frontdoor/index/index/docId/5155},
	abstract = {This thesis focuses on the creation of a user-friendly and efficient user interface (UI) for a large language model (LLM). As LLMs become more widely used in different fields, it is important to have a UI that meets user needs when interacting with them [1], [2], [3]. The main goal of this study is to find key design principles, design guidelines, and methods that ensure that the UI is easy to use, efficient, and satisfying.

To achieve this, the thesis analyzes the interaction principles according to ISO 9241-110 on the existing LLM tool Chat GPT and focuses also on other different UI designs [4]. For this purpose, a prototype is created based on the analysis. This thesis executes a usability test to evaluate the usability of the initial prototype. It focuses on factors including flexibility, accuracy of system answers, ease of navigation, and feature efficacy. The feedback from this test is used to enhance the design.

The second iteration of the prototype is evaluated using several measures, including the System Usability Scale (SUS) and the User Experience Questionnaire (UEQ). In addition, feedback was gathered from four experts in the field of UX and AI. The results showed improvements in ease of use, task efficiency, and overall user satisfaction. The results highlight the importance of clear guidance, privacy, and system responsiveness.

In summary, the thesis provides a set of design guidelines and recommendations for creating a user-friendly user interface for LLMs. These include the aspects of user-centered design, contextual help features, and optimized workflows. In addition, it provides guidance for the future development of AI tools that prioritize both functionality and a positive user experience.},
	language = {eng},
	urldate = {2025-08-08},
	school = {Technische Hochschule Ingolstadt},
	author = {Ambacher, Julian Emanuel},
	month = sep,
	year = {2024},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/F9IKW8QS/Ambacher - 2024 - Designing a user-friendly and optimized version of a user interface for a large language model (LLM).pdf:application/pdf},
}

@article{noauthor_annual_nodate,
	title = {Annual {Comprehensive} {Financial} {Report} for the {Fiscal} {Year} {Ended} {June} 30, 2023},
	language = {en},
	file = {PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/6XN9JPBJ/Annual Comprehensive Financial Report for the Fiscal Year Ended June 30, 2023.pdf:application/pdf},
}

@book{hgb_handelsgesetzbuch_2025,
	title = {Handelsgesetzbuch in der im {Bundesgesetzblatt} {Teil} {III}, {Gliederungsnummer} 4100-1, veröffentlichten bereinigten {Fassung}, das zuletzt durch {Artikel} 1 des {Gesetzes} vom 28. {Februar} 2025 ({BGBl}. 2025 {I} {Nr}. 69) geändert worden ist},
	url = {https://www.gesetze-im-internet.de/hgb/HGB.pdf},
	author = {{HGB}},
	month = feb,
	year = {2025},
}

@book{wohlin_experimentation_2024,
	address = {Berlin, Heidelberg},
	title = {Experimentation in {Software} {Engineering}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-69305-6 978-3-662-69306-3},
	url = {https://link.springer.com/10.1007/978-3-662-69306-3},
	language = {en},
	urldate = {2025-08-24},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	year = {2024},
	doi = {10.1007/978-3-662-69306-3},
	file = {PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/EPFIGBV5/Wohlin et al. - 2024 - Experimentation in Software Engineering.pdf:application/pdf},
}

@book{collis_business_2014,
	address = {Basingstoke},
	edition = {1. Publ.; 4. ed},
	title = {Business research: a practical guide for undergraduate \& postgraduate students},
	isbn = {978-0-230-30183-2},
	shorttitle = {Business research},
	language = {en},
	publisher = {Palgrave Macmillan},
	author = {Collis, Jill and Hussey, Roger},
	year = {2014},
	file = {PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/7BY6JSH4/Collis and Hussey - 2014 - Business research a practical guide for undergraduate & postgraduate students.pdf:application/pdf},
}

@article{wohlin_towards_2015,
	title = {Towards a decision-making structure for selecting a research design in empirical software engineering},
	volume = {20},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-014-9319-7},
	doi = {10.1007/s10664-014-9319-7},
	abstract = {Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders’ activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering research. The article provides an introduction to each decision point and its constituents, as well as to the relationships between the different parts in the decision-making structure. The intention is the structure should act as a starting point for the research design before going into the details of the research design chosen. The article provides an in-depth discussion of decision points in relation to the research design when conducting empirical research.},
	language = {en},
	number = {6},
	urldate = {2025-08-24},
	journal = {Empirical Software Engineering},
	author = {Wohlin, Claes and Aurum, Aybüke},
	month = dec,
	year = {2015},
	keywords = {Empirical software engineering research, Research design, Research methods, Selecting research method},
	pages = {1427--1455},
}

@article{saito_precision-recall_2015,
	title = {The {Precision}-{Recall} {Plot} {Is} {More} {Informative} than the {ROC} {Plot} {When} {Evaluating} {Binary} {Classifiers} on {Imbalanced} {Datasets}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
	doi = {10.1371/journal.pone.0118432},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
	language = {en},
	number = {3},
	urldate = {2025-08-24},
	journal = {PLOS ONE},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Bioinformatics, Caenorhabditis elegans, Exponential functions, Genome-wide association studies, Interpolation, Measurement, MicroRNAs, Support vector machines},
	pages = {e0118432},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/CTBBR652/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers o.pdf:application/pdf},
}

@misc{natarajan_human---loop_2024,
	title = {Human-in-the-loop or {AI}-in-the-loop? {Automate} or {Collaborate}?},
	shorttitle = {Human-in-the-loop or {AI}-in-the-loop?},
	url = {http://arxiv.org/abs/2412.14232},
	doi = {10.48550/arXiv.2412.14232},
	abstract = {Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop (AI2L) systems: the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component’s performance, neglecting the human expert’s critical role. Consequently, we propose an AI2L perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an AI2L approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.},
	language = {en},
	urldate = {2025-08-24},
	publisher = {arXiv},
	author = {Natarajan, Sriraam and Mathur, Saurabh and Sidheekh, Sahil and Stammer, Wolfgang and Kersting, Kristian},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14232 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/4YEXK3FV/Natarajan et al. - 2024 - Human-in-the-loop or AI-in-the-loop Automate or Collaborate.pdf:application/pdf},
}

@article{mosqueira-rey_human---loop_2023,
	title = {Human-in-the-loop machine learning: a state of the art},
	volume = {56},
	issn = {1573-7462},
	shorttitle = {Human-in-the-loop machine learning},
	url = {https://doi.org/10.1007/s10462-022-10246-w},
	doi = {10.1007/s10462-022-10246-w},
	abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.},
	language = {en},
	number = {4},
	urldate = {2025-08-24},
	journal = {Artificial Intelligence Review},
	author = {Mosqueira-Rey, Eduardo and Hernández-Pereira, Elena and Alonso-Ríos, David and Bobes-Bascarán, José and Fernández-Leal, Ángel},
	month = apr,
	year = {2023},
	keywords = {Active learning, Curriculum learning, Explainable AI, Human-in-the-loop machine learning, Interactive machine learning, Machine teaching},
	pages = {3005--3054},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/HN5ZN5HT/Mosqueira-Rey et al. - 2023 - Human-in-the-loop machine learning a state of the art.pdf:application/pdf},
}

@article{wu_survey_2022,
	title = {A {Survey} of {Human}-in-the-loop for {Machine} {Learning}},
	volume = {135},
	issn = {0167739X},
	url = {http://arxiv.org/abs/2108.00941},
	doi = {10.1016/j.future.2022.05.014},
	abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneﬁcial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-theloop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent humanin-the-loop. Using the above categorization, we summarize the major approaches in the ﬁeld; along with their technical strengths/ weaknesses, we have a simple classiﬁcation and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions.},
	language = {en},
	urldate = {2025-08-24},
	journal = {Future Generation Computer Systems},
	author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
	month = oct,
	year = {2022},
	note = {arXiv:2108.00941 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {364--381},
	file = {PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/P5Y3UKMG/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf:application/pdf},
}
