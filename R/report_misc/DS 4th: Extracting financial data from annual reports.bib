@misc{12EGovGEinzelnorm,
  title = {{\S} 12 {{EGovG}} - {{Einzelnorm}}},
  urldate = {2025-04-07},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5U8LSTMF/__12.html}
}

@misc{adhikariComparativeStudyPDF2024,
  title = {A {{Comparative Study}} of {{PDF Parsing Tools Across Diverse Document Categories}}},
  author = {Adhikari, Narayan S. and Agarwal, Shradha},
  year = {2024},
  month = oct,
  number = {arXiv:2410.09871},
  eprint = {2410.09871},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.09871},
  urldate = {2025-09-06},
  abstract = {PDF is one of the most prominent data formats, making PDF parsing crucial for information extraction and retrieval, particularly with the rise of RAG systems. While various PDF parsing tools exist, their effectiveness across different document types remains understudied, especially beyond academic papers. Our research aims to address this gap by comparing 10 popular PDF parsing tools across 6 document categories using the DocLayNet dataset. These tools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2, Unstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat and Table Transformer(TATR). We evaluated both text extraction and table detection capabilities. For text extraction, PyMuPDF and pypdfium generally outperformed others, but all parsers struggled with Scientific and Patent documents. For these challenging categories, learning-based tools like Nougat demonstrated superior performance. In table detection, TATR excelled in the Financial, Patent, Law \& Regulations, and Scientific categories. Table detection tool Camelot performed best for tender documents, while PyMuPDF performed superior in the Manual category. Our findings highlight the importance of selecting appropriate parsing tools based on document type and specific tasks, providing valuable insights for researchers and practitioners working with diverse document sources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries,Computer Science - Information Retrieval},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IG3BZ7PS/Adhikari and Agarwal - 2024 - A Comparative Study of PDF Parsing Tools Across Diverse Document Categories.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/FPK4BWJH/2410.html}
}

@phdthesis{ambacherDesigningUserfriendlyOptimized2024,
  title = {Designing a User-Friendly and Optimized Version of a User Interface for a Large Language Model ({{LLM}})},
  author = {Ambacher, Julian Emanuel},
  year = {2024},
  month = sep,
  urldate = {2025-08-08},
  abstract = {This thesis focuses on the creation of a user-friendly and efficient user interface (UI) for a large language model (LLM). As LLMs become more widely used in different fields, it is important to have a UI that meets user needs when interacting with them [1], [2], [3]. The main goal of this study is to find key design principles, design guidelines, and methods that ensure that the UI is easy to use, efficient, and satisfying. To achieve this, the thesis analyzes the interaction principles according to ISO 9241-110 on the existing LLM tool Chat GPT and focuses also on other different UI designs [4]. For this purpose, a prototype is created based on the analysis. This thesis executes a usability test to evaluate the usability of the initial prototype. It focuses on factors including flexibility, accuracy of system answers, ease of navigation, and feature efficacy. The feedback from this test is used to enhance the design. The second iteration of the prototype is evaluated using several measures, including the System Usability Scale (SUS) and the User Experience Questionnaire (UEQ). In addition, feedback was gathered from four experts in the field of UX and AI. The results showed improvements in ease of use, task efficiency, and overall user satisfaction. The results highlight the importance of clear guidance, privacy, and system responsiveness. In summary, the thesis provides a set of design guidelines and recommendations for creating a user-friendly user interface for LLMs. These include the aspects of user-centered design, contextual help features, and optimized workflows. In addition, it provides guidance for the future development of AI tools that prioritize both functionality and a positive user experience.},
  copyright = {https://rightsstatements.org/page/InC/1.0/?language=de},
  langid = {english},
  school = {Technische Hochschule Ingolstadt},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/F9IKW8QS/Ambacher - 2024 - Designing a user-friendly and optimized version of a user interface for a large language model (LLM).pdf}
}

@misc{amgenscholarsprogramHowInterpretViolin,
  title = {How to {{Interpret Violin Charts}}},
  author = {{Amgen Scholars Program}},
  urldate = {2025-08-29},
  abstract = {This text describes how to interpret violin charts.},
  howpublished = {https://www.labxchange.org/library/items/lb:LabXchange:46f64d7a:html:1},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Y9US5MB4/lbLabXchange46f64d7ahtml1.html}
}

@article{AnnualComprehensiveFinancial,
  title = {Annual {{Comprehensive Financial Report}} for the {{Fiscal Year Ended June}} 30, 2023},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/6XN9JPBJ/Annual Comprehensive Financial Report for the Fiscal Year Ended June 30, 2023.pdf}
}

@misc{auerDoclingTechnicalReport2024,
  title = {Docling {{Technical Report}}},
  author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and de Lima, Rafael Teixeira and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
  year = {2024},
  month = dec,
  number = {arXiv:2408.09869},
  eprint = {2408.09869},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.09869},
  urldate = {2025-04-07},
  abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CB5G2B4S/Auer et al. - 2024 - Docling Technical Report.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/I7GPQFC3/2408.html}
}

@misc{bbbinfrastruktur-verwaltungsgmbhGeschaftsbericht20232024,
  title = {Gesch{\"a}ftsbericht 2023},
  author = {{BBB Infrastruktur-Verwaltungs GmbH} and {BBB Infrastruktur GmbH \& Co. KG}},
  year = {2024},
  month = oct,
  urldate = {2025-01-10},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NN95EQA8/GB_BBB_Infra_2023.pdf}
}

@article{bentleyKnowingYouKnow2025,
  title = {Knowing You Know Nothing in the Age of Generative {{AI}}},
  author = {Bentley, Sarah V.},
  year = {2025},
  month = mar,
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  number = {1},
  pages = {409},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-025-04731-0},
  urldate = {2025-08-26},
  abstract = {Generative AI is a revolutionary new technology whose impact promises to democratise knowledge. And yet, unlike the printing press, which expanded knowledge through the amplification of one voice to many, generative AI reduces many voices to one. Its disruptive nature provides us with a timely reminder of both the power and fallibility of knowledge: its authorship, ownership, and veracity. This Comment situates generative AI within the evolutionary context of human information dissemination and knowledge production. Whilst acknowledging the extraordinary potential of this new tool, it asks the question---given that knowledge is probably our most valuable asset, should we not be applying more of it to better understand the impact of AI-mediated knowledge tools on both our information practices and their associated knowledge outcomes?},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Education,Psychology,Science,technology and society},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IJX3A6BT/Bentley - 2025 - Knowing you know nothing in the age of generative AI.pdf}
}

@misc{berkovUnderstandingLLMLogprobs2025,
  title = {Understanding {{LLM Logprobs}}},
  author = {Berkov, Mikhail},
  year = {2025},
  month = apr,
  journal = {Thinking Sand},
  urldate = {2025-09-03},
  abstract = {Where we explain what LLM logprobs are and what we can do with them.},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/VLDV7TGS/understanding-llm-logprobs-029794105903.html}
}

@misc{blecherNougatNeuralOptical2023,
  title = {Nougat: {{Neural Optical Understanding}} for {{Academic Documents}}},
  shorttitle = {Nougat},
  author = {Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  year = {2023},
  month = aug,
  number = {arXiv:2308.13418},
  eprint = {2308.13418},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.13418},
  urldate = {2025-09-06},
  abstract = {Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/QPDYIURR/Blecher et al. - 2023 - Nougat Neural Optical Understanding for Academic Documents.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/93ZDUZC8/2308.html}
}

@misc{bmireferato2MinikommentarGesetzZur2013,
  title = {Minikommentar Zum {{Gesetz}} Zur {{F{\"o}rderung}} Der Elektroni- Schen {{Verwaltung}} Sowie Zur {{{\"A}nderung}} Weiterer {{Vor-}} Schriften},
  editor = {{BMI, Referat O2}},
  year = {2013},
  month = jul,
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MXWPI7DC/e-government-gesetz-minikommentar.pdf}
}

@article{boseakEvaluatingLogLikelihoodConfidence2025,
  title = {Evaluating {{Log-Likelihood}} for {{Confidence Estimation}} in {{LLM-Based Multiple-Choice Question Answering}}},
  author = {Boseak, Christopher},
  year = {2025},
  journal = {Innovative Journal of Applied Science},
  volume = {02},
  number = {04},
  doi = {10.70844/ijas.2025.2.29},
  urldate = {2025-08-27},
  abstract = {Reliable deployment of Large Language Models (LLMs) in question-answering tasks requires well-calibrated confidence estimates. This work investigates whether token-level log-likelihoods---sums of log-probabilities over answer tokens---can serve as effective confidence signals in Multiple-Choice Question Answering (MCQA). We compare three methods: (1) Raw log-likelihood, (2) length-normalized log- likelihood and (3) conventional softmax-based choice probability. Across four diverse MCQA benchmarks, we find that no single scoring method is universally best. Length normalization can significantly improve calibration but may reduce accuracy, while softmax and raw log-likelihood yield identical predictions. These results highlight important trade-offs between calibration and accuracy and offer insights into selecting or adapting confidence measures for different tasks. Our findings inform the design of more trustworthy LLM-based QA systems and lay groundwork for broader uncertainty quantification efforts.}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2025-08-29},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ZX7G2H6X/Breiman - 2001 - Random Forests.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-09-03},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/K77MWHGN/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/SFTB6AHG/2005.html}
}

@misc{bundesanzeigerJahresabschlussGeschaftsjahrVom,
  title = {Jahresabschluss Zum {{Gesch{\"a}ftsjahr}} Vom 01.01.2008 Bis Zum 31.12.2008},
  author = {{Bundesanzeiger}},
  urldate = {2025-01-10},
  howpublished = {https://www.bundesanzeiger.de/pub/de/suchergebnis?7},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G3DTIWF3/suchergebnis.html}
}

@article{burnwalComprehensiveSurveyPrediction2023,
  title = {A {{Comprehensive Survey}} on {{Prediction Models}} and the {{Impact}} of {{XGBoost}}},
  author = {Burnwal, Yashkumar and Jaiswal, {\relax Dr}. R. C.},
  year = {2023},
  month = dec,
  journal = {International Journal for Research in Applied Science and Engineering Technology},
  volume = {11},
  number = {12},
  pages = {1552--1556},
  issn = {23219653},
  doi = {10.22214/ijraset.2023.57625},
  urldate = {2025-08-29},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DADY3HXQ/Burnwal and Jaiswal - 2023 - A Comprehensive Survey on Prediction Models and the Impact of XGBoost.pdf}
}

@article{caiSurveyMixtureExperts2025a,
  title = {A {{Survey}} on {{Mixture}} of {{Experts}} in {{Large Language Models}}},
  author = {Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  year = {2025},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {2407.06204},
  primaryclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2025.3554028},
  urldate = {2025-08-28},
  abstract = {Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TN8XG237/Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf}
}

@misc{carvalhoTFIDFCRFNovelSupervised2020,
  title = {{{TF-IDFC-RF}}: {{A Novel Supervised Term Weighting Scheme}}},
  shorttitle = {{{TF-IDFC-RF}}},
  author = {Carvalho, Flavio and Guedes, Gustavo Paiva},
  year = {2020},
  month = aug,
  number = {arXiv:2003.07193},
  eprint = {2003.07193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.07193},
  urldate = {2025-08-28},
  abstract = {Sentiment Analysis is a branch of Affective Computing usually considered a binary classification task. In this line of reasoning, Sentiment Analysis can be applied in several contexts to classify the attitude expressed in text samples, for example, movie reviews, sarcasm, among others. A common approach to represent text samples is the use of the Vector Space Model to compute numerical feature vectors consisting of the weight of terms. The most popular term weighting scheme is TF-IDF (Term Frequency - Inverse Document Frequency). It is an Unsupervised Weighting Scheme (UWS) since it does not consider the class information in the weighting of terms. Apart from that, there are Supervised Weighting Schemes (SWS), which consider the class information on term weighting calculation. Several SWS have been recently proposed, demonstrating better results than TF-IDF. In this scenario, this work presents a comparative study on different term weighting schemes and proposes a novel supervised term weighting scheme, named as TF-IDFC-RF (Term Frequency - Inverse Document Frequency in Class - Relevance Frequency). The effectiveness of TF-IDFC-RF is validated with SVM (Support Vector Machine) and NB (Naive Bayes) classifiers on four commonly used Sentiment Analysis datasets. TF-IDFC-RF shows promising results, outperforming all other weighting schemes on two datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4D3FHD43/Carvalho and Guedes - 2020 - TF-IDFC-RF A Novel Supervised Term Weighting Scheme.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/X5SCN2RE/2003.html}
}

@article{chamberlainKnowledgeNotEverything2020,
  title = {Knowledge Is Not Everything},
  author = {Chamberlain, Paul},
  year = {2020},
  month = jan,
  journal = {Design for Health},
  volume = {4},
  number = {1},
  pages = {1--3},
  publisher = {Routledge},
  issn = {2473-5132},
  doi = {10.1080/24735132.2020.1731203},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WN5KQAU3/Chamberlain - 2020 - Knowledge is not everything.pdf}
}

@misc{chengInstructionPreTrainingLanguage2024,
  title = {Instruction {{Pre-Training}}: {{Language Models}} Are {{Supervised Multitask Learners}}},
  shorttitle = {Instruction {{Pre-Training}}},
  author = {Cheng, Daixuan and Gu, Yuxian and Huang, Shaohan and Bi, Junyu and Huang, Minlie and Wei, Furu},
  year = {2024},
  month = nov,
  number = {arXiv:2406.14491},
  eprint = {2406.14491},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.14491},
  urldate = {2025-09-03},
  abstract = {Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CV978XHS/Cheng et al. - 2024 - Instruction Pre-Training Language Models are Supervised Multitask Learners.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LN46DD2W/2406.html}
}

@inproceedings{chenTableVLMMultimodalPretraining2023,
  title = {{{TableVLM}}: {{Multi-modal Pre-training}} for {{Table Structure Recognition}}},
  shorttitle = {{{TableVLM}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Leiyuan and Huang, Chengsong and Zheng, Xiaoqing and Lin, Jinshu and Huang, Xuanjing},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {2437--2449},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.137},
  urldate = {2025-01-10},
  abstract = {Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97\% in tree-editing-distance-score on ComplexTable.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7LWJ5DED/Chen et al. - 2023 - TableVLM Multi-modal Pre-training for Table Structure Recognition.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2025-08-29},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5GNMBEP4/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@book{collisBusinessResearchPractical2014,
  title = {Business Research: A Practical Guide for Undergraduate \& Postgraduate Students},
  shorttitle = {Business Research},
  author = {Collis, Jill and Hussey, Roger},
  year = {2014},
  edition = {1. Publ.; 4. ed},
  publisher = {Palgrave Macmillan},
  address = {Basingstoke},
  isbn = {978-0-230-30183-2},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7BY6JSH4/Collis and Hussey - 2014 - Business research a practical guide for undergraduate & postgraduate students.pdf}
}

@misc{daVisionGridTransformer2023,
  title = {Vision {{Grid Transformer}} for {{Document Layout Analysis}}},
  author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14978},
  eprint = {2308.14978},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14978},
  urldate = {2025-04-21},
  abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G5Y444YW/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LV6XRTVL/2308.html}
}

@misc{DiveDeepLearning,
  title = {Dive into {{Deep Learning}} --- {{Dive}} into {{Deep Learning}} 1.0.3 Documentation},
  urldate = {2025-09-03},
  howpublished = {https://d2l.ai/index.html},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/EF6MURIW/index.html}
}

@inproceedings{dongSurveyIncontextLearning2024,
  title = {A {{Survey}} on {{In-context Learning}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Li, Lei and Sui, Zhifang},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {1107--1128},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.64},
  urldate = {2025-09-03},
  abstract = {With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/438H3DCL/Dong et al. - 2024 - A Survey on In-context Learning.pdf}
}

@article{el-hajRetrievingClassifyingAnalysing2020,
  title = {Retrieving, Classifying and Analysing Narrative Commentary in Unstructured (Glossy) Annual Reports Published as {{PDF}} Files},
  author = {{El-Haj}, Mahmoud and Alves, Paulo and Rayson, Paul and Walker, Martin and Young, Steven},
  year = {2020},
  month = jan,
  journal = {Accounting and Business Research},
  volume = {50},
  number = {1},
  pages = {6--34},
  publisher = {Routledge},
  issn = {0001-4788},
  doi = {10.1080/00014788.2019.1609346},
  urldate = {2025-08-26},
  abstract = {We provide a methodological contribution by developing, describing and evaluating a method for automatically retrieving and analysing text from digital PDF annual report files published by firms listed on the London Stock Exchange (LSE). The retrieval method retains information on document structure, enabling clear delineation between narrative and financial statement components of reports, and between individual sections within the narratives component. Retrieval accuracy exceeds 95\% for manual validations using a random sample of 586 reports. Large-sample statistical validations using a comprehensive sample of reports published by non-financial LSE firms confirm that report length, narrative tone and (to a lesser degree) readability vary predictably with economic and regulatory factors. We demonstrate how the method is adaptable to non-English language documents and different regulatory regimes using a case study of Portuguese reports. We use the procedure to construct new research resources including corpora for commonly occurring annual report sections and a dataset of text properties for over 26,000 U.K. annual reports.},
  keywords = {Annual reports,narrative reporting,textual analysis,unstructured documents},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NE6CEKLF/El-Haj et al. - 2020 - Retrieving, classifying and analysing narrative commentary in unstructured (glossy) annual reports p.pdf}
}

@article{erridaDeterminantsOrganizationalChange2021,
  title = {The Determinants of Organizational Change Management Success: {{Literature}} Review and Case Study},
  shorttitle = {The Determinants of Organizational Change Management Success},
  author = {Errida, Abdelouahab and Lotfi, Bouchra},
  year = {2021},
  month = jan,
  journal = {International Journal of Engineering Business Management},
  volume = {13},
  pages = {18479790211016273},
  publisher = {SAGE Publications Ltd STM},
  issn = {1847-9790},
  doi = {10.1177/18479790211016273},
  urldate = {2025-09-02},
  abstract = {The main purpose of this study is identifying the various factors affecting change management success, as well as examine their relevance in the case of a Moroccan construction company. A combination of a literature review and research action was employed to this end. Specifically, an in-depth review of 37 organizational change management models was conducted to identify the factors that affect change management success. Additionally, a research action approach validated the identified factors. Several factors that affect organizational change management success were identified and categorized into 12 categories relevant to the successful implementation of organizational change initiatives within the case company. While further research is needed to explore the relevance of the identified factors in other organizations and sectors, this study provides an integrated understanding of change management success based on the analysis of various organizational change models. Understanding success factors can help managers implement change initiatives in their organizations effectively.},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5MWDGQNX/Errida and Lotfi - 2021 - The determinants of organizational change management success Literature review and case study.pdf}
}

@article{francisComparisonStudyOptical2025,
  title = {A Comparison Study on Optical Character Recognition Models in Mathematical Equations and in Any Language},
  author = {Francis, {\relax Sofi}. A. and Sangeetha, M.},
  year = {2025},
  month = mar,
  journal = {Results in Control and Optimization},
  volume = {18},
  pages = {100532},
  issn = {2666-7207},
  doi = {10.1016/j.rico.2025.100532},
  urldate = {2025-09-06},
  abstract = {Optical Character Recognition[OCR] is a technology that makes use of artificial intelligence and machine learning to extract readable text from documents, images, tags or any other type of sources. It allows one to convert characters and text objects into digital data that can be easily processed, analyzed, and modified. OCR can be applied to various types of languages in both written and spoken format. It can process everything from hand-written documents to typed-out text, making it a highly versatile technology. OCR makes use of a variety of algorithms and methods to process images, and then produces readable output, whatever language it is used for. This technology has the potential to be used for industries, banking, the medical field, security, and document storage among others. OCR faces significant challenges in accurately predicting language and mathematical expressions due to variations in handwriting styles, complex layouts, and the ambiguity of symbols. In this research, we propose assessing the results of different models that have been trained to identify an improved OCR system. The best OCR model is With the help of a decision tree model chosen.},
  keywords = {Artificial intelligence,Data analysis,Machine learning,Model comparison,Optical character recognition,Optimization,Pre-trained models,Text extraction},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/6STMK8IX/Francis and Sangeetha - 2025 - A comparison study on optical character recognition models in mathematical equations and in any lang.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/WLT4VA7I/S2666720725000189.html}
}

@misc{fuDeepThinkConfidence2025,
  title = {Deep {{Think}} with {{Confidence}}},
  author = {Fu, Yichao and Wang, Xuewei and Tian, Yuandong and Zhao, Jiawei},
  year = {2025},
  month = aug,
  number = {arXiv:2508.15260},
  eprint = {2508.15260},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.15260},
  urldate = {2025-09-03},
  abstract = {Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9\% accuracy and reduces generated tokens by up to 84.7\% compared to full parallel thinking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/V377Y4UC/Fu et al. - 2025 - Deep Think with Confidence.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/ZHDYE2Y3/2508.html}
}

@misc{gengGeneratingStructuredOutputs2025,
  title = {Generating {{Structured Outputs}} from {{Language Models}}: {{Benchmark}} and {{Studies}}},
  shorttitle = {Generating {{Structured Outputs}} from {{Language Models}}},
  author = {Geng, Saibo and Cooper, Hudson and Moskal, Micha{\l} and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
  year = {2025},
  month = jan,
  number = {arXiv:2501.10868},
  eprint = {2501.10868},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10868},
  urldate = {2025-07-06},
  abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/N8GNB3MS/Geng et al. - 2025 - Generating Structured Outputs from Language Models Benchmark and Studies.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/FMFPCEXK/2501.html}
}

@misc{googleGemma3nModel,
  title = {Gemma 3n Model Overview},
  author = {{Google}},
  journal = {Google AI for Developers},
  urldate = {2025-08-28},
  howpublished = {https://ai.google.dev/gemma/docs/gemma-3n},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/GYP8QF4W/gemma-3n.html}
}

@article{goughertyTestingReliabilityAIbased2024,
  title = {Testing the Reliability of an {{AI-based}} Large Language Model to Extract Ecological Information from the Scientific Literature},
  author = {Gougherty, Andrew V. and Clipp, Hannah L.},
  year = {2024},
  month = may,
  journal = {npj Biodiversity},
  volume = {3},
  number = {1},
  pages = {13},
  publisher = {Nature Publishing Group},
  issn = {2731-4243},
  doi = {10.1038/s44185-024-00043-9},
  urldate = {2025-07-09},
  abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy ({$>$}90\%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
  copyright = {2024 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
  langid = {english},
  keywords = {Data mining,Invasive species,Macroecology},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/END5Q8YY/Gougherty and Clipp - 2024 - Testing the reliability of an AI-based large language model to extract ecological information from t.pdf}
}

@misc{grandiniMetricsMultiClassClassification2020,
  title = {Metrics for {{Multi-Class Classification}}: An {{Overview}}},
  shorttitle = {Metrics for {{Multi-Class Classification}}},
  author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
  year = {2020},
  month = aug,
  number = {arXiv:2008.05756},
  eprint = {2008.05756},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.05756},
  urldate = {2025-01-10},
  abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WX2FF8RW/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overview.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/XVQUAGR6/2008.html}
}

@misc{grootendorstVisualGuideMixture2024,
  title = {A {{Visual Guide}} to {{Mixture}} of {{Experts}} ({{MoE}})},
  author = {Grootendorst, Maarten},
  year = {2024},
  month = feb,
  urldate = {2025-08-28},
  abstract = {Demystifying the role of MoE in Large Language Models},
  howpublished = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/BQZXXTCQ/a-visual-guide-to-mixture-of-experts.html}
}

@misc{haddouchiSurveyTaxonomyMethods2024,
  title = {A Survey and Taxonomy of Methods Interpreting Random Forest Models},
  author = {Haddouchi, Maissae and Berrado, Abdelaziz},
  year = {2024},
  month = jul,
  number = {arXiv:2407.12759},
  eprint = {2407.12759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.12759},
  urldate = {2025-08-29},
  abstract = {The interpretability of random forest (RF) models is a research topic of growing interest in the machine learning (ML) community. In the state of the art, RF is considered a powerful learning ensemble given its predictive performance, flexibility, and ease of use. Furthermore, the inner process of the RF model is understandable because it uses an intuitive and intelligible approach for building the RF decision tree ensemble. However, the RF resulting model is regarded as a "black box" because of its numerous deep decision trees. Gaining visibility over the entire process that induces the final decisions by exploring each decision tree is complicated, if not impossible. This complexity limits the acceptance and implementation of RF models in several fields of application. Several papers have tackled the interpretation of RF models. This paper aims to provide an extensive review of methods used in the literature to interpret RF resulting models. We have analyzed these methods and classified them based on different axes. Although this review is not exhaustive, it provides a taxonomy of various techniques that should guide users in choosing the most appropriate tools for interpreting RF models, depending on the interpretability aspects sought. It should also be valuable for researchers who aim to focus their work on the interpretability of RF or ML black boxes in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/56M2D6EZ/Haddouchi and Berrado - 2024 - A survey and taxonomy of methods interpreting random forest models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YMGLSP4M/2407.html}
}

@book{hgbHandelsgesetzbuchImBundesgesetzblatt2025,
  title = {Handelsgesetzbuch in Der Im {{Bundesgesetzblatt Teil III}}, {{Gliederungsnummer}} 4100-1, Ver{\"o}ffentlichten Bereinigten {{Fassung}}, Das Zuletzt Durch {{Artikel}} 1 Des {{Gesetzes}} Vom 28. {{Februar}} 2025 ({{BGBl}}. 2025 {{I Nr}}. 69) Ge{\"a}ndert Worden Ist},
  author = {{HGB}},
  year = {2025},
  month = feb
}

@article{hintzeViolinPlotsBox1998,
  title = {Violin {{Plots}}: {{A Box Plot-Density Trace Synergism}}},
  shorttitle = {Violin {{Plots}}},
  author = {Hintze, Jerry L. and Nelson, Ray D.},
  year = {1998},
  month = may,
  journal = {The American Statistician},
  volume = {52},
  number = {2},
  pages = {181--184},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.1998.10480559},
  urldate = {2025-08-29},
  abstract = {Many modifications build on Tukey's original box plot. A proposed further adaptation, the violin plot, pools the best statistical features of alternative graphical representations of batches of data. It adds the information available from local density estimates to the basic summary statistics inherent in box plots. This marriage of summary statistics and density shape into a single plot provides a useful tool for data analysis and exploration.},
  keywords = {Density estimation,Exploratory data analysis,Graphical techniques}
}

@article{hongChallengesAdvancesInformation2021,
  title = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}: A {{Review}}},
  shorttitle = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}},
  author = {Hong, Zhi and Ward, Logan and Chard, Kyle and Blaiszik, Ben and Foster, Ian},
  year = {2021},
  month = oct,
  journal = {JOM},
  volume = {73},
  pages = {1--18},
  doi = {10.1007/s11837-021-04902-9},
  abstract = {Scientific articles have long been the primary means of disseminating scientific discoveries. Over the centuries, valuable data and potentially groundbreaking insights have been collected and buried deep in the mountain of publications. In materials engineering, such data are spread across technical handbooks specification sheets, journal articles, and laboratory notebooks in myriad formats. Extracting information from papers on a large scale has been a tedious and time-consuming job to which few researchers have wanted to devote their limited time and effort, yet is an activity that is essential for modern data-driven design practices. However, in recent years, significant progress has been made by the computer science community on techniques for automated information extraction from free text. Yet, transformative application of these techniques to scientific literature remains elusive---due not to a lack of interest or effort but to technical and logistical challenges. Using the challenges in the materials science literature as a driving motivation, we review the gaps between state-of-the-art information extraction methods and the practical application of such methods to scientific texts, and offer a comprehensive overview of work that can be undertaken to close these gaps.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NLUQEC5N/Hong et al. - 2021 - Challenges and Advances in Information Extraction from Scientific Literature a Review.pdf}
}

@article{huangFailingsShapleyValues2024,
  title = {On the Failings of {{Shapley}} Values for Explainability},
  author = {Huang, Xuanxiang and {Marques-Silva}, Joao},
  year = {2024},
  month = aug,
  journal = {International Journal of Approximate Reasoning},
  series = {Synergies between {{Machine Learning}} and {{Reasoning}}},
  volume = {171},
  pages = {109112},
  issn = {0888-613X},
  doi = {10.1016/j.ijar.2023.109112},
  urldate = {2025-08-29},
  abstract = {Explainable Artificial Intelligence (XAI) is widely considered to be critical for building trust into the deployment of systems that integrate the use of machine learning (ML) models. For more than two decades Shapley values have been used as the theoretical underpinning for some methods of XAI, being commonly referred to as SHAP scores. Some of these methods of XAI now rank among the most widely used, including in high-risk domains. This paper proves that the existing definitions of SHAP scores will necessarily yield misleading information about the relative importance of features for predictions. The paper identifies a number of ways in which misleading information can be conveyed to human decision makers, and proves that there exist classifiers which will yield such misleading information. Furthermore, the paper offers empirical evidence that such theoretical limitations of SHAP scores are routinely observed in ML classifiers.},
  keywords = {Abductive explanations,Explainable AI (XAI),SHAP scores,Shapley values},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HFK43H9Z/Huang and Marques-Silva - 2024 - On the failings of Shapley values for explainability.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/5N3KKPSS/S0888613X23002438.html}
}

@misc{huComputingSHAPEfficiently2023,
  title = {Computing {{SHAP Efficiently Using Model Structure Information}}},
  author = {Hu, Linwei and Wang, Ke},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02417},
  eprint = {2309.02417},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.02417},
  urldate = {2025-08-28},
  abstract = {SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor \& Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/I88UL4MG/Hu and Wang - 2023 - Computing SHAP Efficiently Using Model Structure Information.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/RFT2NADP/2309.html}
}

@misc{ibmglobaltechnologyservicesToxicTerabyte2006,
  title = {The Toxic Terabyte},
  author = {{IBM Global Technology Services}},
  year = {2006},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CR4HILVE/The Toxic Terabyte.pdf}
}

@article{jhguchBoxPlot2025,
  title = {Box Plot},
  author = {{Jhguch}},
  year = {2025},
  month = jul,
  journal = {Wikipedia},
  urldate = {2025-08-29},
  abstract = {In descriptive statistics, a box plot or boxplot is a method for demonstrating graphically the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also called the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings in each subsection of the box-plot indicate the degree of dispersion (spread) and skewness of the data, which are usually described using the five-number summary. In addition, the box-plot allows one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1302092519},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Z2AQUUNU/index.html}
}

@article{jrHowBigData2015,
  title = {How {{Big Data Will Change Accounting}}},
  author = {Jr, J. and Moffitt, Kevin and Byrnes, Paul},
  year = {2015},
  month = feb,
  journal = {Accounting Horizons},
  volume = {29},
  pages = {150227130540002},
  doi = {10.2308/acch-51069},
  abstract = {SYNOPSIS Big Data will have increasingly important implications for accounting, even as new types of data become accessible. The video, audio, and textual information made available via Big Data can provide for improved managerial accounting, financial accounting, and financial reporting practices. In managerial accounting, Big Data will contribute to the development and evolution of effective management control systems and budgeting processes. In financial accounting, Big Data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision making. In reporting, Big Data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.}
}

@misc{kangScalableBestofNSelection2025,
  title = {Scalable {{Best-of-N Selection}} for {{Large Language Models}} via {{Self-Certainty}}},
  author = {Kang, Zhewei and Zhao, Xuandong and Song, Dawn},
  year = {2025},
  month = feb,
  number = {arXiv:2502.18581},
  eprint = {2502.18581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.18581},
  urldate = {2025-09-03},
  abstract = {Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size \$N\$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TRIFB679/Kang et al. - 2025 - Scalable Best-of-N Selection for Large Language Models via Self-Certainty.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/J5BJ9GCI/2502.html}
}

@misc{kaufLogProbabilitiesAre2024,
  title = {Log {{Probabilities Are}} a {{Reliable Estimate}} of {{Semantic Plausibility}} in {{Base}} and {{Instruction-Tuned Language Models}}},
  author = {Kauf, Carina and Chersoni, Emmanuele and Lenci, Alessandro and Fedorenko, Evelina and Ivanova, Anna A.},
  year = {2024},
  month = oct,
  number = {arXiv:2403.14859},
  eprint = {2403.14859},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14859},
  urldate = {2025-09-03},
  abstract = {Semantic plausibility (e.g. knowing that "the actor won the award" is more likely than "the actor won the battle") serves as an effective proxy for general world knowledge. Language models (LMs) capture vast amounts of world knowledge by learning distributional patterns in text, accessible via log probabilities (LogProbs) they assign to plausible vs. implausible outputs. The new generation of instruction-tuned LMs can now also provide explicit estimates of plausibility via prompting. Here, we evaluate the effectiveness of LogProbs and basic prompting to measure semantic plausibility, both in single-sentence minimal pairs (Experiment 1) and short context-dependent scenarios (Experiment 2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a more reliable measure of semantic plausibility than direct zero-shot prompting, which yields inconsistent and often poor results; (ii) instruction-tuning generally does not alter the sensitivity of LogProbs to semantic plausibility (although sometimes decreases it); (iii) across models, context mostly modulates LogProbs in expected ways, as measured by three novel metrics of context-sensitive plausibility and their match to explicit human plausibility judgments. We conclude that, even in the era of prompt-based evaluations, LogProbs constitute a useful metric of semantic plausibility, both in base and instruction-tuned LMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/AFPBCSVW/Kauf et al. - 2024 - Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Lan.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/6UN27L2M/2403.html}
}

@misc{kellyhongContextRotHow2025,
  title = {Context {{Rot}}: {{How Increasing Input Tokens Impacts LLM Performance}}},
  shorttitle = {Context {{Rot}}},
  author = {{Kelly Hong} and {Anton Troynikov}},
  year = {2025},
  month = jul,
  urldate = {2025-09-02},
  howpublished = {https://research.trychroma.com/context-rot},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/3B4GUEK8/context-rot.html}
}

@misc{khowajaAnalysisLlama4s2025,
  title = {Analysis of {{Llama}} 4's 10 {{Million Token Context Window Claim}}},
  author = {Khowaja, Sander Ali},
  year = {2025},
  month = apr,
  journal = {Medium},
  urldate = {2025-09-02},
  abstract = {Introduction},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ZUDVAV7I/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde.html}
}

@misc{kongOpenTabAdvancingLarge2024,
  title = {{{OpenTab}}: {{Advancing Large Language Models}} as {{Open-domain Table Reasoners}}},
  shorttitle = {{{OpenTab}}},
  author = {Kong, Kezhi and Zhang, Jiani and Shen, Zhengyuan and Srinivasan, Balasubramaniam and Lei, Chuan and Faloutsos, Christos and Rangwala, Huzefa and Karypis, George},
  year = {2024},
  month = apr,
  number = {arXiv:2402.14361},
  eprint = {2402.14361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.14361},
  urldate = {2025-09-05},
  abstract = {Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5\% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/KB7L54VG/Kong et al. - 2024 - OpenTab Advancing Large Language Models as Open-domain Table Reasoners.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/VULH43UL/2402.html}
}

@article{krzywinskiVisualizingSamplesBox2014,
  title = {Visualizing Samples with Box Plots},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2014},
  month = feb,
  journal = {Nature Methods},
  volume = {11},
  number = {2},
  pages = {119--120},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2813},
  langid = {english},
  pmid = {24645192},
  keywords = {Computer Graphics,Data Interpretation Statistical,Humans,Pattern Recognition Visual}
}

@misc{kukaShotBasedPromptingZeroShot,
  title = {Shot-{{Based Prompting}}: {{Zero-Shot}}, {{One-Shot}}, and {{Few-Shot Prompting}}},
  shorttitle = {Shot-{{Based Prompting}}},
  author = {Kuka", "Valeriia},
  urldate = {2025-09-03},
  abstract = {Learn about Shot-Based Prompting, a technique where you show the AI examples to guide output. Understand the benefits over zero- and one-shot prompts for better responses.},
  howpublished = {https://learnprompting.org/docs/basics/few\_shot},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/X48L3KB6/few_shot.html}
}

@inproceedings{kulkarniPruningRandomForest2012,
  title = {Pruning of {{Random Forest}} Classifiers: {{A}} Survey and Future Directions},
  shorttitle = {Pruning of {{Random Forest}} Classifiers},
  booktitle = {2012 {{International Conference}} on {{Data Science}} \& {{Engineering}} ({{ICDSE}})},
  author = {Kulkarni, Vrushali Y and Sinha, Pradeep K},
  year = {2012},
  month = jul,
  pages = {64--68},
  doi = {10.1109/ICDSE.2012.6282329},
  urldate = {2025-08-29},
  abstract = {Random Forest is an ensemble supervised machine learning technique. Based on bagging and random feature selection, number of decision trees (base classifiers) is generated and majority voting is taken for classification. For effective learning and classification of Random Forest, there is need for reducing number of trees (Pruning) in Random Forest. We have presented here systematic survey of pruning efforts of Random Forest classifier along with the required theoretical background. Most of the work for pruning takes static approach while recently dynamic pruning is being targeted. We have also generated a Comparison Chart by taking relevant parameters. There is research scope for analyzing behavior of Random forest, generating accurate and diverse base decision trees, truly dynamic pruning algorithm for Random Forest classifier, and generating optimal subset of Random forest.},
  keywords = {Accuracy,Classification,Correlation,Data mining,Data Mining,Decision trees,Diversity reception,Ensemble,Heuristic algorithms,Machine Learning,Pruning,Random Forest,Vegetation},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/R8MV9J46/6282329.html}
}

@article{kulkarniRandomForestClassifiers2013,
  title = {Random Forest Classifiers: {{A}} Survey and Future Research Directions},
  shorttitle = {Random Forest Classifiers},
  author = {Kulkarni, Vrushali and Sinha, Pradeep},
  year = {2013},
  month = jan,
  journal = {International Journal of Advanced Computing},
  volume = {36},
  pages = {1144--1153}
}

@misc{levySameTaskMore2024,
  title = {Same {{Task}}, {{More Tokens}}: The {{Impact}} of {{Input Length}} on the {{Reasoning Performance}} of {{Large Language Models}}},
  shorttitle = {Same {{Task}}, {{More Tokens}}},
  author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
  year = {2024},
  month = jul,
  number = {arXiv:2402.14848},
  eprint = {2402.14848},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.14848},
  urldate = {2025-09-02},
  abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/VUFPKXJL/Levy et al. - 2024 - Same Task, More Tokens the Impact of Input Length on the Reasoning Performance of Large Language Mo.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YFHFZFXH/2402.html}
}

@misc{liAddressingLastMile2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}: {{Using AIS Technologies}} to {{Enhance Governmental Financial Reporting}}},
  shorttitle = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}},
  author = {Li, Huaxia and Wei, Danyang (Kathy) and Moffitt, Kevin and Vasarhelyi, Miklos A.},
  year = {2023},
  month = mar,
  number = {4385883},
  eprint = {4385883},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4385883},
  urldate = {2025-08-26},
  abstract = {{$<$}p{$>$}Although the Open Government Data (OGD) initiative has gained global momentum over the past two decades, the lack of a machine-readable format for much finan},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting Information Systems (AIS) Open Government Data (OGD),Design Science,Government Accounting,Last Mile Problem,Open Government Data (OGD),PDF Extraction,Robotic Process Automation,Text Mining}
}

@article{liBuildingTrustMachine2024,
  title = {Toward {{Building Trust}} in {{Machine Learning Models}}: {{Quantifying}} the {{Explainability}} by {{SHAP}} and {{References}} to {{Human Strategy}}},
  shorttitle = {Toward {{Building Trust}} in {{Machine Learning Models}}},
  author = {Li, Zhaopeng and Bouazizi, Mondher and Ohtsuki, Tomoaki and Ishii, Masakuni and Nakahara, Eri},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {11010--11023},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3347796},
  urldate = {2025-08-29},
  abstract = {Local model-agnostic Explainable Artificial Intelligence (XAI), such as LIME or SHAP, has recently gained popularity among researchers and data scientists for explaining black box Machine Learning (ML) models. In the industry, practitioners focus not only on how these explanations can validate their models but also on how they can help maintain trust from end-users. Some studies attempted to measure this ability by quantifying what they refer to as the explainability or interpretability of ML models. In this paper, we introduce a new method for measuring explainability with reference to an approximated human model. We develop a human-friendly interface to strategically collect human decision-making and translate it into a set of logical rules and intuitions, or simply annotations. These annotations are then compared with the local explanations derived from common XAI tools. Through a human survey, we demonstrate that it is possible to quantify human intuition and empirically compare it to a given explanation, enabling a practical quantification of explainability. By relying on this new method, we identified several potential flaws in today's ML selection process. Furthermore, we demonstrate how our method can help to better evaluate ML models.},
  keywords = {Annotations,artificial intelligence,Artificial intelligence,Buildings,explainability,Explainable artificial intelligence,machine learning,Machine learning,Mathematical models,Predictive models,Remuneration,Solid modeling,Task analysis},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TVBGGTII/Li et al. - 2024 - Toward Building Trust in Machine Learning Models Quantifying the Explainability by SHAP and Referen.pdf}
}

@misc{liExtractingFinancialData2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Extracting {{Financial Data}} from {{Unstructured Sources}}: {{Leveraging Large Language Models}}},
  shorttitle = {Extracting {{Financial Data}} from {{Unstructured Sources}}},
  author = {Li, Huaxia and Gao, Haoyun (Harry) and Wu, Chengzhang and Vasarhelyi, Miklos A.},
  year = {2023},
  month = sep,
  number = {4567607},
  eprint = {4567607},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4567607},
  urldate = {2025-01-10},
  abstract = {This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from PDF-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate ESG reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5\% accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96\%. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting information systems Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models,ChatGPT,Data extraction,Design Science,Information processing,Large Language Model (LLM),PDF Reports,Unstructured data},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UEUDUT7Y/Li et al. - 2023 - Extracting Financial Data from Unstructured Sources Leveraging Large Language Models.pdf}
}

@misc{liProgrammingExampleSolved2024,
  title = {Is {{Programming}} by {{Example}} Solved by {{LLMs}}?},
  author = {Li, Wen-Ding and Ellis, Kevin},
  year = {2024},
  month = nov,
  number = {arXiv:2406.08316},
  eprint = {2406.08316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08316},
  urldate = {2025-08-26},
  abstract = {Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have "solved" PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ED7B99GA/Li and Ellis - 2024 - Is Programming by Example solved by LLMs.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/AVJXACWT/2406.html}
}

@misc{liProgrammingExampleSolved2024a,
  title = {Is {{Programming}} by {{Example}} Solved by {{LLMs}}?},
  author = {Li, Wen-Ding and Ellis, Kevin},
  year = {2024},
  month = nov,
  number = {arXiv:2406.08316},
  eprint = {2406.08316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08316},
  urldate = {2025-09-03},
  abstract = {Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have "solved" PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G8V224FP/Li and Ellis - 2024 - Is Programming by Example solved by LLMs.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/HC39EX2S/2406.html}
}

@misc{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = nov,
  number = {arXiv:2307.03172},
  eprint = {2307.03172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.03172},
  urldate = {2025-09-02},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HKGL3AXC/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long Contexts.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/2PBRPXQB/2307.html}
}

@misc{luLargeLanguageModel2024,
  title = {Large {{Language Model}} for {{Table Processing}}: {{A Survey}}},
  shorttitle = {Large {{Language Model}} for {{Table Processing}}},
  author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
  year = {2024},
  month = oct,
  eprint = {2402.05121},
  primaryclass = {cs},
  doi = {10.1007/s11704-024-40763-6},
  urldate = {2025-01-10},
  abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/85WXEHCW/Lu et al. - 2024 - Large Language Model for Table Processing A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/UXHNNW2A/2402.html}
}

@misc{lundbergExplainableAITrees2019,
  title = {Explainable {{AI}} for {{Trees}}: {{From Local Explanations}} to {{Global Understanding}}},
  shorttitle = {Explainable {{AI}} for {{Trees}}},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1905.04610},
  urldate = {2025-08-28},
  abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = nov,
  number = {arXiv:1705.07874},
  eprint = {1705.07874},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07874},
  urldate = {2025-08-28},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IKVK7ME5/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictions.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/9JCTXFDT/1705.html}
}

@misc{maEstimatingLLMUncertainty2025,
  title = {Estimating {{LLM Uncertainty}} with {{Evidence}}},
  author = {Ma, Huan and Chen, Jingdong and Zhou, Joey Tianyi and Wang, Guangyu and Zhang, Changqing},
  year = {2025},
  month = may,
  number = {arXiv:2502.00290},
  eprint = {2502.00290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.00290},
  urldate = {2025-09-03},
  abstract = {Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7I8VCVHT/Ma et al. - 2025 - Estimating LLM Uncertainty with Evidence.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/CMK95XPC/2502.html}
}

@misc{mahmudiNaturalLanguageProcessing2024,
  title = {Natural {{Language Processing}}},
  author = {Mahmudi, Aso},
  year = {22 Dec 2024 6:46pm},
  journal = {School of Computing and Information Systems},
  urldate = {2025-09-05},
  abstract = {Natural Language Processing},
  howpublished = {https://cis.unimelb.edu.au/research/artificial-intelligence/research/Natural-Language-Processing},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WQ38JSFQ/Natural-Language-Processing.html}
}

@misc{manningIntroductionInformationRetrieval2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  month = jul,
  journal = {Cambridge Aspire website},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511809071},
  urldate = {2025-08-28},
  abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
  howpublished = {https://www.cambridge.org/highereducation/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C},
  isbn = {9780511809071},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/THGVGUJU/Manning et al. - 2008 - Introduction to Information Retrieval.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/IZYLA5JU/669D108D20F556C5C30957D63B5AB65C.html}
}

@article{mienyeSurveyDecisionTrees2024,
  title = {A {{Survey}} of {{Decision Trees}}: {{Concepts}}, {{Algorithms}}, and {{Applications}}},
  shorttitle = {A {{Survey}} of {{Decision Trees}}},
  author = {Mienye, Ibomoiye Domor and Jere, Nobert},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {86716--86727},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3416838},
  urldate = {2025-08-29},
  abstract = {Machine learning (ML) has been instrumental in solving complex problems and significantly advancing different areas of our lives. Decision tree-based methods have gained significant popularity among the diverse range of ML algorithms due to their simplicity and interpretability. This paper presents a comprehensive overview of decision trees, including the core concepts, algorithms, applications, their early development to the recent high-performing ensemble algorithms and their mathematical and algorithmic representations, which are lacking in the literature and will be beneficial to ML researchers and industry experts. Some of the algorithms include classification and regression tree (CART), Iterative Dichotomiser 3 (ID3), C4.5, C5.0, Chi-squared Automatic Interaction Detection (CHAID), conditional inference trees, and other tree-based ensemble algorithms, such as random forest, gradient-boosted decision trees, and rotation forest. Their utilisation in recent literature is also discussed, focusing on applications in medical diagnosis and fraud detection.},
  keywords = {Algorithm design and analysis,Algorithms,C4.5,C5.0,CART,Classification algorithms,decision tree,Decision trees,ensemble learning,Ensemble learning,ID3,Indexes,machine learning,Machine learning,Machine learning algorithms,Peer-to-peer computing,Random forests},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IFNN3GXN/Mienye and Jere - 2024 - A Survey of Decision Trees Concepts, Algorithms, and Applications.pdf}
}

@misc{minaeeLargeLanguageModels2025,
  title = {Large {{Language Models}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}}},
  author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  year = {2025},
  month = mar,
  number = {arXiv:2402.06196},
  eprint = {2402.06196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06196},
  urldate = {2025-09-03},
  abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/FSLQC6XD/Minaee et al. - 2025 - Large Language Models A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/K3XMZUZI/2402.html}
}

@book{molnarInterpretableMachineLearning2025,
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  shorttitle = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  year = {2025},
  edition = {Third edition},
  publisher = {Christoph Molnar},
  address = {Munich, Germany},
  isbn = {978-3-911578-03-5},
  langid = {english},
  annotation = {OCLC: 1518801363}
}

@article{mosqueira-reyHumanintheloopMachineLearning2023,
  title = {Human-in-the-Loop Machine Learning: A State of the Art},
  shorttitle = {Human-in-the-Loop Machine Learning},
  author = {{Mosqueira-Rey}, Eduardo and {Hern{\'a}ndez-Pereira}, Elena and {Alonso-R{\'i}os}, David and {Bobes-Bascar{\'a}n}, Jos{\'e} and {Fern{\'a}ndez-Leal}, {\'A}ngel},
  year = {2023},
  month = apr,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {4},
  pages = {3005--3054},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10246-w},
  urldate = {2025-08-24},
  abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.},
  langid = {english},
  keywords = {Active learning,Curriculum learning,Explainable AI,Human-in-the-loop machine learning,Interactive machine learning,Machine teaching},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HN5ZN5HT/Mosqueira-Rey et al. - 2023 - Human-in-the-loop machine learning a state of the art.pdf}
}

@article{nanniniOperationalizingExplainableArtificial2024,
  title = {Operationalizing {{Explainable Artificial Intelligence}} in the {{European Union Regulatory Ecosystem}}},
  author = {Nannini, Luca and {Alonso-Moral}, Jose Maria and Catal{\'a}, Alejandro and Lama, Manuel and Barro, Sen{\'e}n},
  year = {2024},
  month = jul,
  journal = {IEEE Intelligent Systems},
  volume = {39},
  number = {4},
  pages = {37--48},
  issn = {1941-1294},
  doi = {10.1109/MIS.2024.3383155},
  urldate = {2025-08-29},
  abstract = {The European Union's (EU's) regulatory ecosystem presents challenges with balancing legal and sociotechnical drivers for explainable artificial intelligence (XAI) systems. Core tensions emerge on dimensions of oversight, user needs, and litigation. This article maps provisions on algorithmic transparency and explainability across major EU data, AI, and platform policies using qualitative analysis. We characterize the involved stakeholders and organizational implementation targets. Constraints become visible between useful transparency for accountability and confidentiality protections. Through an AI hiring system example, we explore the complications with operationalizing explainability. Customization is required to satisfy explainability desires within confidentiality and proportionality bounds. The findings advise technologists on prudent XAI technique selection given multidimensional tensions. The outcomes recommend that policy makers balance worthy transparency goals with cohesive legislation, enabling equitable dispute resolution.},
  keywords = {Artificial intelligence,Ecosystems,Europe,Explainable AI,Intelligent systems,Law,Regulation,Sociotechnical systems,Stakeholders},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/D87DMTD4/Nannini et al. - 2024 - Operationalizing Explainable Artificial Intelligence in the European Union Regulatory Ecosystem.pdf}
}

@misc{nassarTableFormerTableStructure2022,
  title = {{{TableFormer}}: {{Table Structure Understanding}} with {{Transformers}}},
  shorttitle = {{{TableFormer}}},
  author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01017},
  eprint = {2203.01017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.01017},
  urldate = {2025-04-07},
  abstract = {Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91\% to 98.5\% on simple tables and from 88.7\% to 95\% on complex tables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MHEENV38/Nassar et al. - 2022 - TableFormer Table Structure Understanding with Transformers.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/VXX5JEGY/2203.html}
}

@misc{natarajanHumanintheloopAIintheloopAutomate2024,
  title = {Human-in-the-Loop or {{AI-in-the-loop}}? {{Automate}} or {{Collaborate}}?},
  shorttitle = {Human-in-the-Loop or {{AI-in-the-loop}}?},
  author = {Natarajan, Sriraam and Mathur, Saurabh and Sidheekh, Sahil and Stammer, Wolfgang and Kersting, Kristian},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14232},
  eprint = {2412.14232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14232},
  urldate = {2025-08-24},
  abstract = {Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop (AI2L) systems: the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an AI2L perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an AI2L approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4YEXK3FV/Natarajan et al. - 2024 - Human-in-the-loop or AI-in-the-loop Automate or Collaborate.pdf}
}

@misc{osullivanMathematicsShapleyValues2023,
  title = {The Mathematics behind {{Shapley Values}}},
  author = {O'Sullivan, Conor},
  year = {2023},
  month = mar,
  urldate = {2025-08-28},
  abstract = {Shapley values are a fair way to divide the value of a game amongst its players. We explain the mathematics behind the Shapley value formula. To understand why it is fair, we also discuss the Shapley value axioms that the formula is derived from. The formula may seem scary but you will find it has an intuitive explanation.}
}

@misc{paulStateoftheArtModelArchitectures2025,
  title = {State-of-the-{{Art Model Architectures}} for {{Document Layout Analysis}}},
  author = {Paul, Rohan},
  year = {2025},
  month = aug,
  urldate = {2025-09-06},
  abstract = {Browse all previoiusly published AI Tutorials here.I write everyday for my readers on actionable AI.},
  howpublished = {https://www.rohan-paul.com/p/state-of-the-art-model-architectures},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/VU54LQ8H/state-of-the-art-model-architectures.html}
}

@misc{PromptEngineeringGuide,
  title = {Prompt {{Engineering Guide}}},
  urldate = {2025-09-03},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  howpublished = {https://www.promptingguide.ai/techniques},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/FGVIDCSR/techniques.html}
}

@misc{pythonologyExtractTextLinks2023,
  title = {Extract Text, Links, Images, Tables from {{Pdf}} with {{Python}} {\textbar} {{PyMuPDF}}, {{PyPdf}}, {{PdfPlumber}} Tutorial},
  author = {{Pythonology}},
  year = {2023},
  month = jan,
  urldate = {2025-01-10},
  abstract = {Use these Python libraries to convert a Pdf into an image, extract text, images, links, and tables from pdfs using the 3 popular Python libraries PyMuPDF, PyPdf, PdfPlumber. Here is source code and article I have written: https://pythonology.eu/what-is-the-be...  -- Support Pythonology -- https://www.buymeacoffee.com/pythonology -- Best Online Resource for Python -- Datacamp: The best online resource to learn Python, Web Scraping, Data analysis, and Data Science (Affiliate link) https://datacamp.pxf.io/pythonology}
}

@misc{qinLargeLanguageModels2025,
  title = {Large {{Language Models Meet NLP}}: {{A Survey}}},
  shorttitle = {Large {{Language Models Meet NLP}}},
  author = {Qin, Libo and Chen, Qiguang and Feng, Xiachong and Wu, Yang and Zhang, Yongheng and Li, Yinghui and Li, Min and Che, Wanxiang and Yu, Philip S.},
  year = {2025},
  month = aug,
  number = {arXiv:2405.12819},
  eprint = {2405.12819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12819},
  urldate = {2025-09-05},
  abstract = {While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen paradigm and (2) parameter-tuning paradigm to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the corresponding challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the potential and limitations of LLMs, while also serving as a practical guide for building effective LLMs in NLP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/RXCMRFX7/Qin et al. - 2025 - Large Language Models Meet NLP A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/X8VMGQK4/2405.html}
}

@misc{QwenQwen34BHugging2025,
  title = {Qwen/{{Qwen3-4B}} {$\cdot$} {{Hugging Face}}},
  year = {2025},
  month = aug,
  urldate = {2025-09-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/Qwen/Qwen3-4B},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/BY45LNQU/Qwen3-4B.html}
}

@misc{qwenteamQwen3ThinkDeeper2025,
  title = {Qwen3: {{Think Deeper}}, {{Act Faster}}},
  shorttitle = {Qwen3},
  author = {{Qwen Team}},
  year = {2025},
  month = apr,
  journal = {Qwen},
  urldate = {2025-08-28},
  abstract = {QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.},
  chapter = {blog},
  howpublished = {https://qwenlm.github.io/blog/qwen3/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/AWFWL8VG/qwen3.html}
}

@misc{rajExploringNewApproaches2025,
  title = {Exploring New {{Approaches}} for {{Information Retrieval}} through {{Natural Language Processing}}},
  author = {Raj, Manak and Mishra, Nidhi},
  year = {2025},
  month = may,
  number = {arXiv:2505.02199},
  eprint = {2505.02199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.02199},
  urldate = {2025-09-05},
  abstract = {This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7VPSKIJH/Raj and Mishra - 2025 - Exploring new Approaches for Information Retrieval through Natural Language Processing.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/HF5RVHD9/2505.html}
}

@misc{raschkaInstructionPretrainingLLMs2025,
  title = {Instruction {{Pretraining LLMs}}},
  author = {Raschka, Sebastian},
  year = {2025},
  month = jul,
  urldate = {2025-09-03},
  abstract = {The Latest Research in Instruction Finetuning},
  howpublished = {https://magazine.sebastianraschka.com/p/instruction-pretraining-llms},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UCYMKBW8/instruction-pretraining-llms.html}
}

@article{rathiImportanceTermWeighting2023,
  title = {The Importance of {{Term Weighting}} in Semantic Understanding of Text: {{A}} Review of Techniques},
  shorttitle = {The Importance of {{Term Weighting}} in Semantic Understanding of Text},
  author = {Rathi, R. N. and Mustafi, A.},
  year = {2023},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {82},
  number = {7},
  pages = {9761--9783},
  issn = {1573-7721},
  doi = {10.1007/s11042-022-12538-3},
  urldate = {2025-08-28},
  abstract = {In this paper we review a wide spectrum of techniques which have been proposed in literature to enable acceptable recognition of language and text by machines. We discuss many techniques which have been proposed by researchers in the field of term weighting and explore the mathematical foundations of these methods. Term weighting schemes have broadly been classified as supervised and statistical methods and we present numerous examples from both categories to highlight the difference in approaches between the two broad categories. We pay particular attention to the Vector Space Model and its variants which form the basis of many of the other methods which have been discussed in the paper.},
  langid = {english},
  keywords = {Term weighting,Term weighting techniques,Word embedding},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/JDRLDIWI/Rathi and Mustafi - 2023 - The importance of Term Weighting in semantic understanding of text A review of techniques.pdf}
}

@article{raymaekersFastLinearModel2024,
  title = {Fast {{Linear Model Trees}} by {{PILOT}}},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J. and Verdonck, Tim and Yao, Ruicong},
  year = {2024},
  month = sep,
  journal = {Machine Learning},
  volume = {113},
  number = {9},
  eprint = {2302.03931},
  primaryclass = {stat},
  pages = {6561--6610},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-024-06590-3},
  urldate = {2025-08-29},
  abstract = {Linear model trees are regression trees that incorporate linear models in the leaf nodes. This preserves the intuitive interpretation of decision trees and at the same time enables them to better capture linear relationships, which is hard for standard decision trees. But most existing methods for fitting linear model trees are time consuming and therefore not scalable to large data sets. In addition, they are more prone to overfitting and extrapolation issues than standard regression trees. In this paper we introduce PILOT, a new algorithm for linear model trees that is fast, regularized, stable and interpretable. PILOT trains in a greedy fashion like classic regression trees, but incorporates an \$L{\textasciicircum}2\$ boosting approach and a model selection rule for fitting linear models in the nodes. The abbreviation PILOT stands for \$PI\$ecewise \$L\$inear \$O\$rganic \$T\$ree, where `organic' refers to the fact that no pruning is carried out. PILOT has the same low time and space complexity as CART without its pruning. An empirical study indicates that PILOT tends to outperform standard decision trees and other linear model trees on a variety of data sets. Moreover, we prove its consistency in an additive model setting under weak assumptions. When the data is generated by a linear model, the convergence rate is polynomial.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/J2E4DARI/Raymaekers et al. - 2024 - Fast Linear Model Trees by PILOT.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/7QJ5Q43J/2302.html}
}

@article{rivera-lopezInductionDecisionTrees2022,
  title = {Induction of Decision Trees as Classification Models through Metaheuristics},
  author = {{Rivera-Lopez}, Rafael and {Canul-Reich}, Juana and {Mezura-Montes}, Efr{\'e}n and {Cruz-Ch{\'a}vez}, Marco Antonio},
  year = {2022},
  month = mar,
  journal = {Swarm and Evolutionary Computation},
  volume = {69},
  pages = {101006},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2021.101006},
  urldate = {2025-08-29},
  abstract = {The induction of decision trees is a widely-used approach to build classification models that guarantee high performance and expressiveness. Since a recursive-partitioning strategy guided for some splitting criterion is commonly used to induce these classifiers, overfitting, attribute selection bias, and instability to small training set changes are well-known problems in them. Other approaches, such as incremental induction, classifier ensembles, and the global search in the decision-tree-space, have been implemented to overcome these problems. In particular, metaheuristics such as simulated annealing, genetic algorithms, genetic programming, and ant colony optimization have been used to induce compact and accurate decision trees. This paper presents a state-of-the-art review of the use of single-solution-based metaheuristics and swarm and evolutionary computation algorithms to build decision trees as classification models. We outline the decision-tree-induction process components and detail the existing literature studies on metaheuristic-based approaches to building these classifiers. Several timelines showing the chronological order in which these approaches were introduced in the literature are included. A summary analysis of these studies is also conducted, focusing on their internal components and experimental studies. This work provides a useful reference point for future research in this field.},
  keywords = {Evolutionary algorithms,Machine learning,Single-solution-based metaheuristics,Swarm intelligence methods},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/45J79K5E/Rivera-Lopez et al. - 2022 - Induction of decision trees as classification models through metaheuristics.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/GK6H625Q/S2210650221001681.html}
}

@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The {{Probabilistic Relevance Framework}}: {{BM25}} and {{Beyond}}},
  shorttitle = {The {{Probabilistic Relevance Framework}}},
  author = {Robertson, Stephen and Zaragoza, Hugo},
  year = {2009},
  month = jan,
  journal = {Foundations and Trends in Information Retrieval},
  volume = {3},
  pages = {333--389},
  doi = {10.1561/1500000019},
  abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970---1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UCFZGJJN/Robertson and Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Beyond.pdf}
}

@incollection{robertsonSimpleEffectiveApproximations1994a,
  title = {Some {{Simple Effective Approximations}} to the 2-{{Poisson Model}} for {{Probabilistic Weighted Retrieval}}},
  booktitle = {{{SIGIR}} '94},
  author = {Robertson, S. E. and Walker, S.},
  editor = {Croft, Bruce W. and Van Rijsbergen, C. J.},
  year = {1994},
  pages = {232--241},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-2099-5_24},
  urldate = {2025-09-06},
  abstract = {The 2--Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.},
  isbn = {978-3-540-19889-5 978-1-4471-2099-5},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MBY8AFDJ/Robertson and Walker - 1994 - Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval.pdf}
}

@article{robertsonUnderstandingInverseDocument2004,
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for {{IDF}}},
  shorttitle = {Understanding Inverse Document Frequency},
  author = {Robertson, Stephen},
  year = {2004},
  month = oct,
  journal = {Journal of Documentation},
  volume = {60},
  number = {5},
  pages = {503--520},
  issn = {0022-0418},
  doi = {10.1108/00220410410560582},
  urldate = {2025-08-28},
  abstract = {The term-weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon's Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in the traditional probabilistic model of information retrieval.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/YJACLU4B/Robertson - 2004 - Understanding inverse document frequency on theoretical arguments for IDF.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/EM8M2D47/00220410410560582.html}
}

@article{saitoPrecisionRecallPlotMore2015,
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  year = {2015},
  month = mar,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0118432},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118432},
  urldate = {2025-08-24},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  langid = {english},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome-wide association studies,Interpolation,Measurement,MicroRNAs,Support vector machines},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CTBBR652/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers o.pdf}
}

@misc{senatsverwaltungfuerfinanzenberlinBeteiligungsbericht2024,
  title = {{Beteiligungsbericht}},
  author = {{Senatsverwaltung f{\"u}r Finanzen Berlin}},
  year = {2024},
  month = nov,
  urldate = {2025-01-10},
  abstract = {Beteiligungsberichte},
  langid = {ngerman},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/E2VJKHW4/beteiligungsbericht_2024_gesamt.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YIZUVU22/artikel.941274.html}
}

@incollection{shapley17ValueNPerson2016,
  title = {17. {{A Value}} for n-{{Person Games}}},
  booktitle = {Contributions to the {{Theory}} of {{Games}}, {{Volume II}}},
  author = {Shapley, L. S.},
  editor = {Kuhn, Harold William and Tucker, Albert William},
  year = {2016},
  month = mar,
  pages = {307--318},
  publisher = {Princeton University Press},
  urldate = {2025-08-28},
  isbn = {978-1-4008-8197-0},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/STWE3ECY/P295.pdf}
}

@misc{shengPdfTableUnifiedToolkit2024,
  title = {{{PdfTable}}: {{A Unified Toolkit}} for {{Deep Learning-Based Table Extraction}}},
  shorttitle = {{{PdfTable}}},
  author = {Sheng, Lei and Xu, Shuai-Shuai},
  year = {2024},
  month = sep,
  number = {arXiv:2409.05125},
  eprint = {2409.05125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05125},
  urldate = {2025-01-10},
  abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Y833Q63R/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/74UPJXVR/2409.html}
}

@inproceedings{smockPubTables1MComprehensiveTable2022,
  title = {{{PubTables-1M}}: {{Towards}} Comprehensive Table Extraction from Unstructured Documents},
  shorttitle = {{{PubTables-1M}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  year = {2022},
  month = jun,
  pages = {4624--4632},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00459},
  urldate = {2025-09-06},
  abstract = {Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/BEAFV9IW/Smock et al. - 2022 - PubTables-1M Towards comprehensive table extraction from unstructured documents.pdf}
}

@misc{tahirUnderstandingLLMContext2025,
  title = {🧠{{Understanding LLM Context Windows}}: {{Tokens}}, {{Attention}}, and {{Challenges}}},
  shorttitle = {🧠{{Understanding LLM Context Windows}}},
  author = {Tahir},
  year = {2025},
  month = feb,
  journal = {Medium},
  urldate = {2025-09-02},
  abstract = {Learn what a context window is in large language models, how tokenization works, and the challenges of long context windows. Discover the{\dots}},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/L83QPNZK/understanding-llm-context-windows-tokens-attention-and-challenges-c98e140f174d.html}
}

@misc{teamChameleonMixedModalEarlyFusion2024,
  title = {Chameleon: {{Mixed-Modal Early-Fusion Foundation Models}}},
  shorttitle = {Chameleon},
  author = {Team, Chameleon},
  year = {2024},
  month = may,
  number = {arXiv:2405.09818},
  eprint = {2405.09818},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.09818},
  urldate = {2025-08-28},
  abstract = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S3JJJ44G/Team - 2024 - Chameleon Mixed-Modal Early-Fusion Foundation Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/SXYSAWCZ/2405.html}
}

@misc{UserParticipationSoftware2010,
  title = {User {{Participation}} in {{Software Development Projects}} -- {{Communications}} of the {{ACM}}},
  year = {2010},
  month = mar,
  urldate = {2025-09-02},
  langid = {american},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DNJWJ8LB/user-participation-in-software-development-projects.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-09-03},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CNWJQVQ8/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/JCLDAZEZ/1706.html}
}

@misc{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2022},
  month = feb,
  number = {arXiv:2109.01652},
  eprint = {2109.01652},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.01652},
  urldate = {2025-09-03},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5RSKFHMM/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/KRY92BUA/2109.html}
}

@article{wickham40YearsBoxplots2011,
  title = {40 Years of Boxplots},
  author = {Wickham, H and Stryjewski, L},
  year = {2011},
  month = jan,
  abstract = {The boxplot plot has been around for over 40 years. This paper summarises the improvements, exten-sions and variations since Tukey first introduced his "schematic plot" in 1970. We focus particularly on richer displays of density and extensions to 2d.}
}

@misc{willardEfficientGuidedGeneration2023,
  title = {Efficient {{Guided Generation}} for {{Large Language Models}}},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  number = {arXiv:2307.09702},
  eprint = {2307.09702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09702},
  urldate = {2025-09-03},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MAPMIKPX/Willard and Louf - 2023 - Efficient Guided Generation for Large Language Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/4EH865A8/2307.html}
}

@article{wohlinDecisionmakingStructureSelecting2015,
  title = {Towards a Decision-Making Structure for Selecting a Research Design in Empirical Software Engineering},
  author = {Wohlin, Claes and Aurum, Ayb{\"u}ke},
  year = {2015},
  month = dec,
  journal = {Empirical Software Engineering},
  volume = {20},
  number = {6},
  pages = {1427--1455},
  issn = {1573-7616},
  doi = {10.1007/s10664-014-9319-7},
  urldate = {2025-08-24},
  abstract = {Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders' activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering research. The article provides an introduction to each decision point and its constituents, as well as to the relationships between the different parts in the decision-making structure. The intention is the structure should act as a starting point for the research design before going into the details of the research design chosen. The article provides an in-depth discussion of decision points in relation to the research design when conducting empirical research.},
  langid = {english},
  keywords = {Empirical software engineering research,Research design,Research methods,Selecting research method}
}

@book{wohlinExperimentationSoftwareEngineering2024,
  title = {Experimentation in {{Software Engineering}}},
  author = {Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
  year = {2024},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-69306-3},
  urldate = {2025-08-24},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-662-69305-6 978-3-662-69306-3},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/EPFIGBV5/Wohlin et al. - 2024 - Experimentation in Software Engineering.pdf}
}

@article{wuSurveyHumanintheloopMachine2022,
  title = {A {{Survey}} of {{Human-in-the-loop}} for {{Machine Learning}}},
  author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  year = {2022},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {135},
  eprint = {2108.00941},
  primaryclass = {cs},
  pages = {364--381},
  issn = {0167739X},
  doi = {10.1016/j.future.2022.05.014},
  urldate = {2025-08-24},
  abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-theloop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent humanin-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/ weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/P5Y3UKMG/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf}
}

@inproceedings{xuLayoutLMPretrainingText2020,
  title = {{{LayoutLM}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Document Image Understanding}}},
  shorttitle = {{{LayoutLM}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year = {2020},
  month = aug,
  eprint = {1912.13318},
  primaryclass = {cs},
  pages = {1192--1200},
  doi = {10.1145/3394486.3403172},
  urldate = {2025-04-27},
  abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IADVHXM6/Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Document Image Understanding.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LZU6Z3JF/1912.html}
}

@misc{yangFastTreeSHAPAccelerating2022,
  title = {Fast {{TreeSHAP}}: {{Accelerating SHAP Value Computation}} for {{Trees}}},
  shorttitle = {Fast {{TreeSHAP}}},
  author = {Yang, Jilei},
  year = {2022},
  month = jul,
  number = {arXiv:2109.09847},
  eprint = {2109.09847},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.09847},
  urldate = {2025-08-28},
  abstract = {SHAP (SHapley Additive exPlanation) values are one of the leading tools for interpreting machine learning models, with strong theoretical guarantees (consistency, local accuracy) and a wide availability of implementations and use cases. Even though computing SHAP values takes exponential time in general, TreeSHAP takes polynomial time on tree-based models. While the speedup is significant, TreeSHAP can still dominate the computation time of industry-level machine learning solutions on datasets with millions or more entries, causing delays in post-hoc model diagnosis and interpretation service. In this paper we present two new algorithms, Fast TreeSHAP v1 and v2, designed to improve the computational efficiency of TreeSHAP for large datasets. We empirically find that Fast TreeSHAP v1 is 1.5x faster than TreeSHAP while keeping the memory cost unchanged. Similarly, Fast TreeSHAP v2 is 2.5x faster than TreeSHAP, at the cost of a slightly higher memory usage, thanks to the pre-computation of expensive TreeSHAP steps. We also show that Fast TreeSHAP v2 is well-suited for multi-time model interpretations, resulting in as high as 3x faster explanation of newly incoming samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/KZCLC8ZF/Yang - 2022 - Fast TreeSHAP Accelerating SHAP Value Computation for Trees.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/3G2F5YW2/2109.html}
}

@misc{zhangDiveDeepLearning2023,
  title = {Dive into {{Deep Learning}}},
  author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  year = {2023},
  month = aug,
  number = {arXiv:2106.11342},
  eprint = {2106.11342},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.11342},
  urldate = {2025-09-03},
  abstract = {This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NH4XI5EQ/Zhang et al. - 2023 - Dive into Deep Learning.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/EWJXBAXJ/2106.html}
}

@misc{zhangDocumentParsingUnveiled2024,
  title = {Document {{Parsing Unveiled}}: {{Techniques}}, {{Challenges}}, and {{Prospects}} for {{Structured Information Extraction}}},
  shorttitle = {Document {{Parsing Unveiled}}},
  author = {Zhang, Qintong and Huang, Victor Shea-Jay and Wang, Bin and Zhang, Junyuan and Wang, Zhengren and Liang, Hao and Wang, Shawn and Lin, Matthieu and Zhang, Wentao and He, Conghui},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21169},
  eprint = {2410.21169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21169},
  urldate = {2025-08-26},
  abstract = {Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S5HD9YFM/Zhang et al. - 2024 - Document Parsing Unveiled Techniques, Challenges, and Prospects for Structured Information Extracti.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/NQMJG9YM/2410.html}
}

@misc{zhangMixtureExpertsLarge2025,
  title = {Mixture of {{Experts}} in {{Large Language Models}}},
  author = {Zhang, Danyang and Song, Junhao and Bi, Ziqian and Yuan, Yingfang and Wang, Tianyang and Yeong, Joe and Hao, Junfeng},
  year = {2025},
  month = jul,
  number = {arXiv:2507.11181},
  eprint = {2507.11181},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.11181},
  urldate = {2025-08-28},
  abstract = {This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DM22BENS/Zhang et al. - 2025 - Mixture of Experts in Large Language Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LS7IP732/2507.html}
}

@misc{zhaoCalibrateUseImproving2021,
  title = {Calibrate {{Before Use}}: {{Improving Few-Shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  year = {2021},
  month = jun,
  number = {arXiv:2102.09690},
  eprint = {2102.09690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09690},
  urldate = {2025-09-03},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute) and reduces variance across different choices of the prompt.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/8DLKPI77/Zhao et al. - 2021 - Calibrate Before Use Improving Few-Shot Performance of Language Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/XPTQ6W5R/2102.html}
}

@misc{zhongPubLayNetLargestDataset2019,
  title = {{{PubLayNet}}: Largest Dataset Ever for Document Layout Analysis},
  shorttitle = {{{PubLayNet}}},
  author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  year = {2019},
  month = aug,
  number = {arXiv:1908.07836},
  eprint = {1908.07836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.07836},
  urldate = {2025-04-21},
  abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4REPKJNM/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layout analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/TKQLXWYB/1908.html}
}

@misc{zhuLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Information Retrieval}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Information Retrieval}}},
  author = {Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Dou, Zhicheng and Wen, Ji-Rong},
  year = {2024},
  month = jan,
  number = {arXiv:2308.07107},
  eprint = {2308.07107},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07107},
  urldate = {2025-09-05},
  abstract = {As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/I6AGGT6F/Zhu et al. - 2024 - Large Language Models for Information Retrieval A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/W7Y7VSZB/2308.html}
}
