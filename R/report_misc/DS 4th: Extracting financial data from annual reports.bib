@misc{12EGovGEinzelnorm,
  title = {{\S} 12 {{EGovG}} - {{Einzelnorm}}},
  urldate = {2025-04-07},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5U8LSTMF/__12.html}
}

@phdthesis{ambacherDesigningUserfriendlyOptimized2024,
  title = {Designing a User-Friendly and Optimized Version of a User Interface for a Large Language Model ({{LLM}})},
  author = {Ambacher, Julian Emanuel},
  year = {2024},
  month = sep,
  urldate = {2025-08-08},
  abstract = {This thesis focuses on the creation of a user-friendly and efficient user interface (UI) for a large language model (LLM). As LLMs become more widely used in different fields, it is important to have a UI that meets user needs when interacting with them [1], [2], [3]. The main goal of this study is to find key design principles, design guidelines, and methods that ensure that the UI is easy to use, efficient, and satisfying. To achieve this, the thesis analyzes the interaction principles according to ISO 9241-110 on the existing LLM tool Chat GPT and focuses also on other different UI designs [4]. For this purpose, a prototype is created based on the analysis. This thesis executes a usability test to evaluate the usability of the initial prototype. It focuses on factors including flexibility, accuracy of system answers, ease of navigation, and feature efficacy. The feedback from this test is used to enhance the design. The second iteration of the prototype is evaluated using several measures, including the System Usability Scale (SUS) and the User Experience Questionnaire (UEQ). In addition, feedback was gathered from four experts in the field of UX and AI. The results showed improvements in ease of use, task efficiency, and overall user satisfaction. The results highlight the importance of clear guidance, privacy, and system responsiveness. In summary, the thesis provides a set of design guidelines and recommendations for creating a user-friendly user interface for LLMs. These include the aspects of user-centered design, contextual help features, and optimized workflows. In addition, it provides guidance for the future development of AI tools that prioritize both functionality and a positive user experience.},
  copyright = {https://rightsstatements.org/page/InC/1.0/?language=de},
  langid = {english},
  school = {Technische Hochschule Ingolstadt},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/F9IKW8QS/Ambacher - 2024 - Designing a user-friendly and optimized version of a user interface for a large language model (LLM).pdf}
}

@misc{amgenscholarsprogramHowInterpretViolin,
  title = {How to {{Interpret Violin Charts}}},
  author = {{Amgen Scholars Program}},
  urldate = {2025-08-29},
  abstract = {This text describes how to interpret violin charts.},
  howpublished = {https://www.labxchange.org/library/items/lb:LabXchange:46f64d7a:html:1},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Y9US5MB4/lbLabXchange46f64d7ahtml1.html}
}

@article{AnnualComprehensiveFinancial,
  title = {Annual {{Comprehensive Financial Report}} for the {{Fiscal Year Ended June}} 30, 2023},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/6XN9JPBJ/Annual Comprehensive Financial Report for the Fiscal Year Ended June 30, 2023.pdf}
}

@misc{auerDoclingTechnicalReport2024,
  title = {Docling {{Technical Report}}},
  author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and de Lima, Rafael Teixeira and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
  year = {2024},
  month = dec,
  number = {arXiv:2408.09869},
  eprint = {2408.09869},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.09869},
  urldate = {2025-04-07},
  abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CB5G2B4S/Auer et al. - 2024 - Docling Technical Report.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/I7GPQFC3/2408.html}
}

@misc{bbbinfrastruktur-verwaltungsgmbhGeschaftsbericht20232024,
  title = {Gesch{\"a}ftsbericht 2023},
  author = {{BBB Infrastruktur-Verwaltungs GmbH} and {BBB Infrastruktur GmbH \& Co. KG}},
  year = {2024},
  month = oct,
  urldate = {2025-01-10},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NN95EQA8/GB_BBB_Infra_2023.pdf}
}

@article{bentleyKnowingYouKnow2025,
  title = {Knowing You Know Nothing in the Age of Generative {{AI}}},
  author = {Bentley, Sarah V.},
  year = {2025},
  month = mar,
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  number = {1},
  pages = {409},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-025-04731-0},
  urldate = {2025-08-26},
  abstract = {Generative AI is a revolutionary new technology whose impact promises to democratise knowledge. And yet, unlike the printing press, which expanded knowledge through the amplification of one voice to many, generative AI reduces many voices to one. Its disruptive nature provides us with a timely reminder of both the power and fallibility of knowledge: its authorship, ownership, and veracity. This Comment situates generative AI within the evolutionary context of human information dissemination and knowledge production. Whilst acknowledging the extraordinary potential of this new tool, it asks the question---given that knowledge is probably our most valuable asset, should we not be applying more of it to better understand the impact of AI-mediated knowledge tools on both our information practices and their associated knowledge outcomes?},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Education,Psychology,Science,technology and society},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IJX3A6BT/Bentley - 2025 - Knowing you know nothing in the age of generative AI.pdf}
}

@misc{bmireferato2MinikommentarGesetzZur2013,
  title = {Minikommentar Zum {{Gesetz}} Zur {{F{\"o}rderung}} Der Elektroni- Schen {{Verwaltung}} Sowie Zur {{{\"A}nderung}} Weiterer {{Vor-}} Schriften},
  editor = {{BMI, Referat O2}},
  year = {2013},
  month = jul,
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MXWPI7DC/e-government-gesetz-minikommentar.pdf}
}

@article{boseakEvaluatingLogLikelihoodConfidence2025,
  title = {Evaluating {{Log-Likelihood}} for {{Confidence Estimation}} in {{LLM-Based Multiple-Choice Question Answering}}},
  author = {Boseak, Christopher},
  year = {2025},
  journal = {Innovative Journal of Applied Science},
  volume = {02},
  number = {04},
  doi = {10.70844/ijas.2025.2.29},
  urldate = {2025-08-27},
  abstract = {Reliable deployment of Large Language Models (LLMs) in question-answering tasks requires well-calibrated confidence estimates. This work investigates whether token-level log-likelihoods---sums of log-probabilities over answer tokens---can serve as effective confidence signals in Multiple-Choice Question Answering (MCQA). We compare three methods: (1) Raw log-likelihood, (2) length-normalized log- likelihood and (3) conventional softmax-based choice probability. Across four diverse MCQA benchmarks, we find that no single scoring method is universally best. Length normalization can significantly improve calibration but may reduce accuracy, while softmax and raw log-likelihood yield identical predictions. These results highlight important trade-offs between calibration and accuracy and offer insights into selecting or adapting confidence measures for different tasks. Our findings inform the design of more trustworthy LLM-based QA systems and lay groundwork for broader uncertainty quantification efforts.}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2025-08-29},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ZX7G2H6X/Breiman - 2001 - Random Forests.pdf}
}

@misc{bundesanzeigerJahresabschlussGeschaftsjahrVom,
  title = {Jahresabschluss Zum {{Gesch{\"a}ftsjahr}} Vom 01.01.2008 Bis Zum 31.12.2008},
  author = {{Bundesanzeiger}},
  urldate = {2025-01-10},
  howpublished = {https://www.bundesanzeiger.de/pub/de/suchergebnis?7},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G3DTIWF3/suchergebnis.html}
}

@article{burnwalComprehensiveSurveyPrediction2023,
  title = {A {{Comprehensive Survey}} on {{Prediction Models}} and the {{Impact}} of {{XGBoost}}},
  author = {Burnwal, Yashkumar and Jaiswal, {\relax Dr}. R. C.},
  year = {2023},
  month = dec,
  journal = {International Journal for Research in Applied Science and Engineering Technology},
  volume = {11},
  number = {12},
  pages = {1552--1556},
  issn = {23219653},
  doi = {10.22214/ijraset.2023.57625},
  urldate = {2025-08-29},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DADY3HXQ/Burnwal and Jaiswal - 2023 - A Comprehensive Survey on Prediction Models and the Impact of XGBoost.pdf}
}

@article{caiSurveyMixtureExperts2025a,
  title = {A {{Survey}} on {{Mixture}} of {{Experts}} in {{Large Language Models}}},
  author = {Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  year = {2025},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {2407.06204},
  primaryclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2025.3554028},
  urldate = {2025-08-28},
  abstract = {Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TN8XG237/Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf}
}

@misc{carvalhoTFIDFCRFNovelSupervised2020,
  title = {{{TF-IDFC-RF}}: {{A Novel Supervised Term Weighting Scheme}}},
  shorttitle = {{{TF-IDFC-RF}}},
  author = {Carvalho, Flavio and Guedes, Gustavo Paiva},
  year = {2020},
  month = aug,
  number = {arXiv:2003.07193},
  eprint = {2003.07193},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.07193},
  urldate = {2025-08-28},
  abstract = {Sentiment Analysis is a branch of Affective Computing usually considered a binary classification task. In this line of reasoning, Sentiment Analysis can be applied in several contexts to classify the attitude expressed in text samples, for example, movie reviews, sarcasm, among others. A common approach to represent text samples is the use of the Vector Space Model to compute numerical feature vectors consisting of the weight of terms. The most popular term weighting scheme is TF-IDF (Term Frequency - Inverse Document Frequency). It is an Unsupervised Weighting Scheme (UWS) since it does not consider the class information in the weighting of terms. Apart from that, there are Supervised Weighting Schemes (SWS), which consider the class information on term weighting calculation. Several SWS have been recently proposed, demonstrating better results than TF-IDF. In this scenario, this work presents a comparative study on different term weighting schemes and proposes a novel supervised term weighting scheme, named as TF-IDFC-RF (Term Frequency - Inverse Document Frequency in Class - Relevance Frequency). The effectiveness of TF-IDFC-RF is validated with SVM (Support Vector Machine) and NB (Naive Bayes) classifiers on four commonly used Sentiment Analysis datasets. TF-IDFC-RF shows promising results, outperforming all other weighting schemes on two datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4D3FHD43/Carvalho and Guedes - 2020 - TF-IDFC-RF A Novel Supervised Term Weighting Scheme.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/X5SCN2RE/2003.html}
}

@article{chamberlainKnowledgeNotEverything2020,
  title = {Knowledge Is Not Everything},
  author = {Chamberlain, Paul},
  year = {2020},
  month = jan,
  journal = {Design for Health},
  volume = {4},
  number = {1},
  pages = {1--3},
  publisher = {Routledge},
  issn = {2473-5132},
  doi = {10.1080/24735132.2020.1731203},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WN5KQAU3/Chamberlain - 2020 - Knowledge is not everything.pdf}
}

@inproceedings{chenTableVLMMultimodalPretraining2023,
  title = {{{TableVLM}}: {{Multi-modal Pre-training}} for {{Table Structure Recognition}}},
  shorttitle = {{{TableVLM}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Leiyuan and Huang, Chengsong and Zheng, Xiaoqing and Lin, Jinshu and Huang, Xuanjing},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {2437--2449},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.137},
  urldate = {2025-01-10},
  abstract = {Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97\% in tree-editing-distance-score on ComplexTable.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7LWJ5DED/Chen et al. - 2023 - TableVLM Multi-modal Pre-training for Table Structure Recognition.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2025-08-29},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5GNMBEP4/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@book{collisBusinessResearchPractical2014,
  title = {Business Research: A Practical Guide for Undergraduate \& Postgraduate Students},
  shorttitle = {Business Research},
  author = {Collis, Jill and Hussey, Roger},
  year = {2014},
  edition = {1. Publ.; 4. ed},
  publisher = {Palgrave Macmillan},
  address = {Basingstoke},
  isbn = {978-0-230-30183-2},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7BY6JSH4/Collis and Hussey - 2014 - Business research a practical guide for undergraduate & postgraduate students.pdf}
}

@misc{daVisionGridTransformer2023,
  title = {Vision {{Grid Transformer}} for {{Document Layout Analysis}}},
  author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14978},
  eprint = {2308.14978},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14978},
  urldate = {2025-04-21},
  abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G5Y444YW/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LV6XRTVL/2308.html}
}

@article{el-hajRetrievingClassifyingAnalysing2020,
  title = {Retrieving, Classifying and Analysing Narrative Commentary in Unstructured (Glossy) Annual Reports Published as {{PDF}} Files},
  author = {{El-Haj}, Mahmoud and Alves, Paulo and Rayson, Paul and Walker, Martin and Young, Steven},
  year = {2020},
  month = jan,
  journal = {Accounting and Business Research},
  volume = {50},
  number = {1},
  pages = {6--34},
  publisher = {Routledge},
  issn = {0001-4788},
  doi = {10.1080/00014788.2019.1609346},
  urldate = {2025-08-26},
  abstract = {We provide a methodological contribution by developing, describing and evaluating a method for automatically retrieving and analysing text from digital PDF annual report files published by firms listed on the London Stock Exchange (LSE). The retrieval method retains information on document structure, enabling clear delineation between narrative and financial statement components of reports, and between individual sections within the narratives component. Retrieval accuracy exceeds 95\% for manual validations using a random sample of 586 reports. Large-sample statistical validations using a comprehensive sample of reports published by non-financial LSE firms confirm that report length, narrative tone and (to a lesser degree) readability vary predictably with economic and regulatory factors. We demonstrate how the method is adaptable to non-English language documents and different regulatory regimes using a case study of Portuguese reports. We use the procedure to construct new research resources including corpora for commonly occurring annual report sections and a dataset of text properties for over 26,000 U.K. annual reports.},
  keywords = {Annual reports,narrative reporting,textual analysis,unstructured documents},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NE6CEKLF/El-Haj et al. - 2020 - Retrieving, classifying and analysing narrative commentary in unstructured (glossy) annual reports p.pdf}
}

@misc{gengGeneratingStructuredOutputs2025,
  title = {Generating {{Structured Outputs}} from {{Language Models}}: {{Benchmark}} and {{Studies}}},
  shorttitle = {Generating {{Structured Outputs}} from {{Language Models}}},
  author = {Geng, Saibo and Cooper, Hudson and Moskal, Micha{\l} and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
  year = {2025},
  month = jan,
  number = {arXiv:2501.10868},
  eprint = {2501.10868},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10868},
  urldate = {2025-07-06},
  abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/N8GNB3MS/Geng et al. - 2025 - Generating Structured Outputs from Language Models Benchmark and Studies.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/FMFPCEXK/2501.html}
}

@misc{googleGemma3nModel,
  title = {Gemma 3n Model Overview},
  author = {{Google}},
  journal = {Google AI for Developers},
  urldate = {2025-08-28},
  howpublished = {https://ai.google.dev/gemma/docs/gemma-3n},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/GYP8QF4W/gemma-3n.html}
}

@article{goughertyTestingReliabilityAIbased2024,
  title = {Testing the Reliability of an {{AI-based}} Large Language Model to Extract Ecological Information from the Scientific Literature},
  author = {Gougherty, Andrew V. and Clipp, Hannah L.},
  year = {2024},
  month = may,
  journal = {npj Biodiversity},
  volume = {3},
  number = {1},
  pages = {13},
  publisher = {Nature Publishing Group},
  issn = {2731-4243},
  doi = {10.1038/s44185-024-00043-9},
  urldate = {2025-07-09},
  abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy ({$>$}90\%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
  copyright = {2024 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
  langid = {english},
  keywords = {Data mining,Invasive species,Macroecology},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/END5Q8YY/Gougherty and Clipp - 2024 - Testing the reliability of an AI-based large language model to extract ecological information from t.pdf}
}

@misc{grandiniMetricsMultiClassClassification2020,
  title = {Metrics for {{Multi-Class Classification}}: An {{Overview}}},
  shorttitle = {Metrics for {{Multi-Class Classification}}},
  author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
  year = {2020},
  month = aug,
  number = {arXiv:2008.05756},
  eprint = {2008.05756},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.05756},
  urldate = {2025-01-10},
  abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WX2FF8RW/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overview.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/XVQUAGR6/2008.html}
}

@misc{grootendorstVisualGuideMixture2024,
  title = {A {{Visual Guide}} to {{Mixture}} of {{Experts}} ({{MoE}})},
  author = {Grootendorst, Maarten},
  year = {2024},
  month = feb,
  urldate = {2025-08-28},
  abstract = {Demystifying the role of MoE in Large Language Models},
  howpublished = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/BQZXXTCQ/a-visual-guide-to-mixture-of-experts.html}
}

@misc{haddouchiSurveyTaxonomyMethods2024,
  title = {A Survey and Taxonomy of Methods Interpreting Random Forest Models},
  author = {Haddouchi, Maissae and Berrado, Abdelaziz},
  year = {2024},
  month = jul,
  number = {arXiv:2407.12759},
  eprint = {2407.12759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.12759},
  urldate = {2025-08-29},
  abstract = {The interpretability of random forest (RF) models is a research topic of growing interest in the machine learning (ML) community. In the state of the art, RF is considered a powerful learning ensemble given its predictive performance, flexibility, and ease of use. Furthermore, the inner process of the RF model is understandable because it uses an intuitive and intelligible approach for building the RF decision tree ensemble. However, the RF resulting model is regarded as a "black box" because of its numerous deep decision trees. Gaining visibility over the entire process that induces the final decisions by exploring each decision tree is complicated, if not impossible. This complexity limits the acceptance and implementation of RF models in several fields of application. Several papers have tackled the interpretation of RF models. This paper aims to provide an extensive review of methods used in the literature to interpret RF resulting models. We have analyzed these methods and classified them based on different axes. Although this review is not exhaustive, it provides a taxonomy of various techniques that should guide users in choosing the most appropriate tools for interpreting RF models, depending on the interpretability aspects sought. It should also be valuable for researchers who aim to focus their work on the interpretability of RF or ML black boxes in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/56M2D6EZ/Haddouchi and Berrado - 2024 - A survey and taxonomy of methods interpreting random forest models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YMGLSP4M/2407.html}
}

@book{hgbHandelsgesetzbuchImBundesgesetzblatt2025,
  title = {Handelsgesetzbuch in Der Im {{Bundesgesetzblatt Teil III}}, {{Gliederungsnummer}} 4100-1, Ver{\"o}ffentlichten Bereinigten {{Fassung}}, Das Zuletzt Durch {{Artikel}} 1 Des {{Gesetzes}} Vom 28. {{Februar}} 2025 ({{BGBl}}. 2025 {{I Nr}}. 69) Ge{\"a}ndert Worden Ist},
  author = {{HGB}},
  year = {2025},
  month = feb
}

@article{hintzeViolinPlotsBox1998,
  title = {Violin {{Plots}}: {{A Box Plot-Density Trace Synergism}}},
  shorttitle = {Violin {{Plots}}},
  author = {Hintze, Jerry L. and Nelson, Ray D.},
  year = {1998},
  month = may,
  journal = {The American Statistician},
  volume = {52},
  number = {2},
  pages = {181--184},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.1998.10480559},
  urldate = {2025-08-29},
  abstract = {Many modifications build on Tukey's original box plot. A proposed further adaptation, the violin plot, pools the best statistical features of alternative graphical representations of batches of data. It adds the information available from local density estimates to the basic summary statistics inherent in box plots. This marriage of summary statistics and density shape into a single plot provides a useful tool for data analysis and exploration.},
  keywords = {Density estimation,Exploratory data analysis,Graphical techniques}
}

@article{hongChallengesAdvancesInformation2021,
  title = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}: A {{Review}}},
  shorttitle = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}},
  author = {Hong, Zhi and Ward, Logan and Chard, Kyle and Blaiszik, Ben and Foster, Ian},
  year = {2021},
  month = oct,
  journal = {JOM},
  volume = {73},
  pages = {1--18},
  doi = {10.1007/s11837-021-04902-9},
  abstract = {Scientific articles have long been the primary means of disseminating scientific discoveries. Over the centuries, valuable data and potentially groundbreaking insights have been collected and buried deep in the mountain of publications. In materials engineering, such data are spread across technical handbooks specification sheets, journal articles, and laboratory notebooks in myriad formats. Extracting information from papers on a large scale has been a tedious and time-consuming job to which few researchers have wanted to devote their limited time and effort, yet is an activity that is essential for modern data-driven design practices. However, in recent years, significant progress has been made by the computer science community on techniques for automated information extraction from free text. Yet, transformative application of these techniques to scientific literature remains elusive---due not to a lack of interest or effort but to technical and logistical challenges. Using the challenges in the materials science literature as a driving motivation, we review the gaps between state-of-the-art information extraction methods and the practical application of such methods to scientific texts, and offer a comprehensive overview of work that can be undertaken to close these gaps.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NLUQEC5N/Hong et al. - 2021 - Challenges and Advances in Information Extraction from Scientific Literature a Review.pdf}
}

@article{huangFailingsShapleyValues2024,
  title = {On the Failings of {{Shapley}} Values for Explainability},
  author = {Huang, Xuanxiang and {Marques-Silva}, Joao},
  year = {2024},
  month = aug,
  journal = {International Journal of Approximate Reasoning},
  series = {Synergies between {{Machine Learning}} and {{Reasoning}}},
  volume = {171},
  pages = {109112},
  issn = {0888-613X},
  doi = {10.1016/j.ijar.2023.109112},
  urldate = {2025-08-29},
  abstract = {Explainable Artificial Intelligence (XAI) is widely considered to be critical for building trust into the deployment of systems that integrate the use of machine learning (ML) models. For more than two decades Shapley values have been used as the theoretical underpinning for some methods of XAI, being commonly referred to as SHAP scores. Some of these methods of XAI now rank among the most widely used, including in high-risk domains. This paper proves that the existing definitions of SHAP scores will necessarily yield misleading information about the relative importance of features for predictions. The paper identifies a number of ways in which misleading information can be conveyed to human decision makers, and proves that there exist classifiers which will yield such misleading information. Furthermore, the paper offers empirical evidence that such theoretical limitations of SHAP scores are routinely observed in ML classifiers.},
  keywords = {Abductive explanations,Explainable AI (XAI),SHAP scores,Shapley values},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HFK43H9Z/Huang and Marques-Silva - 2024 - On the failings of Shapley values for explainability.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/5N3KKPSS/S0888613X23002438.html}
}

@misc{huComputingSHAPEfficiently2023,
  title = {Computing {{SHAP Efficiently Using Model Structure Information}}},
  author = {Hu, Linwei and Wang, Ke},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02417},
  eprint = {2309.02417},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.02417},
  urldate = {2025-08-28},
  abstract = {SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor \& Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/I88UL4MG/Hu and Wang - 2023 - Computing SHAP Efficiently Using Model Structure Information.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/RFT2NADP/2309.html}
}

@misc{ibmglobaltechnologyservicesToxicTerabyte2006,
  title = {The Toxic Terabyte},
  author = {{IBM Global Technology Services}},
  year = {2006},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CR4HILVE/The Toxic Terabyte.pdf}
}

@article{jhguchBoxPlot2025,
  title = {Box Plot},
  author = {{Jhguch}},
  year = {2025},
  month = jul,
  journal = {Wikipedia},
  urldate = {2025-08-29},
  abstract = {In descriptive statistics, a box plot or boxplot is a method for demonstrating graphically the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also called the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings in each subsection of the box-plot indicate the degree of dispersion (spread) and skewness of the data, which are usually described using the five-number summary. In addition, the box-plot allows one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1302092519},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Z2AQUUNU/index.html}
}

@article{jrHowBigData2015,
  title = {How {{Big Data Will Change Accounting}}},
  author = {Jr, J. and Moffitt, Kevin and Byrnes, Paul},
  year = {2015},
  month = feb,
  journal = {Accounting Horizons},
  volume = {29},
  pages = {150227130540002},
  doi = {10.2308/acch-51069},
  abstract = {SYNOPSIS Big Data will have increasingly important implications for accounting, even as new types of data become accessible. The video, audio, and textual information made available via Big Data can provide for improved managerial accounting, financial accounting, and financial reporting practices. In managerial accounting, Big Data will contribute to the development and evolution of effective management control systems and budgeting processes. In financial accounting, Big Data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision making. In reporting, Big Data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.}
}

@article{krzywinskiVisualizingSamplesBox2014,
  title = {Visualizing Samples with Box Plots},
  author = {Krzywinski, Martin and Altman, Naomi},
  year = {2014},
  month = feb,
  journal = {Nature Methods},
  volume = {11},
  number = {2},
  pages = {119--120},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2813},
  langid = {english},
  pmid = {24645192},
  keywords = {Computer Graphics,Data Interpretation Statistical,Humans,Pattern Recognition Visual}
}

@inproceedings{kulkarniPruningRandomForest2012,
  title = {Pruning of {{Random Forest}} Classifiers: {{A}} Survey and Future Directions},
  shorttitle = {Pruning of {{Random Forest}} Classifiers},
  booktitle = {2012 {{International Conference}} on {{Data Science}} \& {{Engineering}} ({{ICDSE}})},
  author = {Kulkarni, Vrushali Y and Sinha, Pradeep K},
  year = {2012},
  month = jul,
  pages = {64--68},
  doi = {10.1109/ICDSE.2012.6282329},
  urldate = {2025-08-29},
  abstract = {Random Forest is an ensemble supervised machine learning technique. Based on bagging and random feature selection, number of decision trees (base classifiers) is generated and majority voting is taken for classification. For effective learning and classification of Random Forest, there is need for reducing number of trees (Pruning) in Random Forest. We have presented here systematic survey of pruning efforts of Random Forest classifier along with the required theoretical background. Most of the work for pruning takes static approach while recently dynamic pruning is being targeted. We have also generated a Comparison Chart by taking relevant parameters. There is research scope for analyzing behavior of Random forest, generating accurate and diverse base decision trees, truly dynamic pruning algorithm for Random Forest classifier, and generating optimal subset of Random forest.},
  keywords = {Accuracy,Classification,Correlation,Data mining,Data Mining,Decision trees,Diversity reception,Ensemble,Heuristic algorithms,Machine Learning,Pruning,Random Forest,Vegetation},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/R8MV9J46/6282329.html}
}

@article{kulkarniRandomForestClassifiers2013,
  title = {Random Forest Classifiers: {{A}} Survey and Future Research Directions},
  shorttitle = {Random Forest Classifiers},
  author = {Kulkarni, Vrushali and Sinha, Pradeep},
  year = {2013},
  month = jan,
  journal = {International Journal of Advanced Computing},
  volume = {36},
  pages = {1144--1153}
}

@misc{liAddressingLastMile2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}: {{Using AIS Technologies}} to {{Enhance Governmental Financial Reporting}}},
  shorttitle = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}},
  author = {Li, Huaxia and Wei, Danyang (Kathy) and Moffitt, Kevin and Vasarhelyi, Miklos A.},
  year = {2023},
  month = mar,
  number = {4385883},
  eprint = {4385883},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4385883},
  urldate = {2025-08-26},
  abstract = {{$<$}p{$>$}Although the Open Government Data (OGD) initiative has gained global momentum over the past two decades, the lack of a machine-readable format for much finan},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting Information Systems (AIS) Open Government Data (OGD),Design Science,Government Accounting,Last Mile Problem,Open Government Data (OGD),PDF Extraction,Robotic Process Automation,Text Mining}
}

@article{liBuildingTrustMachine2024,
  title = {Toward {{Building Trust}} in {{Machine Learning Models}}: {{Quantifying}} the {{Explainability}} by {{SHAP}} and {{References}} to {{Human Strategy}}},
  shorttitle = {Toward {{Building Trust}} in {{Machine Learning Models}}},
  author = {Li, Zhaopeng and Bouazizi, Mondher and Ohtsuki, Tomoaki and Ishii, Masakuni and Nakahara, Eri},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {11010--11023},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3347796},
  urldate = {2025-08-29},
  abstract = {Local model-agnostic Explainable Artificial Intelligence (XAI), such as LIME or SHAP, has recently gained popularity among researchers and data scientists for explaining black box Machine Learning (ML) models. In the industry, practitioners focus not only on how these explanations can validate their models but also on how they can help maintain trust from end-users. Some studies attempted to measure this ability by quantifying what they refer to as the explainability or interpretability of ML models. In this paper, we introduce a new method for measuring explainability with reference to an approximated human model. We develop a human-friendly interface to strategically collect human decision-making and translate it into a set of logical rules and intuitions, or simply annotations. These annotations are then compared with the local explanations derived from common XAI tools. Through a human survey, we demonstrate that it is possible to quantify human intuition and empirically compare it to a given explanation, enabling a practical quantification of explainability. By relying on this new method, we identified several potential flaws in today's ML selection process. Furthermore, we demonstrate how our method can help to better evaluate ML models.},
  keywords = {Annotations,artificial intelligence,Artificial intelligence,Buildings,explainability,Explainable artificial intelligence,machine learning,Machine learning,Mathematical models,Predictive models,Remuneration,Solid modeling,Task analysis},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TVBGGTII/Li et al. - 2024 - Toward Building Trust in Machine Learning Models Quantifying the Explainability by SHAP and Referen.pdf}
}

@misc{liExtractingFinancialData2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Extracting {{Financial Data}} from {{Unstructured Sources}}: {{Leveraging Large Language Models}}},
  shorttitle = {Extracting {{Financial Data}} from {{Unstructured Sources}}},
  author = {Li, Huaxia and Gao, Haoyun (Harry) and Wu, Chengzhang and Vasarhelyi, Miklos A.},
  year = {2023},
  month = sep,
  number = {4567607},
  eprint = {4567607},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4567607},
  urldate = {2025-01-10},
  abstract = {This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from PDF-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate ESG reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5\% accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96\%. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting information systems Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models,ChatGPT,Data extraction,Design Science,Information processing,Large Language Model (LLM),PDF Reports,Unstructured data},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UEUDUT7Y/Li et al. - 2023 - Extracting Financial Data from Unstructured Sources Leveraging Large Language Models.pdf}
}

@misc{liProgrammingExampleSolved2024,
  title = {Is {{Programming}} by {{Example}} Solved by {{LLMs}}?},
  author = {Li, Wen-Ding and Ellis, Kevin},
  year = {2024},
  month = nov,
  number = {arXiv:2406.08316},
  eprint = {2406.08316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08316},
  urldate = {2025-08-26},
  abstract = {Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have "solved" PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ED7B99GA/Li and Ellis - 2024 - Is Programming by Example solved by LLMs.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/AVJXACWT/2406.html}
}

@misc{luLargeLanguageModel2024,
  title = {Large {{Language Model}} for {{Table Processing}}: {{A Survey}}},
  shorttitle = {Large {{Language Model}} for {{Table Processing}}},
  author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
  year = {2024},
  month = oct,
  eprint = {2402.05121},
  primaryclass = {cs},
  doi = {10.1007/s11704-024-40763-6},
  urldate = {2025-01-10},
  abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/85WXEHCW/Lu et al. - 2024 - Large Language Model for Table Processing A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/UXHNNW2A/2402.html}
}

@misc{lundbergExplainableAITrees2019,
  title = {Explainable {{AI}} for {{Trees}}: {{From Local Explanations}} to {{Global Understanding}}},
  shorttitle = {Explainable {{AI}} for {{Trees}}},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1905.04610},
  urldate = {2025-08-28},
  abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = nov,
  number = {arXiv:1705.07874},
  eprint = {1705.07874},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07874},
  urldate = {2025-08-28},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IKVK7ME5/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictions.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/9JCTXFDT/1705.html}
}

@misc{manningIntroductionInformationRetrieval2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  month = jul,
  journal = {Cambridge Aspire website},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511809071},
  urldate = {2025-08-28},
  abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
  howpublished = {https://www.cambridge.org/highereducation/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C},
  isbn = {9780511809071},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/THGVGUJU/Manning et al. - 2008 - Introduction to Information Retrieval.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/IZYLA5JU/669D108D20F556C5C30957D63B5AB65C.html}
}

@article{mienyeSurveyDecisionTrees2024,
  title = {A {{Survey}} of {{Decision Trees}}: {{Concepts}}, {{Algorithms}}, and {{Applications}}},
  shorttitle = {A {{Survey}} of {{Decision Trees}}},
  author = {Mienye, Ibomoiye Domor and Jere, Nobert},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {86716--86727},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3416838},
  urldate = {2025-08-29},
  abstract = {Machine learning (ML) has been instrumental in solving complex problems and significantly advancing different areas of our lives. Decision tree-based methods have gained significant popularity among the diverse range of ML algorithms due to their simplicity and interpretability. This paper presents a comprehensive overview of decision trees, including the core concepts, algorithms, applications, their early development to the recent high-performing ensemble algorithms and their mathematical and algorithmic representations, which are lacking in the literature and will be beneficial to ML researchers and industry experts. Some of the algorithms include classification and regression tree (CART), Iterative Dichotomiser 3 (ID3), C4.5, C5.0, Chi-squared Automatic Interaction Detection (CHAID), conditional inference trees, and other tree-based ensemble algorithms, such as random forest, gradient-boosted decision trees, and rotation forest. Their utilisation in recent literature is also discussed, focusing on applications in medical diagnosis and fraud detection.},
  keywords = {Algorithm design and analysis,Algorithms,C4.5,C5.0,CART,Classification algorithms,decision tree,Decision trees,ensemble learning,Ensemble learning,ID3,Indexes,machine learning,Machine learning,Machine learning algorithms,Peer-to-peer computing,Random forests},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IFNN3GXN/Mienye and Jere - 2024 - A Survey of Decision Trees Concepts, Algorithms, and Applications.pdf}
}

@book{molnarInterpretableMachineLearning2025,
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  shorttitle = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  year = {2025},
  edition = {Third edition},
  publisher = {Christoph Molnar},
  address = {Munich, Germany},
  isbn = {978-3-911578-03-5},
  langid = {english},
  annotation = {OCLC: 1518801363}
}

@article{mosqueira-reyHumanintheloopMachineLearning2023,
  title = {Human-in-the-Loop Machine Learning: A State of the Art},
  shorttitle = {Human-in-the-Loop Machine Learning},
  author = {{Mosqueira-Rey}, Eduardo and {Hern{\'a}ndez-Pereira}, Elena and {Alonso-R{\'i}os}, David and {Bobes-Bascar{\'a}n}, Jos{\'e} and {Fern{\'a}ndez-Leal}, {\'A}ngel},
  year = {2023},
  month = apr,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {4},
  pages = {3005--3054},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10246-w},
  urldate = {2025-08-24},
  abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.},
  langid = {english},
  keywords = {Active learning,Curriculum learning,Explainable AI,Human-in-the-loop machine learning,Interactive machine learning,Machine teaching},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HN5ZN5HT/Mosqueira-Rey et al. - 2023 - Human-in-the-loop machine learning a state of the art.pdf}
}

@article{nanniniOperationalizingExplainableArtificial2024,
  title = {Operationalizing {{Explainable Artificial Intelligence}} in the {{European Union Regulatory Ecosystem}}},
  author = {Nannini, Luca and {Alonso-Moral}, Jose Maria and Catal{\'a}, Alejandro and Lama, Manuel and Barro, Sen{\'e}n},
  year = {2024},
  month = jul,
  journal = {IEEE Intelligent Systems},
  volume = {39},
  number = {4},
  pages = {37--48},
  issn = {1941-1294},
  doi = {10.1109/MIS.2024.3383155},
  urldate = {2025-08-29},
  abstract = {The European Union's (EU's) regulatory ecosystem presents challenges with balancing legal and sociotechnical drivers for explainable artificial intelligence (XAI) systems. Core tensions emerge on dimensions of oversight, user needs, and litigation. This article maps provisions on algorithmic transparency and explainability across major EU data, AI, and platform policies using qualitative analysis. We characterize the involved stakeholders and organizational implementation targets. Constraints become visible between useful transparency for accountability and confidentiality protections. Through an AI hiring system example, we explore the complications with operationalizing explainability. Customization is required to satisfy explainability desires within confidentiality and proportionality bounds. The findings advise technologists on prudent XAI technique selection given multidimensional tensions. The outcomes recommend that policy makers balance worthy transparency goals with cohesive legislation, enabling equitable dispute resolution.},
  keywords = {Artificial intelligence,Ecosystems,Europe,Explainable AI,Intelligent systems,Law,Regulation,Sociotechnical systems,Stakeholders},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/D87DMTD4/Nannini et al. - 2024 - Operationalizing Explainable Artificial Intelligence in the European Union Regulatory Ecosystem.pdf}
}

@misc{nassarTableFormerTableStructure2022,
  title = {{{TableFormer}}: {{Table Structure Understanding}} with {{Transformers}}},
  shorttitle = {{{TableFormer}}},
  author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01017},
  eprint = {2203.01017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.01017},
  urldate = {2025-04-07},
  abstract = {Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91\% to 98.5\% on simple tables and from 88.7\% to 95\% on complex tables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MHEENV38/Nassar et al. - 2022 - TableFormer Table Structure Understanding with Transformers.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/VXX5JEGY/2203.html}
}

@misc{natarajanHumanintheloopAIintheloopAutomate2024,
  title = {Human-in-the-Loop or {{AI-in-the-loop}}? {{Automate}} or {{Collaborate}}?},
  shorttitle = {Human-in-the-Loop or {{AI-in-the-loop}}?},
  author = {Natarajan, Sriraam and Mathur, Saurabh and Sidheekh, Sahil and Stammer, Wolfgang and Kersting, Kristian},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14232},
  eprint = {2412.14232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14232},
  urldate = {2025-08-24},
  abstract = {Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop (AI2L) systems: the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an AI2L perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an AI2L approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4YEXK3FV/Natarajan et al. - 2024 - Human-in-the-loop or AI-in-the-loop Automate or Collaborate.pdf}
}

@misc{osullivanMathematicsShapleyValues2023,
  title = {The Mathematics behind {{Shapley Values}}},
  author = {O'Sullivan, Conor},
  year = {2023},
  month = mar,
  urldate = {2025-08-28},
  abstract = {Shapley values are a fair way to divide the value of a game amongst its players. We explain the mathematics behind the Shapley value formula. To understand why it is fair, we also discuss the Shapley value axioms that the formula is derived from. The formula may seem scary but you will find it has an intuitive explanation.}
}

@misc{pythonologyExtractTextLinks2023,
  title = {Extract Text, Links, Images, Tables from {{Pdf}} with {{Python}} {\textbar} {{PyMuPDF}}, {{PyPdf}}, {{PdfPlumber}} Tutorial},
  author = {{Pythonology}},
  year = {2023},
  month = jan,
  urldate = {2025-01-10},
  abstract = {Use these Python libraries to convert a Pdf into an image, extract text, images, links, and tables from pdfs using the 3 popular Python libraries PyMuPDF, PyPdf, PdfPlumber. Here is source code and article I have written: https://pythonology.eu/what-is-the-be...  -- Support Pythonology -- https://www.buymeacoffee.com/pythonology -- Best Online Resource for Python -- Datacamp: The best online resource to learn Python, Web Scraping, Data analysis, and Data Science (Affiliate link) https://datacamp.pxf.io/pythonology}
}

@misc{qwenteamQwen3ThinkDeeper2025,
  title = {Qwen3: {{Think Deeper}}, {{Act Faster}}},
  shorttitle = {Qwen3},
  author = {{Qwen Team}},
  year = {2025},
  month = apr,
  journal = {Qwen},
  urldate = {2025-08-28},
  abstract = {QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.},
  chapter = {blog},
  howpublished = {https://qwenlm.github.io/blog/qwen3/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/AWFWL8VG/qwen3.html}
}

@article{rathiImportanceTermWeighting2023,
  title = {The Importance of {{Term Weighting}} in Semantic Understanding of Text: {{A}} Review of Techniques},
  shorttitle = {The Importance of {{Term Weighting}} in Semantic Understanding of Text},
  author = {Rathi, R. N. and Mustafi, A.},
  year = {2023},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {82},
  number = {7},
  pages = {9761--9783},
  issn = {1573-7721},
  doi = {10.1007/s11042-022-12538-3},
  urldate = {2025-08-28},
  abstract = {In this paper we review a wide spectrum of techniques which have been proposed in literature to enable acceptable recognition of language and text by machines. We discuss many techniques which have been proposed by researchers in the field of term weighting and explore the mathematical foundations of these methods. Term weighting schemes have broadly been classified as supervised and statistical methods and we present numerous examples from both categories to highlight the difference in approaches between the two broad categories. We pay particular attention to the Vector Space Model and its variants which form the basis of many of the other methods which have been discussed in the paper.},
  langid = {english},
  keywords = {Term weighting,Term weighting techniques,Word embedding},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/JDRLDIWI/Rathi and Mustafi - 2023 - The importance of Term Weighting in semantic understanding of text A review of techniques.pdf}
}

@article{raymaekersFastLinearModel2024,
  title = {Fast {{Linear Model Trees}} by {{PILOT}}},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J. and Verdonck, Tim and Yao, Ruicong},
  year = {2024},
  month = sep,
  journal = {Machine Learning},
  volume = {113},
  number = {9},
  eprint = {2302.03931},
  primaryclass = {stat},
  pages = {6561--6610},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-024-06590-3},
  urldate = {2025-08-29},
  abstract = {Linear model trees are regression trees that incorporate linear models in the leaf nodes. This preserves the intuitive interpretation of decision trees and at the same time enables them to better capture linear relationships, which is hard for standard decision trees. But most existing methods for fitting linear model trees are time consuming and therefore not scalable to large data sets. In addition, they are more prone to overfitting and extrapolation issues than standard regression trees. In this paper we introduce PILOT, a new algorithm for linear model trees that is fast, regularized, stable and interpretable. PILOT trains in a greedy fashion like classic regression trees, but incorporates an \$L{\textasciicircum}2\$ boosting approach and a model selection rule for fitting linear models in the nodes. The abbreviation PILOT stands for \$PI\$ecewise \$L\$inear \$O\$rganic \$T\$ree, where `organic' refers to the fact that no pruning is carried out. PILOT has the same low time and space complexity as CART without its pruning. An empirical study indicates that PILOT tends to outperform standard decision trees and other linear model trees on a variety of data sets. Moreover, we prove its consistency in an additive model setting under weak assumptions. When the data is generated by a linear model, the convergence rate is polynomial.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/J2E4DARI/Raymaekers et al. - 2024 - Fast Linear Model Trees by PILOT.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/7QJ5Q43J/2302.html}
}

@article{rivera-lopezInductionDecisionTrees2022,
  title = {Induction of Decision Trees as Classification Models through Metaheuristics},
  author = {{Rivera-Lopez}, Rafael and {Canul-Reich}, Juana and {Mezura-Montes}, Efr{\'e}n and {Cruz-Ch{\'a}vez}, Marco Antonio},
  year = {2022},
  month = mar,
  journal = {Swarm and Evolutionary Computation},
  volume = {69},
  pages = {101006},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2021.101006},
  urldate = {2025-08-29},
  abstract = {The induction of decision trees is a widely-used approach to build classification models that guarantee high performance and expressiveness. Since a recursive-partitioning strategy guided for some splitting criterion is commonly used to induce these classifiers, overfitting, attribute selection bias, and instability to small training set changes are well-known problems in them. Other approaches, such as incremental induction, classifier ensembles, and the global search in the decision-tree-space, have been implemented to overcome these problems. In particular, metaheuristics such as simulated annealing, genetic algorithms, genetic programming, and ant colony optimization have been used to induce compact and accurate decision trees. This paper presents a state-of-the-art review of the use of single-solution-based metaheuristics and swarm and evolutionary computation algorithms to build decision trees as classification models. We outline the decision-tree-induction process components and detail the existing literature studies on metaheuristic-based approaches to building these classifiers. Several timelines showing the chronological order in which these approaches were introduced in the literature are included. A summary analysis of these studies is also conducted, focusing on their internal components and experimental studies. This work provides a useful reference point for future research in this field.},
  keywords = {Evolutionary algorithms,Machine learning,Single-solution-based metaheuristics,Swarm intelligence methods},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/45J79K5E/Rivera-Lopez et al. - 2022 - Induction of decision trees as classification models through metaheuristics.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/GK6H625Q/S2210650221001681.html}
}

@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The {{Probabilistic Relevance Framework}}: {{BM25}} and {{Beyond}}},
  shorttitle = {The {{Probabilistic Relevance Framework}}},
  author = {Robertson, Stephen and Zaragoza, Hugo},
  year = {2009},
  month = jan,
  journal = {Foundations and Trends in Information Retrieval},
  volume = {3},
  pages = {333--389},
  doi = {10.1561/1500000019},
  abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970---1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UCFZGJJN/Robertson and Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Beyond.pdf}
}

@article{robertsonUnderstandingInverseDocument2004,
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for {{IDF}}},
  shorttitle = {Understanding Inverse Document Frequency},
  author = {Robertson, Stephen},
  year = {2004},
  month = oct,
  journal = {Journal of Documentation},
  volume = {60},
  number = {5},
  pages = {503--520},
  issn = {0022-0418},
  doi = {10.1108/00220410410560582},
  urldate = {2025-08-28},
  abstract = {The term-weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon's Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in the traditional probabilistic model of information retrieval.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/YJACLU4B/Robertson - 2004 - Understanding inverse document frequency on theoretical arguments for IDF.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/EM8M2D47/00220410410560582.html}
}

@article{saitoPrecisionRecallPlotMore2015,
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  year = {2015},
  month = mar,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0118432},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118432},
  urldate = {2025-08-24},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  langid = {english},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome-wide association studies,Interpolation,Measurement,MicroRNAs,Support vector machines},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CTBBR652/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers o.pdf}
}

@misc{senatsverwaltungfuerfinanzenberlinBeteiligungsbericht2024,
  title = {{Beteiligungsbericht}},
  author = {{Senatsverwaltung f{\"u}r Finanzen Berlin}},
  year = {2024},
  month = nov,
  urldate = {2025-01-10},
  abstract = {Beteiligungsberichte},
  langid = {ngerman},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/E2VJKHW4/beteiligungsbericht_2024_gesamt.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YIZUVU22/artikel.941274.html}
}

@incollection{shapley17ValueNPerson2016,
  title = {17. {{A Value}} for n-{{Person Games}}},
  booktitle = {Contributions to the {{Theory}} of {{Games}}, {{Volume II}}},
  author = {Shapley, L. S.},
  editor = {Kuhn, Harold William and Tucker, Albert William},
  year = {2016},
  month = mar,
  pages = {307--318},
  publisher = {Princeton University Press},
  urldate = {2025-08-28},
  isbn = {978-1-4008-8197-0},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/STWE3ECY/P295.pdf}
}

@misc{shengPdfTableUnifiedToolkit2024,
  title = {{{PdfTable}}: {{A Unified Toolkit}} for {{Deep Learning-Based Table Extraction}}},
  shorttitle = {{{PdfTable}}},
  author = {Sheng, Lei and Xu, Shuai-Shuai},
  year = {2024},
  month = sep,
  number = {arXiv:2409.05125},
  eprint = {2409.05125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05125},
  urldate = {2025-01-10},
  abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Y833Q63R/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/74UPJXVR/2409.html}
}

@misc{teamChameleonMixedModalEarlyFusion2024,
  title = {Chameleon: {{Mixed-Modal Early-Fusion Foundation Models}}},
  shorttitle = {Chameleon},
  author = {Team, Chameleon},
  year = {2024},
  month = may,
  number = {arXiv:2405.09818},
  eprint = {2405.09818},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.09818},
  urldate = {2025-08-28},
  abstract = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S3JJJ44G/Team - 2024 - Chameleon Mixed-Modal Early-Fusion Foundation Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/SXYSAWCZ/2405.html}
}

@article{wickham40YearsBoxplots2011,
  title = {40 Years of Boxplots},
  author = {Wickham, H and Stryjewski, L},
  year = {2011},
  month = jan,
  abstract = {The boxplot plot has been around for over 40 years. This paper summarises the improvements, exten-sions and variations since Tukey first introduced his "schematic plot" in 1970. We focus particularly on richer displays of density and extensions to 2d.}
}

@article{wohlinDecisionmakingStructureSelecting2015,
  title = {Towards a Decision-Making Structure for Selecting a Research Design in Empirical Software Engineering},
  author = {Wohlin, Claes and Aurum, Ayb{\"u}ke},
  year = {2015},
  month = dec,
  journal = {Empirical Software Engineering},
  volume = {20},
  number = {6},
  pages = {1427--1455},
  issn = {1573-7616},
  doi = {10.1007/s10664-014-9319-7},
  urldate = {2025-08-24},
  abstract = {Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders' activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering research. The article provides an introduction to each decision point and its constituents, as well as to the relationships between the different parts in the decision-making structure. The intention is the structure should act as a starting point for the research design before going into the details of the research design chosen. The article provides an in-depth discussion of decision points in relation to the research design when conducting empirical research.},
  langid = {english},
  keywords = {Empirical software engineering research,Research design,Research methods,Selecting research method}
}

@book{wohlinExperimentationSoftwareEngineering2024,
  title = {Experimentation in {{Software Engineering}}},
  author = {Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
  year = {2024},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-69306-3},
  urldate = {2025-08-24},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-662-69305-6 978-3-662-69306-3},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/EPFIGBV5/Wohlin et al. - 2024 - Experimentation in Software Engineering.pdf}
}

@article{wuSurveyHumanintheloopMachine2022,
  title = {A {{Survey}} of {{Human-in-the-loop}} for {{Machine Learning}}},
  author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  year = {2022},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {135},
  eprint = {2108.00941},
  primaryclass = {cs},
  pages = {364--381},
  issn = {0167739X},
  doi = {10.1016/j.future.2022.05.014},
  urldate = {2025-08-24},
  abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-theloop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent humanin-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/ weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/P5Y3UKMG/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf}
}

@inproceedings{xuLayoutLMPretrainingText2020,
  title = {{{LayoutLM}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Document Image Understanding}}},
  shorttitle = {{{LayoutLM}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year = {2020},
  month = aug,
  eprint = {1912.13318},
  primaryclass = {cs},
  pages = {1192--1200},
  doi = {10.1145/3394486.3403172},
  urldate = {2025-04-27},
  abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IADVHXM6/Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Document Image Understanding.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LZU6Z3JF/1912.html}
}

@misc{yangFastTreeSHAPAccelerating2022,
  title = {Fast {{TreeSHAP}}: {{Accelerating SHAP Value Computation}} for {{Trees}}},
  shorttitle = {Fast {{TreeSHAP}}},
  author = {Yang, Jilei},
  year = {2022},
  month = jul,
  number = {arXiv:2109.09847},
  eprint = {2109.09847},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.09847},
  urldate = {2025-08-28},
  abstract = {SHAP (SHapley Additive exPlanation) values are one of the leading tools for interpreting machine learning models, with strong theoretical guarantees (consistency, local accuracy) and a wide availability of implementations and use cases. Even though computing SHAP values takes exponential time in general, TreeSHAP takes polynomial time on tree-based models. While the speedup is significant, TreeSHAP can still dominate the computation time of industry-level machine learning solutions on datasets with millions or more entries, causing delays in post-hoc model diagnosis and interpretation service. In this paper we present two new algorithms, Fast TreeSHAP v1 and v2, designed to improve the computational efficiency of TreeSHAP for large datasets. We empirically find that Fast TreeSHAP v1 is 1.5x faster than TreeSHAP while keeping the memory cost unchanged. Similarly, Fast TreeSHAP v2 is 2.5x faster than TreeSHAP, at the cost of a slightly higher memory usage, thanks to the pre-computation of expensive TreeSHAP steps. We also show that Fast TreeSHAP v2 is well-suited for multi-time model interpretations, resulting in as high as 3x faster explanation of newly incoming samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/KZCLC8ZF/Yang - 2022 - Fast TreeSHAP Accelerating SHAP Value Computation for Trees.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/3G2F5YW2/2109.html}
}

@misc{zhangDocumentParsingUnveiled2024,
  title = {Document {{Parsing Unveiled}}: {{Techniques}}, {{Challenges}}, and {{Prospects}} for {{Structured Information Extraction}}},
  shorttitle = {Document {{Parsing Unveiled}}},
  author = {Zhang, Qintong and Huang, Victor Shea-Jay and Wang, Bin and Zhang, Junyuan and Wang, Zhengren and Liang, Hao and Wang, Shawn and Lin, Matthieu and Zhang, Wentao and He, Conghui},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21169},
  eprint = {2410.21169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21169},
  urldate = {2025-08-26},
  abstract = {Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S5HD9YFM/Zhang et al. - 2024 - Document Parsing Unveiled Techniques, Challenges, and Prospects for Structured Information Extracti.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/NQMJG9YM/2410.html}
}

@misc{zhangMixtureExpertsLarge2025,
  title = {Mixture of {{Experts}} in {{Large Language Models}}},
  author = {Zhang, Danyang and Song, Junhao and Bi, Ziqian and Yuan, Yingfang and Wang, Tianyang and Yeong, Joe and Hao, Junfeng},
  year = {2025},
  month = jul,
  number = {arXiv:2507.11181},
  eprint = {2507.11181},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.11181},
  urldate = {2025-08-28},
  abstract = {This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DM22BENS/Zhang et al. - 2025 - Mixture of Experts in Large Language Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LS7IP732/2507.html}
}

@misc{zhongPubLayNetLargestDataset2019,
  title = {{{PubLayNet}}: Largest Dataset Ever for Document Layout Analysis},
  shorttitle = {{{PubLayNet}}},
  author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  year = {2019},
  month = aug,
  number = {arXiv:1908.07836},
  eprint = {1908.07836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.07836},
  urldate = {2025-04-21},
  abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4REPKJNM/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layout analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/TKQLXWYB/1908.html}
}
