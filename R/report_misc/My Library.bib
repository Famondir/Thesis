@misc{12EGovGEinzelnorm,
  title = {{\S} 12 {{EGovG}} - {{Einzelnorm}}},
  urldate = {2025-04-07},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5U8LSTMF/__12.html}
}

@phdthesis{ambacherDesigningUserfriendlyOptimized2024,
  title = {Designing a User-Friendly and Optimized Version of a User Interface for a Large Language Model ({{LLM}})},
  author = {Ambacher, Julian Emanuel},
  year = {2024},
  month = sep,
  urldate = {2025-08-08},
  abstract = {This thesis focuses on the creation of a user-friendly and efficient user interface (UI) for a large language model (LLM). As LLMs become more widely used in different fields, it is important to have a UI that meets user needs when interacting with them [1], [2], [3]. The main goal of this study is to find key design principles, design guidelines, and methods that ensure that the UI is easy to use, efficient, and satisfying. To achieve this, the thesis analyzes the interaction principles according to ISO 9241-110 on the existing LLM tool Chat GPT and focuses also on other different UI designs [4]. For this purpose, a prototype is created based on the analysis. This thesis executes a usability test to evaluate the usability of the initial prototype. It focuses on factors including flexibility, accuracy of system answers, ease of navigation, and feature efficacy. The feedback from this test is used to enhance the design. The second iteration of the prototype is evaluated using several measures, including the System Usability Scale (SUS) and the User Experience Questionnaire (UEQ). In addition, feedback was gathered from four experts in the field of UX and AI. The results showed improvements in ease of use, task efficiency, and overall user satisfaction. The results highlight the importance of clear guidance, privacy, and system responsiveness. In summary, the thesis provides a set of design guidelines and recommendations for creating a user-friendly user interface for LLMs. These include the aspects of user-centered design, contextual help features, and optimized workflows. In addition, it provides guidance for the future development of AI tools that prioritize both functionality and a positive user experience.},
  copyright = {https://rightsstatements.org/page/InC/1.0/?language=de},
  langid = {english},
  school = {Technische Hochschule Ingolstadt},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/F9IKW8QS/Ambacher - 2024 - Designing a user-friendly and optimized version of a user interface for a large language model (LLM).pdf}
}

@misc{americasmarketintelligenceGlobalEcommerceSpend2023,
  title = {Global: E-Commerce Spend per Capita by Region 2023},
  shorttitle = {Global},
  author = {{Americas Market Intelligence}},
  year = {2023},
  journal = {Statista},
  urldate = {2025-01-19},
  abstract = {In 2023, the United States witnessed an estimated annual per capita spending on e-commerce of 3,370 U.S.},
  howpublished = {https://www.statista.com/statistics/518729/annual-b2c-e-commerce-spending-by-region/?\_\_sso\_cookie\_checker=failed},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/QK5BHZRQ/annual-b2c-e-commerce-spending-by-region.html}
}

@article{AnnualComprehensiveFinancial,
  title = {Annual {{Comprehensive Financial Report}} for the {{Fiscal Year Ended June}} 30, 2023},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/6XN9JPBJ/Annual Comprehensive Financial Report for the Fiscal Year Ended June 30, 2023.pdf}
}

@misc{auerDoclingTechnicalReport2024,
  title = {Docling {{Technical Report}}},
  author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and de Lima, Rafael Teixeira and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
  year = {2024},
  month = dec,
  number = {arXiv:2408.09869},
  eprint = {2408.09869},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.09869},
  urldate = {2025-04-07},
  abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CB5G2B4S/Auer et al. - 2024 - Docling Technical Report.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/I7GPQFC3/2408.html}
}

@misc{bbbinfrastruktur-verwaltungsgmbhGeschaftsbericht20232024,
  title = {Gesch{\"a}ftsbericht 2023},
  author = {{BBB Infrastruktur-Verwaltungs GmbH} and {BBB Infrastruktur GmbH \& Co. KG}},
  year = {2024},
  month = oct,
  urldate = {2025-01-10},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NN95EQA8/GB_BBB_Infra_2023.pdf}
}

@article{bentleyKnowingYouKnow2025,
  title = {Knowing You Know Nothing in the Age of Generative {{AI}}},
  author = {Bentley, Sarah V.},
  year = {2025},
  month = mar,
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  number = {1},
  pages = {409},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-025-04731-0},
  urldate = {2025-08-26},
  abstract = {Generative AI is a revolutionary new technology whose impact promises to democratise knowledge. And yet, unlike the printing press, which expanded knowledge through the amplification of one voice to many, generative AI reduces many voices to one. Its disruptive nature provides us with a timely reminder of both the power and fallibility of knowledge: its authorship, ownership, and veracity. This Comment situates generative AI within the evolutionary context of human information dissemination and knowledge production. Whilst acknowledging the extraordinary potential of this new tool, it asks the question---given that knowledge is probably our most valuable asset, should we not be applying more of it to better understand the impact of AI-mediated knowledge tools on both our information practices and their associated knowledge outcomes?},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Education,Psychology,Science,technology and society},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IJX3A6BT/Bentley - 2025 - Knowing you know nothing in the age of generative AI.pdf}
}

@misc{BillboardDesignPricing,
  title = {Billboard {{Design Pricing}} (\$249 {{Onwards}})},
  journal = {DesignBro},
  urldate = {2025-01-21},
  abstract = {World-class creative designers. \ding{77} Hand-selected \& trusted graphic designers. \ding{77} 98\% satisfaction rate. \ding{77} Agency quality creative designing for a freelance price.\ding{77}},
  howpublished = {https://designbro.com/pricing/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/QKSZ4T9H/billboard.html}
}

@misc{bmireferato2MinikommentarGesetzZur2013,
  title = {Minikommentar Zum {{Gesetz}} Zur {{F{\"o}rderung}} Der Elektroni- Schen {{Verwaltung}} Sowie Zur {{{\"A}nderung}} Weiterer {{Vor-}} Schriften},
  editor = {{BMI, Referat O2}},
  year = {2013},
  month = jul,
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MXWPI7DC/e-government-gesetz-minikommentar.pdf}
}

@article{boseakEvaluatingLogLikelihoodConfidence2025,
  title = {Evaluating {{Log-Likelihood}} for {{Confidence Estimation}} in {{LLM-Based Multiple-Choice Question Answering}}},
  author = {Boseak, Christopher},
  year = {2025},
  journal = {Innovative Journal of Applied Science},
  volume = {02},
  number = {04},
  doi = {10.70844/ijas.2025.2.29},
  urldate = {2025-08-27},
  abstract = {Reliable deployment of Large Language Models (LLMs) in question-answering tasks requires well-calibrated confidence estimates. This work investigates whether token-level log-likelihoods---sums of log-probabilities over answer tokens---can serve as effective confidence signals in Multiple-Choice Question Answering (MCQA). We compare three methods: (1) Raw log-likelihood, (2) length-normalized log- likelihood and (3) conventional softmax-based choice probability. Across four diverse MCQA benchmarks, we find that no single scoring method is universally best. Length normalization can significantly improve calibration but may reduce accuracy, while softmax and raw log-likelihood yield identical predictions. These results highlight important trade-offs between calibration and accuracy and offer insights into selecting or adapting confidence measures for different tasks. Our findings inform the design of more trustworthy LLM-based QA systems and lay groundwork for broader uncertainty quantification efforts.}
}

@misc{bundesanzeigerJahresabschlussGeschaftsjahrVom,
  title = {Jahresabschluss Zum {{Gesch{\"a}ftsjahr}} Vom 01.01.2008 Bis Zum 31.12.2008},
  author = {{Bundesanzeiger}},
  urldate = {2025-01-10},
  howpublished = {https://www.bundesanzeiger.de/pub/de/suchergebnis?7},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G3DTIWF3/suchergebnis.html}
}

@article{caiSurveyMixtureExperts2025a,
  title = {A {{Survey}} on {{Mixture}} of {{Experts}} in {{Large Language Models}}},
  author = {Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  year = {2025},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {2407.06204},
  primaryclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2025.3554028},
  urldate = {2025-08-28},
  abstract = {Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/TN8XG237/Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf}
}

@misc{chadj.treadwayDigitalVsTraditional2022,
  title = {Digital vs. {{Traditional Advertising}}: {{A Fair Comparison}}},
  author = {{Chad J. Treadway}},
  year = {2022},
  urldate = {2025-01-20},
  howpublished = {https://cubecreative.design/blog/small-business-marketing/traditional-vs-digital-advertising-comparison\#billboard-and-out-of-home-media-advertising},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/6BDQPSCI/traditional-vs-digital-advertising-comparison.html}
}

@article{chamberlainKnowledgeNotEverything2020,
  title = {Knowledge Is Not Everything},
  author = {Chamberlain, Paul},
  year = {2020},
  month = jan,
  journal = {Design for Health},
  volume = {4},
  number = {1},
  pages = {1--3},
  publisher = {Routledge},
  issn = {2473-5132},
  doi = {10.1080/24735132.2020.1731203},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WN5KQAU3/Chamberlain - 2020 - Knowledge is not everything.pdf}
}

@inproceedings{chenTableVLMMultimodalPretraining2023,
  title = {{{TableVLM}}: {{Multi-modal Pre-training}} for {{Table Structure Recognition}}},
  shorttitle = {{{TableVLM}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Leiyuan and Huang, Chengsong and Zheng, Xiaoqing and Lin, Jinshu and Huang, Xuanjing},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {2437--2449},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.137},
  urldate = {2025-01-10},
  abstract = {Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97\% in tree-editing-distance-score on ComplexTable.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7LWJ5DED/Chen et al. - 2023 - TableVLM Multi-modal Pre-training for Table Structure Recognition.pdf}
}

@book{collisBusinessResearchPractical2014,
  title = {Business Research: A Practical Guide for Undergraduate \& Postgraduate Students},
  shorttitle = {Business Research},
  author = {Collis, Jill and Hussey, Roger},
  year = {2014},
  edition = {1. Publ.; 4. ed},
  publisher = {Palgrave Macmillan},
  address = {Basingstoke},
  isbn = {978-0-230-30183-2},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/7BY6JSH4/Collis and Hussey - 2014 - Business research a practical guide for undergraduate & postgraduate students.pdf}
}

@misc{coronaGetData,
  title = {Get the {{Data}}},
  journal = {Inside Airbnb},
  urldate = {2024-07-20},
  abstract = {Adding data to the debate},
  collaborator = {Corona, Alice and Colomb, Clarice and Phibbs, Peter and Veracruz, Sito and Wieditz, Thorben},
  howpublished = {https://insideairbnb.com/get-the-data/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/9A52ITSB/get-the-data.html}
}

@misc{daVisionGridTransformer2023,
  title = {Vision {{Grid Transformer}} for {{Document Layout Analysis}}},
  author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14978},
  eprint = {2308.14978},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14978},
  urldate = {2025-04-21},
  abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/G5Y444YW/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LV6XRTVL/2308.html}
}

@misc{DocumentParsingUnveiled,
  title = {Document {{Parsing Unveiled}}: {{Techniques}}, {{Challenges}}, and {{Prospects}} for {{Structured Information Extraction}}},
  urldate = {2025-08-26},
  howpublished = {https://arxiv.org/html/2410.21169v1},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4LN7U6ZZ/2410.html}
}

@article{el-hajRetrievingClassifyingAnalysing2020,
  title = {Retrieving, Classifying and Analysing Narrative Commentary in Unstructured (Glossy) Annual Reports Published as {{PDF}} Files},
  author = {{El-Haj}, Mahmoud and Alves, Paulo and Rayson, Paul and Walker, Martin and Young, Steven},
  year = {2020},
  month = jan,
  journal = {Accounting and Business Research},
  volume = {50},
  number = {1},
  pages = {6--34},
  publisher = {Routledge},
  issn = {0001-4788},
  doi = {10.1080/00014788.2019.1609346},
  urldate = {2025-08-26},
  abstract = {We provide a methodological contribution by developing, describing and evaluating a method for automatically retrieving and analysing text from digital PDF annual report files published by firms listed on the London Stock Exchange (LSE). The retrieval method retains information on document structure, enabling clear delineation between narrative and financial statement components of reports, and between individual sections within the narratives component. Retrieval accuracy exceeds 95\% for manual validations using a random sample of 586 reports. Large-sample statistical validations using a comprehensive sample of reports published by non-financial LSE firms confirm that report length, narrative tone and (to a lesser degree) readability vary predictably with economic and regulatory factors. We demonstrate how the method is adaptable to non-English language documents and different regulatory regimes using a case study of Portuguese reports. We use the procedure to construct new research resources including corpora for commonly occurring annual report sections and a dataset of text properties for over 26,000 U.K. annual reports.},
  keywords = {Annual reports,narrative reporting,textual analysis,unstructured documents},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NE6CEKLF/El-Haj et al. - 2020 - Retrieving, classifying and analysing narrative commentary in unstructured (glossy) annual reports p.pdf}
}

@misc{evgeniakoptyugEcommerceOnlineShoppers2023,
  title = {E-Commerce: Online Shoppers by Age Group {{Germany}}},
  shorttitle = {E-Commerce},
  author = {{Evgenia Koptyug}},
  year = {2023},
  journal = {Statista},
  urldate = {2025-01-19},
  abstract = {In 2023, over 78 percent of 25- to 45-year-olds in Germany had ordered and purchased products online in the past three months.},
  howpublished = {https://www.statista.com/statistics/506181/e-commerce-online-shoppers-by-age-group-germany/?\_\_sso\_cookie\_checker=failed},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/8B76YXUC/e-commerce-online-shoppers-by-age-group-germany.html}
}

@misc{gengGeneratingStructuredOutputs2025,
  title = {Generating {{Structured Outputs}} from {{Language Models}}: {{Benchmark}} and {{Studies}}},
  shorttitle = {Generating {{Structured Outputs}} from {{Language Models}}},
  author = {Geng, Saibo and Cooper, Hudson and Moskal, Micha{\l} and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
  year = {2025},
  month = jan,
  number = {arXiv:2501.10868},
  eprint = {2501.10868},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10868},
  urldate = {2025-07-06},
  abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/N8GNB3MS/Geng et al. - 2025 - Generating Structured Outputs from Language Models Benchmark and Studies.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/FMFPCEXK/2501.html}
}

@misc{googleGemma3nModel,
  title = {Gemma 3n Model Overview},
  author = {{Google}},
  journal = {Google AI for Developers},
  urldate = {2025-08-28},
  howpublished = {https://ai.google.dev/gemma/docs/gemma-3n},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/GYP8QF4W/gemma-3n.html}
}

@article{goughertyTestingReliabilityAIbased2024,
  title = {Testing the Reliability of an {{AI-based}} Large Language Model to Extract Ecological Information from the Scientific Literature},
  author = {Gougherty, Andrew V. and Clipp, Hannah L.},
  year = {2024},
  month = may,
  journal = {npj Biodiversity},
  volume = {3},
  number = {1},
  pages = {13},
  publisher = {Nature Publishing Group},
  issn = {2731-4243},
  doi = {10.1038/s44185-024-00043-9},
  urldate = {2025-07-09},
  abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy ({$>$}90\%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
  copyright = {2024 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
  langid = {english},
  keywords = {Data mining,Invasive species,Macroecology},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/END5Q8YY/Gougherty and Clipp - 2024 - Testing the reliability of an AI-based large language model to extract ecological information from t.pdf}
}

@misc{grandiniMetricsMultiClassClassification2020,
  title = {Metrics for {{Multi-Class Classification}}: An {{Overview}}},
  shorttitle = {Metrics for {{Multi-Class Classification}}},
  author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
  year = {2020},
  month = aug,
  number = {arXiv:2008.05756},
  eprint = {2008.05756},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.05756},
  urldate = {2025-01-10},
  abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/WX2FF8RW/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overview.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/XVQUAGR6/2008.html}
}

@misc{grootendorstVisualGuideMixture2024,
  title = {A {{Visual Guide}} to {{Mixture}} of {{Experts}} ({{MoE}})},
  author = {Grootendorst, Maarten},
  year = {2024},
  month = feb,
  urldate = {2025-08-28},
  abstract = {Demystifying the role of MoE in Large Language Models},
  howpublished = {https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/BQZXXTCQ/a-visual-guide-to-mixture-of-experts.html}
}

@book{hgbHandelsgesetzbuchImBundesgesetzblatt2025,
  title = {Handelsgesetzbuch in Der Im {{Bundesgesetzblatt Teil III}}, {{Gliederungsnummer}} 4100-1, Ver{\"o}ffentlichten Bereinigten {{Fassung}}, Das Zuletzt Durch {{Artikel}} 1 Des {{Gesetzes}} Vom 28. {{Februar}} 2025 ({{BGBl}}. 2025 {{I Nr}}. 69) Ge{\"a}ndert Worden Ist},
  author = {{HGB}},
  year = {2025},
  month = feb
}

@article{hongChallengesAdvancesInformation2021,
  title = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}: A {{Review}}},
  shorttitle = {Challenges and {{Advances}} in {{Information Extraction}} from {{Scientific Literature}}},
  author = {Hong, Zhi and Ward, Logan and Chard, Kyle and Blaiszik, Ben and Foster, Ian},
  year = {2021},
  month = oct,
  journal = {JOM},
  volume = {73},
  pages = {1--18},
  doi = {10.1007/s11837-021-04902-9},
  abstract = {Scientific articles have long been the primary means of disseminating scientific discoveries. Over the centuries, valuable data and potentially groundbreaking insights have been collected and buried deep in the mountain of publications. In materials engineering, such data are spread across technical handbooks specification sheets, journal articles, and laboratory notebooks in myriad formats. Extracting information from papers on a large scale has been a tedious and time-consuming job to which few researchers have wanted to devote their limited time and effort, yet is an activity that is essential for modern data-driven design practices. However, in recent years, significant progress has been made by the computer science community on techniques for automated information extraction from free text. Yet, transformative application of these techniques to scientific literature remains elusive---due not to a lack of interest or effort but to technical and logistical challenges. Using the challenges in the materials science literature as a driving motivation, we review the gaps between state-of-the-art information extraction methods and the practical application of such methods to scientific texts, and offer a comprehensive overview of work that can be undertaken to close these gaps.},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/NLUQEC5N/Hong et al. - 2021 - Challenges and Advances in Information Extraction from Scientific Literature a Review.pdf}
}

@misc{HowMuchDoes,
  title = {How {{Much Does}} It {{Cost}} to {{Make}} a {{Commercial Ad}}? ({{Video Price}})},
  shorttitle = {How {{Much Does}} It {{Cost}} to {{Make}} a {{Commercial Ad}}?},
  journal = {Start Motion Media Video Company},
  urldate = {2025-01-21},
  abstract = {Start Motion Media Video Services: How much to pay for video ads. Cost factor insights. Celebrating \$1 Billion Client Sales},
  langid = {american},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/FQJFC7NI/commercial-video-cost-how-much-to-pay-for-commercial-video.html}
}

@misc{ibmglobaltechnologyservicesToxicTerabyte2006,
  title = {The Toxic Terabyte},
  author = {{IBM Global Technology Services}},
  year = {2006},
  urldate = {2025-08-26},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CR4HILVE/The Toxic Terabyte.pdf}
}

@article{jrHowBigData2015,
  title = {How {{Big Data Will Change Accounting}}},
  author = {Jr, J. and Moffitt, Kevin and Byrnes, Paul},
  year = {2015},
  month = feb,
  journal = {Accounting Horizons},
  volume = {29},
  pages = {150227130540002},
  doi = {10.2308/acch-51069},
  abstract = {SYNOPSIS Big Data will have increasingly important implications for accounting, even as new types of data become accessible. The video, audio, and textual information made available via Big Data can provide for improved managerial accounting, financial accounting, and financial reporting practices. In managerial accounting, Big Data will contribute to the development and evolution of effective management control systems and budgeting processes. In financial accounting, Big Data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision making. In reporting, Big Data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.}
}

@misc{liAddressingLastMile2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}: {{Using AIS Technologies}} to {{Enhance Governmental Financial Reporting}}},
  shorttitle = {Addressing the {{Last Mile Problem}} in {{Open Government Data}}},
  author = {Li, Huaxia and Wei, Danyang (Kathy) and Moffitt, Kevin and Vasarhelyi, Miklos A.},
  year = {2023},
  month = mar,
  number = {4385883},
  eprint = {4385883},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4385883},
  urldate = {2025-08-26},
  abstract = {{$<$}p{$>$}Although the Open Government Data (OGD) initiative has gained global momentum over the past two decades, the lack of a machine-readable format for much finan},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting Information Systems (AIS) Open Government Data (OGD),Design Science,Government Accounting,Last Mile Problem,Open Government Data (OGD),PDF Extraction,Robotic Process Automation,Text Mining}
}

@misc{liExtractingFinancialData2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {Extracting {{Financial Data}} from {{Unstructured Sources}}: {{Leveraging Large Language Models}}},
  shorttitle = {Extracting {{Financial Data}} from {{Unstructured Sources}}},
  author = {Li, Huaxia and Gao, Haoyun (Harry) and Wu, Chengzhang and Vasarhelyi, Miklos A.},
  year = {2023},
  month = sep,
  number = {4567607},
  eprint = {4567607},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4567607},
  urldate = {2025-01-10},
  abstract = {This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from PDF-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate ESG reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5\% accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96\%. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Accounting information systems Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models,ChatGPT,Data extraction,Design Science,Information processing,Large Language Model (LLM),PDF Reports,Unstructured data},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/UEUDUT7Y/Li et al. - 2023 - Extracting Financial Data from Unstructured Sources Leveraging Large Language Models.pdf}
}

@misc{liProgrammingExampleSolved2024,
  title = {Is {{Programming}} by {{Example}} Solved by {{LLMs}}?},
  author = {Li, Wen-Ding and Ellis, Kevin},
  year = {2024},
  month = nov,
  number = {arXiv:2406.08316},
  eprint = {2406.08316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08316},
  urldate = {2025-08-26},
  abstract = {Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have "solved" PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/ED7B99GA/Li and Ellis - 2024 - Is Programming by Example solved by LLMs.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/AVJXACWT/2406.html}
}

@misc{luLargeLanguageModel2024,
  title = {Large {{Language Model}} for {{Table Processing}}: {{A Survey}}},
  shorttitle = {Large {{Language Model}} for {{Table Processing}}},
  author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
  year = {2024},
  month = oct,
  eprint = {2402.05121},
  primaryclass = {cs},
  doi = {10.1007/s11704-024-40763-6},
  urldate = {2025-01-10},
  abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/85WXEHCW/Lu et al. - 2024 - Large Language Model for Table Processing A Survey.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/UXHNNW2A/2402.html}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9780429029608},
  isbn = {978-0-429-02960-8}
}

@misc{MedianIncomeConsumption,
  title = {Median Income or Consumption per Day},
  journal = {Our World in Data},
  urldate = {2025-01-19},
  abstract = {This data is adjusted for inflation and for differences in the cost of living between countries.},
  howpublished = {https://ourworldindata.org/grapher/daily-median-income},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/VY5LYCMB/daily-median-income.html}
}

@article{mosqueira-reyHumanintheloopMachineLearning2023,
  title = {Human-in-the-Loop Machine Learning: A State of the Art},
  shorttitle = {Human-in-the-Loop Machine Learning},
  author = {{Mosqueira-Rey}, Eduardo and {Hern{\'a}ndez-Pereira}, Elena and {Alonso-R{\'i}os}, David and {Bobes-Bascar{\'a}n}, Jos{\'e} and {Fern{\'a}ndez-Leal}, {\'A}ngel},
  year = {2023},
  month = apr,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {4},
  pages = {3005--3054},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10246-w},
  urldate = {2025-08-24},
  abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.},
  langid = {english},
  keywords = {Active learning,Curriculum learning,Explainable AI,Human-in-the-loop machine learning,Interactive machine learning,Machine teaching},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/HN5ZN5HT/Mosqueira-Rey et al. - 2023 - Human-in-the-loop machine learning a state of the art.pdf}
}

@misc{nassarTableFormerTableStructure2022,
  title = {{{TableFormer}}: {{Table Structure Understanding}} with {{Transformers}}},
  shorttitle = {{{TableFormer}}},
  author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01017},
  eprint = {2203.01017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.01017},
  urldate = {2025-04-07},
  abstract = {Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91\% to 98.5\% on simple tables and from 88.7\% to 95\% on complex tables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/MHEENV38/Nassar et al. - 2022 - TableFormer Table Structure Understanding with Transformers.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/VXX5JEGY/2203.html}
}

@misc{natarajanHumanintheloopAIintheloopAutomate2024,
  title = {Human-in-the-Loop or {{AI-in-the-loop}}? {{Automate}} or {{Collaborate}}?},
  shorttitle = {Human-in-the-Loop or {{AI-in-the-loop}}?},
  author = {Natarajan, Sriraam and Mathur, Saurabh and Sidheekh, Sahil and Stammer, Wolfgang and Kersting, Kristian},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14232},
  eprint = {2412.14232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14232},
  urldate = {2025-08-24},
  abstract = {Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop (AI2L) systems: the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an AI2L perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an AI2L approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4YEXK3FV/Natarajan et al. - 2024 - Human-in-the-loop or AI-in-the-loop Automate or Collaborate.pdf}
}

@misc{pythonologyExtractTextLinks2023,
  title = {Extract Text, Links, Images, Tables from {{Pdf}} with {{Python}} {\textbar} {{PyMuPDF}}, {{PyPdf}}, {{PdfPlumber}} Tutorial},
  author = {{Pythonology}},
  year = {2023},
  month = jan,
  urldate = {2025-01-10},
  abstract = {Use these Python libraries to convert a Pdf into an image, extract text, images, links, and tables from pdfs using the 3 popular Python libraries PyMuPDF, PyPdf, PdfPlumber. Here is source code and article I have written: https://pythonology.eu/what-is-the-be...  -- Support Pythonology -- https://www.buymeacoffee.com/pythonology -- Best Online Resource for Python -- Datacamp: The best online resource to learn Python, Web Scraping, Data analysis, and Data Science (Affiliate link) https://datacamp.pxf.io/pythonology}
}

@misc{qwenteamQwen3ThinkDeeper2025,
  title = {Qwen3: {{Think Deeper}}, {{Act Faster}}},
  shorttitle = {Qwen3},
  author = {{Qwen Team}},
  year = {2025},
  month = apr,
  journal = {Qwen},
  urldate = {2025-08-28},
  abstract = {QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.},
  chapter = {blog},
  howpublished = {https://qwenlm.github.io/blog/qwen3/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/AWFWL8VG/qwen3.html}
}

@misc{ricoRadioAdvertisingCosts2023,
  title = {Radio {{Advertising Costs}}: {{A Simple Guide}} to {{Ad Spend}}},
  shorttitle = {Radio {{Advertising Costs}}},
  author = {Rico, Audrey Rawnie},
  year = {2023},
  month = jul,
  journal = {Fit Small Business},
  urldate = {2025-01-21},
  abstract = {How much does a radio ad cost? Find out all the radio advertising costs you need to know to get your small business started with radio ads.},
  howpublished = {https://fitsmallbusiness.com/radio-advertising-costs/},
  langid = {american},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CE6PQT6N/radio-advertising-costs.html}
}

@article{saitoPrecisionRecallPlotMore2015,
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  year = {2015},
  month = mar,
  journal = {PLOS ONE},
  volume = {10},
  number = {3},
  pages = {e0118432},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118432},
  urldate = {2025-08-24},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  langid = {english},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome-wide association studies,Interpolation,Measurement,MicroRNAs,Support vector machines},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/CTBBR652/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers o.pdf}
}

@misc{senatsverwaltungfuerfinanzenberlinBeteiligungsbericht2024,
  title = {{Beteiligungsbericht}},
  author = {{Senatsverwaltung f{\"u}r Finanzen Berlin}},
  year = {2024},
  month = nov,
  urldate = {2025-01-10},
  abstract = {Beteiligungsberichte},
  langid = {ngerman},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/E2VJKHW4/beteiligungsbericht_2024_gesamt.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/YIZUVU22/artikel.941274.html}
}

@misc{shengPdfTableUnifiedToolkit2024,
  title = {{{PdfTable}}: {{A Unified Toolkit}} for {{Deep Learning-Based Table Extraction}}},
  shorttitle = {{{PdfTable}}},
  author = {Sheng, Lei and Xu, Shuai-Shuai},
  year = {2024},
  month = sep,
  number = {arXiv:2409.05125},
  eprint = {2409.05125},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05125},
  urldate = {2025-01-10},
  abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/Y833Q63R/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/74UPJXVR/2409.html}
}

@misc{stephaniechevalierOnlineShoppersWilling2023,
  title = {Online Shoppers Willing to Wait for Discounts by Age 2022},
  author = {{Stephanie Chevalier}},
  year = {2023},
  journal = {Statista},
  urldate = {2025-01-19},
  abstract = {Younger consumers, specifically Gen Z and millennials, exhibit a higher propensity than older consumers to wait for discounts before making online purchases.},
  howpublished = {https://www.statista.com/statistics/1395897/shoppers-waiting-for-discounts-before-buying-online-age/},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/5BBPQ2LA/shoppers-waiting-for-discounts-before-buying-online-age.html}
}

@misc{teamChameleonMixedModalEarlyFusion2024,
  title = {Chameleon: {{Mixed-Modal Early-Fusion Foundation Models}}},
  shorttitle = {Chameleon},
  author = {Team, Chameleon},
  year = {2024},
  month = may,
  number = {arXiv:2405.09818},
  eprint = {2405.09818},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.09818},
  urldate = {2025-08-28},
  abstract = {We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S3JJJ44G/Team - 2024 - Chameleon Mixed-Modal Early-Fusion Foundation Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/SXYSAWCZ/2405.html}
}

@article{textorRobustCausalInference2016,
  title = {Robust Causal Inference Using Directed Acyclic Graphs: The {{R}} Package `Dagitty'},
  shorttitle = {Robust Causal Inference Using Directed Acyclic Graphs},
  author = {Textor, Johannes and {van der Zander}, Benito and Gilthorpe, Mark S and Li{\'s}kiewicz, Maciej and Ellison, George TH},
  year = {2016},
  month = dec,
  journal = {International Journal of Epidemiology},
  volume = {45},
  number = {6},
  pages = {1887--1894},
  issn = {0300-5771},
  doi = {10.1093/ije/dyw341},
  urldate = {2024-07-27},
  abstract = {Directed acyclic graphs (DAGs), which offer systematic representations of causal relationships, have become an established framework for the analysis of causal inference in epidemiology, often being used to determine covariate adjustment sets for minimizing confounding bias. DAGitty is a popular web application for drawing and analysing DAGs. Here we introduce the R package `dagitty', which provides access to all of the capabilities of the DAGitty web application within the R platform for statistical computing, and also offers several new functions. We describe how the R package `dagitty' can be used to: evaluate whether a DAG is consistent with the dataset it is intended to represent; enumerate `statistically equivalent' but causally different DAGs; and identify exposure-outcome adjustment sets that are valid for causally different but statistically equivalent DAGs. This functionality enables epidemiologists to detect causal misspecifications in DAGs and make robust inferences that remain valid for a range of different DAGs. The R package `dagitty' is available through the comprehensive R archive network (CRAN) at [https://cran.r-project.org/web/packages/dagitty/]. The source code is available on github at [https://github.com/jtextor/dagitty]. The web application `DAGitty' is free software, licensed under the GNU general public licence (GPL) version 2 and is available at [http://dagitty.net/].},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/KPKSMQRL/Textor et al. - 2016 - Robust causal inference using directed acyclic gra.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/6YLDXHXD/2907796.html}
}

@misc{WasSindHaeufigsten,
  title = {{Was sind die h{\"a}ufigsten Fehler bei der Segmentierung Ihres Modepublikums?}},
  urldate = {2025-01-20},
  abstract = {Segmenting your fashion audience is a crucial step to understand your customers, tailor your products and marketing strategies, and increase your sales and loyalty. However, many fashion designers make some common mistakes when segmenting their audience, which can lead to ineffective or even harmful},
  howpublished = {https://www.linkedin.com/advice/0/what-most-common-mistakes-when-segmenting-ujzoe},
  langid = {ngerman},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/J4ZBMCH8/what-most-common-mistakes-when-segmenting-ujzoe.html}
}

@article{wohlinDecisionmakingStructureSelecting2015,
  title = {Towards a Decision-Making Structure for Selecting a Research Design in Empirical Software Engineering},
  author = {Wohlin, Claes and Aurum, Ayb{\"u}ke},
  year = {2015},
  month = dec,
  journal = {Empirical Software Engineering},
  volume = {20},
  number = {6},
  pages = {1427--1455},
  issn = {1573-7616},
  doi = {10.1007/s10664-014-9319-7},
  urldate = {2025-08-24},
  abstract = {Several factors make empirical research in software engineering particularly challenging as it requires studying not only technology but its stakeholders' activities while drawing concepts and theories from social science. Researchers, in general, agree that selecting a research design in empirical software engineering research is challenging, because the implications of using individual research methods are not well recorded. The main objective of this article is to make researchers aware and support them in their research design, by providing a foundation of knowledge about empirical software engineering research decisions, in order to ensure that researchers make well-founded and informed decisions about their research designs. This article provides a decision-making structure containing a number of decision points, each one of them representing a specific aspect on empirical software engineering research. The article provides an introduction to each decision point and its constituents, as well as to the relationships between the different parts in the decision-making structure. The intention is the structure should act as a starting point for the research design before going into the details of the research design chosen. The article provides an in-depth discussion of decision points in relation to the research design when conducting empirical research.},
  langid = {english},
  keywords = {Empirical software engineering research,Research design,Research methods,Selecting research method}
}

@book{wohlinExperimentationSoftwareEngineering2024,
  title = {Experimentation in {{Software Engineering}}},
  author = {Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
  year = {2024},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-69306-3},
  urldate = {2025-08-24},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-662-69305-6 978-3-662-69306-3},
  langid = {english},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/EPFIGBV5/Wohlin et al. - 2024 - Experimentation in Software Engineering.pdf}
}

@misc{Wohnlagen2024,
  title = {{Wohnlagen}},
  year = {2024},
  month = jun,
  urldate = {2024-07-21},
  howpublished = {https://www.berlin.de/sen/wohnen/service/mietspiegel/erlaeuterungen-zum-mietspiegel/wohnlagen/},
  langid = {ngerman},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/D6YQTS5J/wohnlagen.html}
}

@article{wuSurveyHumanintheloopMachine2022,
  title = {A {{Survey}} of {{Human-in-the-loop}} for {{Machine Learning}}},
  author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  year = {2022},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {135},
  eprint = {2108.00941},
  primaryclass = {cs},
  pages = {364--381},
  issn = {0167739X},
  doi = {10.1016/j.future.2022.05.014},
  urldate = {2025-08-24},
  abstract = {Machine learning has become the state-of-the-art technique for many tasks including computer vision, natural language processing, speech processing tasks, etc. However, the unique challenges posed by machine learning suggest that incorporating user knowledge into the system can be beneficial. The purpose of integrating human domain knowledge is also to promote the automation of machine learning. Human-in-theloop is an area that we see as increasingly important in future research due to the knowledge learned by machine learning cannot win human domain knowledge. Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent humanin-the-loop. Using the above categorization, we summarize the major approaches in the field; along with their technical strengths/ weaknesses, we have a simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and to motivate interested readers to consider approaches for designing effective human-in-the-loop solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/P5Y3UKMG/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf}
}

@inproceedings{xuLayoutLMPretrainingText2020,
  title = {{{LayoutLM}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Document Image Understanding}}},
  shorttitle = {{{LayoutLM}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year = {2020},
  month = aug,
  eprint = {1912.13318},
  primaryclass = {cs},
  pages = {1192--1200},
  doi = {10.1145/3394486.3403172},
  urldate = {2025-04-27},
  abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/IADVHXM6/Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Document Image Understanding.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LZU6Z3JF/1912.html}
}

@misc{zhangDocumentParsingUnveiled2024,
  title = {Document {{Parsing Unveiled}}: {{Techniques}}, {{Challenges}}, and {{Prospects}} for {{Structured Information Extraction}}},
  shorttitle = {Document {{Parsing Unveiled}}},
  author = {Zhang, Qintong and Huang, Victor Shea-Jay and Wang, Bin and Zhang, Junyuan and Wang, Zhengren and Liang, Hao and Wang, Shawn and Lin, Matthieu and Zhang, Wentao and He, Conghui},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21169},
  eprint = {2410.21169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21169},
  urldate = {2025-08-26},
  abstract = {Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/S5HD9YFM/Zhang et al. - 2024 - Document Parsing Unveiled Techniques, Challenges, and Prospects for Structured Information Extracti.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/NQMJG9YM/2410.html}
}

@misc{zhangMixtureExpertsLarge2025,
  title = {Mixture of {{Experts}} in {{Large Language Models}}},
  author = {Zhang, Danyang and Song, Junhao and Bi, Ziqian and Yuan, Yingfang and Wang, Tianyang and Yeong, Joe and Hao, Junfeng},
  year = {2025},
  month = jul,
  number = {arXiv:2507.11181},
  eprint = {2507.11181},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.11181},
  urldate = {2025-08-28},
  abstract = {This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/DM22BENS/Zhang et al. - 2025 - Mixture of Experts in Large Language Models.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/LS7IP732/2507.html}
}

@misc{zhongPubLayNetLargestDataset2019,
  title = {{{PubLayNet}}: Largest Dataset Ever for Document Layout Analysis},
  shorttitle = {{{PubLayNet}}},
  author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  year = {2019},
  month = aug,
  number = {arXiv:1908.07836},
  eprint = {1908.07836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.07836},
  urldate = {2025-04-21},
  abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/simon/snap/zotero-snap/common/Zotero/storage/4REPKJNM/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layout analysis.pdf;/home/simon/snap/zotero-snap/common/Zotero/storage/TKQLXWYB/1908.html}
}
