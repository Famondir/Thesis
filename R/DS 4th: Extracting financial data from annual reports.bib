
@misc{nassar_tableformer_2022,
	title = {{TableFormer}: {Table} {Structure} {Understanding} with {Transformers}},
	shorttitle = {{TableFormer}},
	url = {http://arxiv.org/abs/2203.01017},
	doi = {10.48550/arXiv.2203.01017},
	abstract = {Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91\% to 98.5\% on simple tables and from 88.7\% to 95\% on complex tables.},
	urldate = {2025-04-07},
	publisher = {arXiv},
	author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01017 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/MHEENV38/Nassar et al. - 2022 - TableFormer Table Structure Understanding with Transformers.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/VXX5JEGY/2203.html:text/html},
}

@misc{auer_docling_2024,
	title = {Docling {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.09869},
	doi = {10.48550/arXiv.2408.09869},
	abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
	urldate = {2025-04-07},
	publisher = {arXiv},
	author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and Lima, Rafael Teixeira de and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
	month = dec,
	year = {2024},
	note = {arXiv:2408.09869 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/CB5G2B4S/Auer et al. - 2024 - Docling Technical Report.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/I7GPQFC3/2408.html:text/html},
}

@misc{bmi_referat_o2_minikommentar_2013,
	title = {Minikommentar zum {Gesetz} zur {Förderung} der elektroni- schen {Verwaltung} sowie zur Änderung weiterer {Vor}- schriften},
	editor = {{BMI, Referat O2}},
	month = jul,
	year = {2013},
	file = {Minikommentar zum Gesetz zur Förderung der elektroni- schen Verwaltung sowie zur Änderung weiterer Vor- schriften:/home/simon/snap/zotero-snap/common/Zotero/storage/MXWPI7DC/e-government-gesetz-minikommentar.pdf:application/pdf},
}

@misc{noauthor__nodate,
	title = {§ 12 {EGovG} - {Einzelnorm}},
	url = {https://www.gesetze-im-internet.de/egovg/__12.html},
	urldate = {2025-04-07},
	file = {§ 12 EGovG - Einzelnorm:/home/simon/snap/zotero-snap/common/Zotero/storage/5U8LSTMF/__12.html:text/html},
}

@misc{bundesanzeiger_jahresabschluss_nodate,
	title = {Jahresabschluss zum {Geschäftsjahr} vom 01.01.2008 bis zum 31.12.2008},
	url = {https://www.bundesanzeiger.de/pub/de/suchergebnis?7},
	urldate = {2025-01-10},
	author = {{Bundesanzeiger}},
	file = {Suchergebnis – Bundesanzeiger:/home/simon/snap/zotero-snap/common/Zotero/storage/G3DTIWF3/suchergebnis.html:text/html},
}

@misc{grandini_metrics_2020,
	title = {Metrics for {Multi}-{Class} {Classification}: an {Overview}},
	shorttitle = {Metrics for {Multi}-{Class} {Classification}},
	url = {http://arxiv.org/abs/2008.05756},
	doi = {10.48550/arXiv.2008.05756},
	abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
	month = aug,
	year = {2020},
	note = {arXiv:2008.05756 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/WX2FF8RW/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overview.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/XVQUAGR6/2008.html:text/html},
}

@misc{sheng_pdftable_2024,
	title = {{PdfTable}: {A} {Unified} {Toolkit} for {Deep} {Learning}-{Based} {Table} {Extraction}},
	shorttitle = {{PdfTable}},
	url = {http://arxiv.org/abs/2409.05125},
	doi = {10.48550/arXiv.2409.05125},
	abstract = {Currently, a substantial volume of document data exists in an unstructured format, encompassing Portable Document Format (PDF) files and images. Extracting information from these documents presents formidable challenges due to diverse table styles, complex forms, and the inclusion of different languages. Several open-source toolkits, such as Camelot, Plumb a PDF (pdfnumber), and Paddle Paddle Structure V2 (PP-StructureV2), have been developed to facilitate table extraction from PDFs or images. However, each toolkit has its limitations. Camelot and pdfnumber can solely extract tables from digital PDFs and cannot handle image-based PDFs and pictures. On the other hand, PP-StructureV2 can comprehensively extract image-based PDFs and tables from pictures. Nevertheless, it lacks the ability to differentiate between diverse application scenarios, such as wired tables and wireless tables, digital PDFs, and image-based PDFs. To address these issues, we have introduced the PDF table extraction (PdfTable) toolkit. This toolkit integrates numerous open-source models, including seven table recognition models, four Optical character recognition (OCR) recognition tools, and three layout analysis models. By refining the PDF table extraction process, PdfTable achieves adaptability across various application scenarios. We substantiate the efficacy of the PdfTable toolkit through verification on a self-labeled wired table dataset and the open-source wireless Publicly Table Reconition Dataset (PubTabNet). The PdfTable code will available on Github: https://github.com/CycloneBoy/pdf\_table.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Sheng, Lei and Xu, Shuai-Shuai},
	month = sep,
	year = {2024},
	note = {arXiv:2409.05125 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/Y833Q63R/Sheng und Xu - 2024 - PdfTable A Unified Toolkit for Deep Learning-Based Table Extraction.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/74UPJXVR/2409.html:text/html},
}

@inproceedings{chen_tablevlm_2023,
	address = {Toronto, Canada},
	title = {{TableVLM}: {Multi}-modal {Pre}-training for {Table} {Structure} {Recognition}},
	shorttitle = {{TableVLM}},
	url = {https://aclanthology.org/2023.acl-long.137/},
	doi = {10.18653/v1/2023.acl-long.137},
	abstract = {Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97\% in tree-editing-distance-score on ComplexTable.},
	urldate = {2025-01-10},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Leiyuan and Huang, Chengsong and Zheng, Xiaoqing and Lin, Jinshu and Huang, Xuanjing},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2437--2449},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/7LWJ5DED/Chen et al. - 2023 - TableVLM Multi-modal Pre-training for Table Structure Recognition.pdf:application/pdf},
}

@misc{li_extracting_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Extracting {Financial} {Data} from {Unstructured} {Sources}: {Leveraging} {Large} {Language} {Models}},
	shorttitle = {Extracting {Financial} {Data} from {Unstructured} {Sources}},
	url = {https://papers.ssrn.com/abstract=4567607},
	doi = {10.2139/ssrn.4567607},
	abstract = {This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from PDF-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate ESG reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5\% accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96\%. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.},
	language = {en},
	urldate = {2025-01-10},
	publisher = {Social Science Research Network},
	author = {Li, Huaxia and Gao, Haoyun (Harry) and Wu, Chengzhang and Vasarhelyi, Miklos A.},
	month = sep,
	year = {2023},
	keywords = {Accounting information systems Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models, ChatGPT, Data extraction, Design Science, Information processing, Large Language Model (LLM), PDF Reports, Unstructured data},
	file = {Full Text PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/UEUDUT7Y/Li et al. - 2023 - Extracting Financial Data from Unstructured Sources Leveraging Large Language Models.pdf:application/pdf},
}

@misc{pythonology_extract_2023,
	title = {Extract text, links, images, tables from {Pdf} with {Python} {\textbar} {PyMuPDF}, {PyPdf}, {PdfPlumber} tutorial},
	url = {https://www.youtube.com/watch?v=G0PApj7YPBo},
	abstract = {Use these Python libraries to convert a Pdf into an image, extract text, images, links, and tables from pdfs using the 3 popular Python libraries PyMuPDF, PyPdf, PdfPlumber. Here is source code and article I have written:
https://pythonology.eu/what-is-the-be...
 -- Support Pythonology --
https://www.buymeacoffee.com/pythonology
-- Best Online Resource for Python --
Datacamp: The best online resource to learn Python, Web Scraping, Data analysis, and Data Science (Affiliate link)
https://datacamp.pxf.io/pythonology},
	urldate = {2025-01-10},
	author = {{Pythonology}},
	month = jan,
	year = {2023},
}

@misc{bbb_infrastruktur-verwaltungs_gmbh_geschaftsbericht_2024,
	title = {Geschäftsbericht 2023},
	url = {https://www.berlinerbaeder.de/fileadmin/BBB/Dateien/Unternehmen/Geschaeftsberichte/Geschaeftsberichte_der_BBB_Infrastruktur-Verwaltungs_GmbH___BBB_Infrastruktur_GmbH___Co._KG/GB_BBB_Infra_2023.pdf},
	urldate = {2025-01-10},
	author = {{BBB Infrastruktur-Verwaltungs GmbH} and {BBB Infrastruktur GmbH \& Co. KG}},
	month = oct,
	year = {2024},
	file = {GB_BBB_Infra_2023.pdf:/home/simon/snap/zotero-snap/common/Zotero/storage/NN95EQA8/GB_BBB_Infra_2023.pdf:application/pdf},
}

@misc{senatsverwaltung_fur_finanzen_berlin_beteiligungsbericht_2024,
	title = {Beteiligungsbericht},
	url = {https://www.berlin.de/sen/finanzen/vermoegen/downloads/beteiligungsbericht/artikel.941274.php},
	abstract = {Beteiligungsberichte},
	language = {de},
	urldate = {2025-01-10},
	author = {{Senatsverwaltung für Finanzen Berlin}},
	month = nov,
	year = {2024},
	file = {beteiligungsbericht_2024_gesamt:/home/simon/snap/zotero-snap/common/Zotero/storage/E2VJKHW4/beteiligungsbericht_2024_gesamt.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/YIZUVU22/artikel.941274.html:text/html},
}

@misc{lu_large_2024,
	title = {Large {Language} {Model} for {Table} {Processing}: {A} {Survey}},
	shorttitle = {Large {Language} {Model} for {Table} {Processing}},
	url = {http://arxiv.org/abs/2402.05121},
	doi = {10.1007/s11704-024-40763-6},
	abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.},
	urldate = {2025-01-10},
	author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
	month = oct,
	year = {2024},
	note = {arXiv:2402.05121 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/85WXEHCW/Lu et al. - 2024 - Large Language Model for Table Processing A Survey.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/UXHNNW2A/2402.html:text/html},
}

@misc{da_vision_2023,
	title = {Vision {Grid} {Transformer} for {Document} {Layout} {Analysis}},
	url = {http://arxiv.org/abs/2308.14978},
	doi = {10.48550/arXiv.2308.14978},
	abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14978 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/G5Y444YW/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analysis.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/LV6XRTVL/2308.html:text/html},
}

@misc{zhong_publaynet_2019,
	title = {{PubLayNet}: largest dataset ever for document layout analysis},
	shorttitle = {{PubLayNet}},
	url = {http://arxiv.org/abs/1908.07836},
	doi = {10.48550/arXiv.1908.07836},
	abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
	urldate = {2025-04-21},
	publisher = {arXiv},
	author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
	month = aug,
	year = {2019},
	note = {arXiv:1908.07836 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/simon/snap/zotero-snap/common/Zotero/storage/4REPKJNM/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layout analysis.pdf:application/pdf;Snapshot:/home/simon/snap/zotero-snap/common/Zotero/storage/TKQLXWYB/1908.html:text/html},
}
