filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
ungroup() %>% filter(type == classification_type) %>% nrow()
num_true_pos <- num_correct <- nrow(correct_df)
num_false_pos <- num_wrong <- nrow(wrong_df)
num_false_neg <- max(0, num_tables-num_true_pos)
num_true_neg <- total_pages-num_true_pos-num_false_pos-num_false_neg
num_missing <- nrow(missing_df)
runtime <- round(json_data$runtime, 2)
acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2)
recall = round(num_true_pos/(num_true_pos+num_false_neg),2)
F1 = round(2*precision*recall/(precision+recall),2)
# Append the values to the results dataframe
results_df <- results_df %>%
add_row(
package = package,
method = method,
classification_type = classification_type,
true_pos = num_true_pos,
false_pos = num_false_pos,
false_neg = num_false_neg,
true_neg = num_true_neg,
missing = num_missing,
acc = acc,
precision = precision,
recall = recall,
F1 = F1,
runtime_in_s = runtime
)
}
return(results_df %>% as_tibble())
}
metrics <- list()
for (type in c('Aktiva', 'Passiva', 'GuV')) {
metrics[type] <- list(calc_metrics(type))
}
source("./scripts/page_identification_preparations.R")
source("./scripts/page_identification_preparations.R")
# Get a list of all .json files in the folder
json_files <- list.files("../benchmark_results/page_identification/", pattern = "regex.*\\.json$", full.names = TRUE)
calc_metrics <- function(classification_type) {
# Initialize an empty dataframe to store results
results_df <- data.frame(
package = character(),
method = character(),
classification_type = character(),
true_pos = numeric(),
false_pos = numeric(),
false_neg = numeric(),
true_neg = numeric(),
missing = numeric(),
acc = numeric(),
precision = numeric(),
recall = numeric(),
F1 = numeric(),
runtime_in_s = numeric(),
stringsAsFactors = FALSE
)
# Loop through each .json file
for (file in json_files) {
# browser()
# print(file)
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as.data.frame(fromJSON(json_data$correct)) %>% filter(type == classification_type) %>%
filter(file %in% filenames_no_ocr)
wrong_df <- as.data.frame(fromJSON(json_data$wrong)) %>% filter(type == classification_type) %>%
filter(file %in% filenames_no_ocr)
missing_df <- as.data.frame(fromJSON(json_data$missing)) %>% filter(type == classification_type) %>%
filter(file %in% filenames_no_ocr)
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
ungroup() %>% filter(type == classification_type) %>% nrow()
num_true_pos <- num_correct <- nrow(correct_df)
num_false_pos <- num_wrong <- nrow(wrong_df)
num_false_neg <- max(0, num_tables-num_true_pos)
num_true_neg <- total_pages-num_true_pos-num_false_pos-num_false_neg
num_missing <- nrow(missing_df)
runtime <- round(json_data$runtime, 2)
acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2)
recall = round(num_true_pos/(num_true_pos+num_false_neg),2)
F1 = round(2*precision*recall/(precision+recall),2)
# Append the values to the results dataframe
results_df <- results_df %>%
add_row(
package = package,
method = method,
classification_type = classification_type,
true_pos = num_true_pos,
false_pos = num_false_pos,
false_neg = num_false_neg,
true_neg = num_true_neg,
missing = num_missing,
acc = acc,
precision = precision,
recall = recall,
F1 = F1,
runtime_in_s = runtime
)
}
return(results_df %>% as_tibble())
}
metrics <- list()
for (type in c('Aktiva', 'Passiva', 'GuV')) {
metrics[type] <- list(calc_metrics(type))
}
metrics
metric_summaries <- list()
for (df in metrics) {
type <- df$classification_type[[1]]
results_df <- df %>%
group_by(method) %>%
summarise(
precision_mean = mean(precision, na.rm = TRUE),
precision_sd = sd(precision, na.rm = TRUE),
recall_mean = mean(recall, na.rm = TRUE),
recall_sd = sd(recall, na.rm = TRUE),
F1_mean = mean(F1, na.rm = TRUE),
F1_sd = sd(F1, na.rm = TRUE)
) %>%
pivot_longer(
cols = c(precision_mean, precision_sd, recall_mean, recall_sd, F1_mean, F1_sd),
names_to = c("metric", "stat"),
names_pattern = "(.*)_(mean|sd)"
) %>%
pivot_wider(
names_from = metric,
values_from = value
) %>%
mutate_if(is.numeric, ~round(., 3))
metric_summaries[type] <- list(results_df)
}
metric_summaries
calc_metrics_by_company_and_type <- function(classification_type) {
df_list <- list()
# Loop through each .json file
for (file in json_files) {
# browser()
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_correct = n())
wrong_df <- as_tibble(fromJSON(json_data$wrong)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_wrong = n())
# missing_df <- as_tibble(fromJSON(json_data$missing))
# if (nrow(missing_df>0)) {
#     missing_df <- df_missing %>% group_by(company, type) %>% summarise(n_missing = n())
#   }
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
runtime <- round(json_data$runtime, 2)
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
file_count <- data_unnested %>% rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% select(filepath, company) %>% unique() %>% group_by(company) %>% summarise(n_files = n())
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_list[filename] <- list(df_results)
}
return(bind_rows(df_list) %>% as_tibble())
}
metrics_by_company_and_type <- calc_metrics_by_company_and_type(type)
metrics_by_company_and_type
calc_metrics_by_company_and_type <- function() {
df_list <- list()
# Loop through each .json file
for (file in json_files) {
# browser()
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_correct = n())
wrong_df <- as_tibble(fromJSON(json_data$wrong)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_wrong = n())
# missing_df <- as_tibble(fromJSON(json_data$missing))
# if (nrow(missing_df>0)) {
#     missing_df <- df_missing %>% group_by(company, type) %>% summarise(n_missing = n())
#   }
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
runtime <- round(json_data$runtime, 2)
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
file_count <- data_unnested %>% rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% select(filepath, company) %>% unique() %>% group_by(company) %>% summarise(n_files = n())
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_list[filename] <- list(df_results)
}
return(bind_rows(df_list) %>% as_tibble())
}
metrics_by_company_and_type <- calc_metrics_by_company_and_type()
metrics_by_company_and_type %>% filter(str_detect(company, "Netz"))
metrics_by_company_and_type %>% filter(str_detect(company, "Netz"), type == "GuV")
metrics_by_company_and_type %>% filter(str_detect(company, "Netz"), classification_type == "GuV")
calc_metrics_by_company_and_type <- function() {
df_list <- list()
# Loop through each .json file
for (file in json_files) {
browser()
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_correct = n())
wrong_df <- as_tibble(fromJSON(json_data$wrong)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_wrong = n())
# missing_df <- as_tibble(fromJSON(json_data$missing))
# if (nrow(missing_df>0)) {
#     missing_df <- df_missing %>% group_by(company, type) %>% summarise(n_missing = n())
#   }
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
runtime <- round(json_data$runtime, 2)
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
file_count <- data_unnested %>% rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% select(filepath, company) %>% unique() %>% group_by(company) %>% summarise(n_files = n())
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_list[filename] <- list(df_results)
}
return(bind_rows(df_list) %>% as_tibble())
}
metrics_by_company_and_type <- calc_metrics_by_company_and_type()
file
correct_df
num_tables
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>%
# filter(file %in% filenames_no_ocr) %>%
group_by(company, type) %>% summarise(n_correct = n())
correct_df
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>%
filter(file %in% filenames_no_ocr) %>%
group_by(company, type) %>% summarise(n_correct = n())
num_tables <- data_unnested %>% # anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
num_tables
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
num_tables
df_results
df_results
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = n_total-n_correct,
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_results
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = n_total-n_correct,
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_results
source("~/Documents/data_science/Thesis/R/scripts/page_identification_regex.R", echo = TRUE)
source("./scripts/page_identification_preparations.R")
num_multiple_tables_per_document <- df_targets_no_ocr %>%
anti_join(consecutive_pages, by = c("filepath", "page", "type")) %>%
arrange(filepath, page) %>%
group_by(filepath, type) %>%
summarise(count = n()) %>%
filter(count > 1) %>%
group_by(filepath) %>%
summarise(count = n()) %>%
nrow()
multiple_tables_per_type_and_document <- data_unnested %>%
anti_join(consecutive_pages, by = c("filepath", "page", "type")) %>%
arrange(filepath, page) %>%
group_by(filepath, type) %>%
summarise(count = n()) %>%
filter(count > 1) %>%
group_by(type) %>%
summarise("multiple targets in document" = n())
df_special_targets <- multiple_tables_per_type_and_document %>% left_join(consecutive_pages %>% group_by(type) %>% summarise("target two pages long" = n())) %>% mutate_if(is.numeric,coalesce,0)
data_page_identification_regex <- readRDS("data_storage/page_identification_regex.rds")
metrics <- data_page_identification_regex$metrics
metric_summaries <- data_page_identification_regex$metric_summaries
metrics_by_company_and_type <- data_page_identification_regex$metrics_by_company_and_type
metrics_plot_regex_page_identification <- metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c(
# "acc",
"precision", "recall", "F1")) %>%
ggplot() +
geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0) +
facet_grid(metric~classification_type) +
scale_x_discrete(guide = guide_axis(angle = 30)) +
theme(
legend.position = "bottom"
) +
coord_cartesian(ylim = c(0, 1))
metrics_by_company_and_type %>%
select(company, n_files, method, precision, recall, classification_type) %>%
pivot_longer(cols=c(precision, recall), names_to = "metric") %>%
ggplot() +
geom_boxplot(aes(x = company, y = value, fill = n_files), alpha = 0.5) +
geom_jitter(aes(x = company, y = value, color = method), alpha = 1, height = 0) +
facet_grid(classification_type~metric) +
scale_x_discrete(guide = guide_axis(angle = 30)) +
theme(
legend.position = "bottom"
)
consecutive_pages
consecutive_pages$filepath
metrics_by_company_and_type %>% filter(str_detect(company, "Netz"))
metrics_by_company_and_type %>% filter(str_detect(company, "Netz")) %>% .$company
df_targets_no_ocr
df_targets_no_ocr %>% filter(str_detect(filepath, "Netz"))
num_two_tables_on_one_page
consecutive_pages
calc_metrics_by_company_and_type <- function() {
df_list <- list()
# Loop through each .json file
for (file in json_files) {
browser()
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as_tibble(fromJSON(json_data$correct)) %>%
filter(file %in% filenames_no_ocr) %>%
group_by(company, type) %>% summarise(n_correct = n())
wrong_df <- as_tibble(fromJSON(json_data$wrong)) %>% filter(file %in% filenames_no_ocr) %>% group_by(company, type) %>% summarise(n_wrong = n())
# missing_df <- as_tibble(fromJSON(json_data$missing))
# if (nrow(missing_df>0)) {
#     missing_df <- df_missing %>% group_by(company, type) %>% summarise(n_missing = n())
#   }
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
runtime <- round(json_data$runtime, 2)
num_tables <- data_unnested %>% anti_join(consecutive_pages) %>%
rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% group_by(type, company) %>% summarise(n_total = n())
file_count <- data_unnested %>% rowwise() %>% mutate(
company = str_split(filepath, "/")[[1]][3]
) %>% select(filepath, company) %>% unique() %>% group_by(company) %>% summarise(n_files = n())
df_results <- correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
df_list[filename] <- list(df_results)
}
return(bind_rows(df_list) %>% as_tibble())
}
metrics_by_company_and_type <- calc_metrics_by_company_and_type()
metrics_by_company_and_type <- calc_metrics_by_company_and_type()
n_total
correct_df %>% full_join(wrong_df) %>% full_join(num_tables)
correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
)
correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, (n_total-n_correct)),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
) %>% rename(classification_type = type)
correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% ungroup() %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, (n_total-n_correct)),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
)
correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% ungroup() %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = (n_total-n_correct),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
)
correct_df %>% full_join(wrong_df) %>% full_join(num_tables) %>%
full_join(file_count) %>% rowwise() %>% mutate(
num_true_pos = n_correct,
num_false_pos = n_wrong,
num_false_neg = max(0, (n_total-n_correct)),
# acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2),
recall = round(num_true_pos/(num_true_pos+num_false_neg),2),
F1 = round(2*precision*recall/(precision+recall),2),
package = package,
method = method,
runtime_in_s = runtime
)
source("~/Documents/data_science/Thesis/R/scripts/page_identification_regex.R", echo = TRUE)
distance_confidence_plot <- function(df) {
df %>%
mutate(
min_confidence = log(min(min_confidence))
) %>%
ggplot() +
geom_point(aes(x = min_confidence, y = min_distance, color = in_range, shape = type)) +
stat_smooth(aes(x = min_confidence, y = min_distance), method = 'lm', formula = y ~ x+0, se = TRUE, fullrange = TRUE) +
xlim(-3, 0)
}
file_content <- readLines("../benchmark_truth/toc_data.json", warn = FALSE)
file_content
toc_data <- fromJSON(paste(file_content, collapse = "\n"))
toc_data
View(toc_data)
n_toc = nrow(toc_data$files_with_toc)
n_no_toc = length(toc_data$files_without_toc)
install.packages("palmerpenguins")
