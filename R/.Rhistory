# Calculate micro and macro metrics
# Micro: sum over all classes
tp_micro <- sum(sapply(metrics, function(x) x$true_positive))
fp_micro <- sum(sapply(metrics, function(x) x$false_positive))
fn_micro <- sum(sapply(metrics, function(x) x$false_negative))
tn_micro <- sum(sapply(metrics, function(x) x$true_negative))
n_micro <- sum(sapply(metrics, function(x) x$n_pages))
micro_precision <- tp_micro / (tp_micro + fp_micro)
micro_recall <- tp_micro / (tp_micro + fn_micro)
micro_f1 <- ifelse(micro_precision + micro_recall != 0, 2 * micro_precision * micro_recall / (micro_precision + micro_recall), 0)
micro_accuracy <- (tp_micro + tn_micro) / n_micro
# Macro: mean over all classes
macro_precision <- mean(sapply(metrics, function(x) x$precision), na.rm = TRUE)
macro_recall <- mean(sapply(metrics, function(x) x$recall), na.rm = TRUE)
macro_f1 <- mean(sapply(metrics, function(x) x$f1_score), na.rm = TRUE)
macro_accuracy <- mean(sapply(metrics, function(x) x$accuracy), na.rm = TRUE)
new_metrics <- list()
micro_name <- str_c("micro", suffix)
new_metrics[[micro_name]] <- list(
metric_type = micro_name,
true_positive = tp_micro,
false_positive = fp_micro,
false_negative = fn_micro,
true_negative = tn_micro,
n_pages = n_micro,
precision = micro_precision,
recall = micro_recall,
f1_score = micro_f1,
accuracy = micro_accuracy
)
macro_name <- str_c("macro", suffix)
new_metrics[[macro_name]] <- list(
metric_type = macro_name,
true_positive = NA,
false_positive = NA,
false_negative = NA,
true_negative = NA,
n_pages = NA,
precision = macro_precision,
recall = macro_recall,
f1_score = macro_f1,
accuracy = macro_accuracy
)
return(new_metrics)
}
recalc_mectrics_multiclass <- function(df) {
recalc_mectrics_singleclass <- function(df_reeval, classification_type) {
tp <- df_reeval %>% filter(predicted_type == classification_type, match == TRUE) %>% nrow()
fp <- df_reeval %>% filter(predicted_type == classification_type, match == FALSE) %>% nrow()
fn <- df_reeval %>% filter(type == classification_type, match == FALSE) %>% nrow()
tn <- df_reeval %>% filter(type != classification_type, match == TRUE) %>% nrow()
n <- df_reeval %>% nrow()
accuracy <- (tp + tn)/(n)
precision <- tp/(tp+fp)
recall <- tp/(tp+fn)
f1_score <- ifelse(precision+recall != 0, 2*precision*recall/(precision+recall), 0)
metrics <- list()
metrics$metric_type <- classification_type
metrics$true_positive <- tp
metrics$false_positive <- fp
metrics$false_negative <- fn
metrics$true_negative <- tn
metrics$n_pages <- n
metrics$accuracy <- accuracy
metrics$precision <- precision
metrics$recall <- recall
metrics$f1_score <- f1_score
return(metrics %>% as_tibble())
}
df_reeval <- df %>%
select(-type, -match) %>%
group_by(across(-confidence_score)) %>%
summarise(confidence_score = mean(confidence_score, na.rm = TRUE), .groups = "drop") %>%
left_join(df_labels, relationship = "many-to-many", by = join_by(filepath, page)) %>%
mutate(
type = if_else(is.na(type), "other", type),
# match = if_else(predicted_type == classification_type, str_detect(type, predicted_type), !str_detect(type, classification_type))
match = str_detect(type, predicted_type)
)
metrics <- list()
for (class in c('Aktiva', 'GuV', 'Passiva', 'other')) {
metrics[class] <- list(recalc_mectrics_singleclass(df_reeval, class))
}
metrics_minorities <- metrics[names(metrics) %in% c('Aktiva', 'GuV', 'Passiva')]
new_metrics <- calculate_micro_macro_metrics(metrics_minorities, "_minorities")
# browser()
new_metrics2 <- calculate_micro_macro_metrics(metrics, "")
metrics <- c(metrics, new_metrics, new_metrics2)
metrics <- metrics %>% bind_rows()
results <- list(predictions = list(df_reeval), metrics = list(metrics))
return(results)
}
{
json_files_page_identification_llm <- list.files(
"../benchmark_results/page_identification/final/llm/",
pattern = "\\.json$",
full.names = TRUE
) %>%
.[!grepl("_test_", .)] %>%
.[grepl("_four_", .)]
meta_list_llm <- list()
# Loop through each .json file
for (file in json_files_page_identification_llm) {
# print(file)
file_content <- readLines(file, warn = FALSE)
json_data <- fromJSON(paste(file_content, collapse = "\n"))
name_split = (basename(file) %>% str_replace('__no_think', '') %>% str_split("__"))[[1]]
method_index = which(str_starts(name_split, "loop"))-1
# print(name_split)
predictions <- fromJSON(json_data$results)
# classification_type <- str_split(name_split[2], '_')[[1]][3]
results <- recalc_mectrics_multiclass(predictions) %>% as_tibble() %>%
as_tibble() %>% rowwise() %>%
mutate(
model = str_replace(name_split[1], "_vllm", ""),
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(method, "out_of_company"), NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_company', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
classifier_type = paste(str_split(name_split[2], '_')[[1]][c(2,3)], collapse = "_"),
classification_type = NA,
runtime = json_data$runtime,
# predictions = list(predictions),
.before = 1
)
meta_list_llm[[length(meta_list_llm) + 1]] <- results
}
df_multi <- meta_list_llm %>% bind_rows() %>% mutate(
n_examples = as.integer(n_examples)
)
}
norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors.csv") %>%
mutate(
model_name = model_name %>% str_replace("/", "_")
) %>% filter(str_detect(filename, "multi"))
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml")))
norm_factors_many_examples <- norm_factors %>% filter(!(str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml"))) %>%
add_column(n_examples = list(c(7,9,11,13), c(5))) %>% unnest(n_examples)
df_multi_few_examples <- df_multi %>% filter(n_examples <= 3 | is.na(n_examples)) %>%
left_join(norm_factors_few_examples, by = c("model" = "model_name")) %>% mutate(
norm_runtime = runtime*normalization_factor
)
df_multi_many_examples <- df_multi %>% filter(n_examples > 3) %>%
left_join(norm_factors_many_examples, by = c("model" = "model_name", "n_examples" = "n_examples")) %>% mutate(
norm_runtime = runtime*normalization_factor
)
df_multi <- bind_rows(
df_multi_few_examples,
df_multi_many_examples
)
df_selected <- df_multi %>% unnest(metrics) %>% filter(metric_type == "Aktiva")
df_selected$model
df_selected %>%
filter(model %in% c(
"mistralai_Ministral-8B-Instruct-2410",
"mistralai_Mistral-Large-Instruct-2411",
"mistralai_Mistral-Small-3.1-24B-Instruct-2503",
"meta-llama_Llama-4-Scout-17B-16E-Instruct",
"meta-llama_Llama-4-Maverick-17B-128E-Instruct-FP8"
)) %>%
ggplot(aes(x = norm_runtime, y = f1_score)) +
geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
scale_shape(na.value = 15, guide = "legend") +
geom_text(aes(label = n_examples)) +
facet_grid(model~metric_type) +
theme(legend.position = "bottom") +
guides(
color = guide_legend(ncol = 1, title.position = "top"),
shape = guide_legend(ncol = 1, title.position = "top")
) +
scale_x_discrete(guide = guide_axis(angle = 30))
df_selected %>%
ggplot(aes(x = norm_runtime, y = f1_score)) +
geom_point(aes(color = method_family, shape = out_of_company), size = 7, alpha = .6) +
scale_shape(na.value = 15, guide = "legend") +
geom_text(aes(label = n_examples)) +
facet_grid(metric_type~model) +
theme(legend.position = "bottom") +
guides(
color = guide_legend(ncol = 1, title.position = "top"),
shape = guide_legend(ncol = 1, title.position = "top")
) +
scale_x_discrete(guide = guide_axis(angle = 30))
source("~/Documents/data_science/Thesis/R/scripts/page_identification_regex.R", echo = TRUE)
calc_metrics <- function(classification_type) {
# Initialize an empty dataframe to store results
results_df <- data.frame(
package = character(),
method = character(),
classification_type = character(),
true_pos = numeric(),
false_pos = numeric(),
false_neg = numeric(),
true_neg = numeric(),
missing = numeric(),
acc = numeric(),
precision = numeric(),
recall = numeric(),
F1 = numeric(),
runtime_in_s = numeric(),
stringsAsFactors = FALSE
)
# Loop through each .json file
for (file in json_files) {
browser()
# Read the JSON file
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as.data.frame(fromJSON(json_data$correct)) %>% filter(type == classification_type)
wrong_df <- as.data.frame(fromJSON(json_data$wrong)) %>% filter(type == classification_type)
missing_df <- as.data.frame(fromJSON(json_data$missing)) %>% filter(type == classification_type)
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
num_tables <- data_unnested %>% filter(type == classification_type) %>% nrow()
num_true_pos <- num_correct <- nrow(correct_df)
num_false_pos <- num_wrong <- nrow(wrong_df)
num_false_neg <- num_tables-num_correct
num_true_neg <- total_pages-num_true_pos-num_false_pos-num_false_neg
num_missing <- nrow(missing_df)
runtime <- round(json_data$runtime, 2)
acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2)
recall = round(num_true_pos/(num_true_pos+num_false_neg),2)
F1 = round(2*precision*recall/(precision+recall),2)
# Append the values to the results dataframe
results_df <- results_df %>%
add_row(
package = package,
method = method,
classification_type = classification_type,
true_pos = num_true_pos,
false_pos = num_false_pos,
false_neg = num_false_neg,
true_neg = num_true_neg,
missing = num_missing,
acc = acc,
precision = precision,
recall = recall,
F1 = F1,
runtime_in_s = runtime
)
}
return(results_df %>% as_tibble())
}
metrics <- list()
for (type in c('Aktiva', 'Passiva', 'GuV')) {
metrics[type] <- list(calc_metrics(type))
}
View(json_data)
json_data[["special"]]
file
# Loop through each .json file
for (file in json_files) {
# browser()
# Read the JSON file
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as.data.frame(fromJSON(json_data$correct)) %>% filter(type == classification_type)
wrong_df <- as.data.frame(fromJSON(json_data$wrong)) %>% filter(type == classification_type)
missing_df <- as.data.frame(fromJSON(json_data$missing)) %>% filter(type == classification_type)
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
num_tables <- data_unnested %>% filter(type == classification_type) %>% nrow()
num_true_pos <- num_correct <- nrow(correct_df)
num_false_pos <- num_wrong <- nrow(wrong_df)
num_false_neg <- num_tables-num_correct
num_true_neg <- total_pages-num_true_pos-num_false_pos-num_false_neg
num_missing <- nrow(missing_df)
runtime <- round(json_data$runtime, 2)
acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2)
recall = round(num_true_pos/(num_true_pos+num_false_neg),2)
F1 = round(2*precision*recall/(precision+recall),2)
# Append the values to the results dataframe
results_df <- results_df %>%
add_row(
package = package,
method = method,
classification_type = classification_type,
true_pos = num_true_pos,
false_pos = num_false_pos,
false_neg = num_false_neg,
true_neg = num_true_neg,
missing = num_missing,
acc = acc,
precision = precision,
recall = recall,
F1 = F1,
runtime_in_s = runtime
)
}
calc_metrics <- function(classification_type) {
# Initialize an empty dataframe to store results
results_df <- data.frame(
package = character(),
method = character(),
classification_type = character(),
true_pos = numeric(),
false_pos = numeric(),
false_neg = numeric(),
true_neg = numeric(),
missing = numeric(),
acc = numeric(),
precision = numeric(),
recall = numeric(),
F1 = numeric(),
runtime_in_s = numeric(),
stringsAsFactors = FALSE
)
# Loop through each .json file
for (file in json_files) {
# browser()
# Read the JSON file
json_data <- fromJSON(file)
# Extract the required values
correct_df <- as.data.frame(fromJSON(json_data$correct)) %>% filter(type == classification_type)
wrong_df <- as.data.frame(fromJSON(json_data$wrong)) %>% filter(type == classification_type)
missing_df <- as.data.frame(fromJSON(json_data$missing)) %>% filter(type == classification_type)
filename <- basename(file)
package <- strsplit(filename, "_")[[1]][1]
method <- gsub("\\.json$", "", paste(strsplit(filename, "_")[[1]][-1], collapse = " "))
num_tables <- data_unnested %>% filter(type == classification_type) %>% nrow()
num_true_pos <- num_correct <- nrow(correct_df)
num_false_pos <- num_wrong <- nrow(wrong_df)
num_false_neg <- num_tables-num_correct
num_true_neg <- total_pages-num_true_pos-num_false_pos-num_false_neg
num_missing <- nrow(missing_df)
runtime <- round(json_data$runtime, 2)
acc = round((num_true_pos+num_true_neg)/(total_pages),2)
precision = round(num_true_pos/(num_true_pos+num_false_pos),2)
recall = round(num_true_pos/(num_true_pos+num_false_neg),2)
F1 = round(2*precision*recall/(precision+recall),2)
# Append the values to the results dataframe
results_df <- results_df %>%
add_row(
package = package,
method = method,
classification_type = classification_type,
true_pos = num_true_pos,
false_pos = num_false_pos,
false_neg = num_false_neg,
true_neg = num_true_neg,
missing = num_missing,
acc = acc,
precision = precision,
recall = recall,
F1 = F1,
runtime_in_s = runtime
)
}
return(results_df %>% as_tibble())
}
metrics <- list()
for (type in c('Aktiva', 'Passiva', 'GuV')) {
metrics[type] <- list(calc_metrics(type))
}
metric_summaries <- list()
for (df in metrics) {
type <- df$classification_type[[1]]
results_df <- df %>%
group_by(method) %>%
summarise(
precision_mean = mean(precision, na.rm = TRUE),
precision_sd = sd(precision, na.rm = TRUE),
recall_mean = mean(recall, na.rm = TRUE),
recall_sd = sd(recall, na.rm = TRUE),
F1_mean = mean(F1, na.rm = TRUE),
F1_sd = sd(F1, na.rm = TRUE)
) %>%
pivot_longer(
cols = c(precision_mean, precision_sd, recall_mean, recall_sd, F1_mean, F1_sd),
names_to = c("metric", "stat"),
names_pattern = "(.*)_(mean|sd)"
) %>%
pivot_wider(
names_from = metric,
values_from = value
) %>%
mutate_if(is.numeric, ~round(., 3))
metric_summaries[type] <- list(results_df)
}
metric_summaries
metric_summaries %>% as.tibble()
metric_summaries %>% as_tibble()
metric_summaries %>% bind_rows()
metrics$Aktiva
metrics$Passiva
metrics$GuV
metric_summaries %>% bind_rows()
metrics %>% bind_rows()
metrics %>% bind_rows() %>% ggplot() +
geom_scatterplot(aes(x = precision, y = recall, color = method)) +
metrics %>% bind_rows() %>% ggplot() +
geom_scatterplot(aes(x = precision, y = recall, color = method))
metrics %>% bind_rows() %>% ggplot() +
geom_scatter(aes(x = precision, y = recall, color = method))
metrics %>% bind_rows() %>% ggplot() +
geom_point(aes(x = precision, y = recall, color = method))
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = metric), alpha = 0.5)
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = metric), alpha = 0.5) +
facet_wrap(~classification_type, scales = "free_y") +
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = metric), alpha = 0.5) +
facet_wrap(~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = value), alpha = 0.5) +
facet_wrap(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = value), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
ggplot() +
geom_point(aes(x = method, y = value, color = package), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
"
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
ggplot() +
geom_point(aes(x = method, y = value, color = package), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metrc %in% c("acc", "precision", "recall", "F1")) %>%
ggplot() +
geom_point(aes(x = method, y = value, color = package), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c("acc", "precision", "recall", "F1")) %>%
ggplot() +
geom_point(aes(x = method, y = value, color = package), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c("acc", "precision", "recall", "F1")) %>%
ggplot() +
geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c("acc", "precision", "recall", "F1")) %>%
ggplot() +
geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0.02) +
facet_grid(metric~classification_type, scales = "free_y")
metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c("acc", "precision", "recall", "F1")) %>%
ggplot() +
geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0.02) +
facet_grid(metric~classification_type)
