method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
predictions = list(fromJSON(df_joined) %>% as_tibble()),
runtime = json_data$runtime,
request_tokens = list(json_data$request_tokens)
) %>% select(-df_joined)
meta_list_llm[[length(meta_list_llm) + 1]] <- results
}
if (nrow(errors) > 0) {
# browser()
error_list[[length(meta_list_llm) + 1]] <- errors %>% rowwise() %>%
mutate(
model = name_split[1],
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(basename(file), "out_of_sample") == TRUE, NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
request_tokens = list(json_data$request_tokens)
)
}
# results$predictions <- predictions
}
# Loop through each .json file
for (file in json_files_table_extraction_llm) {
print(file)
# Read the JSON file
# Read the JSON file and replace NaN with NULL in the file content
file_content <- readLines(file, warn = FALSE)
file_content <- gsub("\\bNaN\\b", "null", file_content)
file_content <- gsub("\\bInfinity\\b", "null", file_content)
# Remove incomplete last JSON entry and close the list if file ends early
# if (!grepl("\\]$", file_content[length(file_content)])) {
#   # Find the last complete JSON object (ends with "},")
#   last_complete <- max(grep('\\.pdf', file_content))
#   file_content <- c(file_content[1:last_complete], "}]")
# }
json_data <- fromJSON(paste(file_content, collapse = "\n"))
name_split = (basename(file) %>% str_split("__"))[[1]]
method_index = which(str_starts((basename(file) %>% str_split("__"))[[1]], "loop"))-1
# print(name_split)
results <- json_data$results %>% as_tibble() %>% filter(!json_error)
errors <- json_data$results %>% as_tibble() %>% filter(json_error)
if (nrow(results) > 0) {
results <- results %>% rowwise() %>%
mutate(
model = name_split[1],
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(basename(file), "out_of_sample") == TRUE, NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
predictions = list(fromJSON(df_joined) %>% as_tibble()),
runtime = json_data$runtime,
request_tokens = list(json_data$request_tokens)
) %>% select(-df_joined)
meta_list_llm[[length(meta_list_llm) + 1]] <- results
}
if (nrow(errors) > 0) {
# browser()
error_list[[length(meta_list_llm) + 1]] <- errors %>% rowwise() %>%
mutate(
model = name_split[1],
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(basename(file), "out_of_sample") == TRUE, NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
request_tokens = list(json_data$request_tokens)
)
}
# results$predictions <- predictions
}
df_errors <- bind_rows(error_list)
df_errors
error_list
df_azure$model %>% unique()
json_files_table_extraction_llm <- list.files(
"../benchmark_results/table_extraction/llm/final/real_tables_more_examples/openai/",
pattern = "\\.json$",
full.names = TRUE
) %>%
.[!grepl("_test_", .)]
meta_list_llm <- list()
error_list <- list()
# Loop through each .json file
for (file in json_files_table_extraction_llm) {
# print(file)
# Read the JSON file
# Read the JSON file and replace NaN with NULL in the file content
file_content <- readLines(file, warn = FALSE)
file_content <- gsub("\\bNaN\\b", "null", file_content)
file_content <- gsub("\\bInfinity\\b", "null", file_content)
# Remove incomplete last JSON entry and close the list if file ends early
# if (!grepl("\\]$", file_content[length(file_content)])) {
#   # Find the last complete JSON object (ends with "},")
#   last_complete <- max(grep('\\.pdf', file_content))
#   file_content <- c(file_content[1:last_complete], "}]")
# }
json_data <- fromJSON(paste(file_content, collapse = "\n"))
name_split = (basename(file) %>% str_split("__"))[[1]]
method_index = which(str_starts((basename(file) %>% str_split("__"))[[1]], "loop"))-1
# print(name_split)
results <- json_data$results %>% as_tibble() %>% filter(!json_error)
errors <- json_data$results %>% as_tibble() %>% filter(json_error)
if (nrow(results) > 0) {
results <- results %>% rowwise() %>%
mutate(
model = name_split[1],
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(basename(file), "out_of_sample") == TRUE, NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
predictions = list(fromJSON(df_joined) %>% as_tibble()),
runtime = json_data$runtime,
request_tokens = list(json_data$request_tokens)
) %>% select(-df_joined)
meta_list_llm[[length(meta_list_llm) + 1]] <- results
}
if (nrow(errors) > 0) {
# browser()
error_list[[length(meta_list_llm) + 1]] <- errors %>% rowwise() %>%
mutate(
model = name_split[1],
method = name_split[method_index],
n_examples = str_match(method, "\\d+")[[1]],
out_of_company = if_else(str_detect(method, "rag"), str_detect(basename(file), "out_of_sample") == TRUE, NA),
method_family = str_replace(str_replace(method, '\\d+', 'n'), '_out_of_sample', ''),
loop = as.numeric((basename(file) %>% str_match("loop_(.)(_queued)?\\.json"))[2]),
request_tokens = list(json_data$request_tokens)
)
}
# results$predictions <- predictions
}
df_errors <- bind_rows(error_list)
df_azure <- bind_rows(meta_list_llm) %>% select(!starts_with("changed_values")) %>%
filter(grammar_error != TRUE || is.na(grammar_error)) %>%
unnest_wider(`NA`, names_sep = "_") %>%
unnest_wider(`relative_numeric_difference`, names_sep = "_") %>%
unnest_wider(`levenstein_distance`, names_sep = "_") %>%
# rename_with(~ gsub("^NA_", "NA_", .x)) %>%  # Ensures prefix is NA_
mutate(
NA_total_truth = NA_true_positive + NA_false_negative,
NA_precision = if_else(NA_total_truth > 0, if_else((NA_true_positive + NA_false_positive)>0, NA_true_positive/(NA_true_positive + NA_false_positive), 0), NA),
NA_recall = if_else(NA_total_truth > 0, NA_true_positive/(NA_true_positive + NA_false_negative), NA),
NA_F1 = if_else((NA_precision + NA_recall) > 0, (2 * NA_precision * NA_recall)/(NA_precision + NA_recall), 0),
percentage_correct_numeric = if_else((correct_numeric + incorrect_numeric)>0, correct_numeric/(correct_numeric + incorrect_numeric), 0),
percentage_correct_total = (correct_numeric + NA_true_positive)/total_entries
) %>% mutate(
model_family = "chat-gpt"
) %>% mutate(
model = str_replace(model, "_vllm", "")
) %>% mutate(
n_examples = as.numeric(n_examples),
n_examples = if_else(method_family == "zero_shot", 0, n_examples),
n_examples = if_else(method_family == "static_example", 1, n_examples)
)
df_azure %>% filter(model %in% c(
"gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
"openai_gpt-oss-20b", "gpt-oss-120b_azure", "gpt-5-mini_azure"
)) %>% mutate(
model = str_remove(model, "_azure")
) %>%
saveRDS("data_storage/real_table_extraction_azure.rds")
df_azure$model %>% unique()
df_azure %>%
# filter(model %in% c(
#   "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
#   "openai_gpt-oss-20b", "gpt-oss-120b_azure", "gpt-5-mini_azure"
# )) %>%
mutate(
model = str_remove(model, "_azure")
) %>%
saveRDS("data_storage/real_table_extraction_azure.rds")
df_errors
df_azure %>%
# filter(model %in% c(
#   "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
#   "openai_gpt-oss-20b", "gpt-oss-120b_azure", "gpt-5-mini_azure"
# )) %>%
mutate(
model = str_remove(model, "_azure")
) %>%
saveRDS("data_storage/real_table_extraction_extended_azure.rds")
df_errors %>%
saveRDS("data_storage/real_table_extraction_extended_azure_errors.rds")
df_errors
df_real_table_extraction <- readRDS("data_storage/real_table_extraction_extended_llm.rds") %>%
filter(!model %in% c("deepseek-ai_DeepSeek-R1-Distill-Qwen-32B", 'google_gemma-3n-E4B-it')) %>%
mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_synth <- readRDS("data_storage/real_table_extraction_extended_synth.rds") %>%
mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_azure <- readRDS("data_storage/real_table_extraction_extended_azure.rds") %>%
mutate(model = gsub("^[^_]+_", "", model))
model_by_size <- c(
'gemma-3-4b-it', #'gemma-3n-E4B-it',
"gemma-3-12b-it", "gemma-3-27b-it",
"Llama-3.1-8B-Instruct", "Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct",
"Llama-4-Scout-17B-16E-Instruct", "Llama-4-Maverick-17B-128E-Instruct-FP8",
"Ministral-8B-Instruct-2410", "Mistral-Small-3.1-24B-Instruct-2503",
"Mistral-Large-Instruct-2411", "Qwen2.5-0.5B-Instruct",
"Qwen2.5-1.5B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct",
"Qwen2.5-14B-Instruct", "Qwen2.5-32B-Instruct", "Qwen2.5-72B-Instruct",
"Qwen3-0.6B", "Qwen3-1.7B", "Qwen3-4B",
"Qwen3-8B", "Qwen3-14B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-32B",
"Qwen3-235B-A22B-Instruct-2507-FP8", "Qwen3-235B-A22B-Instruct-2507",
# "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
"Falcon3-10B-Instruct", "phi-4"
)
method_order <- c("top_n_rag_examples", "n_random_examples", "top_n_rag_examples_out_of_sample", "static_example", "zero_shot" )
norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors_real_table_extraction.csv") %>%
mutate(
model_name = model_name %>% str_replace("/", "_")
)
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml") | str_ends(filename, "vllm_batched.yaml")))
df_real_table_extraction <- df_real_table_extraction %>% left_join(
norm_factors_few_examples %>% mutate(model_name = gsub("^[^_]+_", "", model_name)) %>% select(model_name, parameter_count, normalization_factor),
by = c("model" = "model_name")
)
df_overview <- bind_rows(df_real_table_extraction, df_real_table_extraction_azure) %>%
filter(out_of_company != TRUE | is.na(out_of_company), n_examples <= 3) %>%
filter(model %in% model_by_size) %>%
mutate(
model = factor(model, levels = model_by_size),
method_family = factor(method_family, levels = method_order),
n_examples = fct_rev(ordered(paste("n =", n_examples)))
)
units_real_tables <- read_csv("../benchmark_truth/real_tables/table_characteristics.csv") %>% mutate(
filepath = paste0('/pvc/benchmark_truth/real_tables/', company, '__', filename),
T_EUR = (T_in_year + T_in_previous_year)>0,
T_EUR_both = (T_in_year + T_in_previous_year)>1
) %>% select(filepath, T_EUR, T_EUR_both)
df_real_table_extraction_synth <- df_real_table_extraction_synth %>% left_join(units_real_tables)
# real
zero_shot_stars <- df_real_table_extraction %>% filter(method == "zero_shot") %>% group_by(model) %>%
reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>real_table_extraction_regex_total_performance_mean, mean_num>real_table_extraction_regex_num_performance_mean, mean_F1>real_table_extraction_regex_NA_F1_mean)
static_example_stars <- df_real_table_extraction %>% filter(method == "static_example") %>% group_by(model) %>%
reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% filter(mean_total>real_table_extraction_regex_total_performance_mean, mean_num>real_table_extraction_regex_num_performance_mean, mean_F1>real_table_extraction_regex_NA_F1_mean)
underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example')) %>% group_by(model, method) %>%
reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(mean_total<real_table_extraction_regex_total_performance_mean, mean_num<real_table_extraction_regex_num_performance_mean, mean_F1<real_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)
super_underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example'), n_examples>1) %>% group_by(model, method) %>%
reframe(mean_total = mean(percentage_correct_total, na.rm = TRUE), mean_num = mean(percentage_correct_numeric, na.rm = TRUE), mean_F1 = mean(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(mean_total<real_table_extraction_regex_total_performance_mean, mean_num<real_table_extraction_regex_num_performance_mean, mean_F1<real_table_extraction_regex_NA_F1_mean)) %>% arrange(mean_total) %>% slice_head(n = 1)
zero_shot_stars %>% full_join(static_example_stars, join_by(model), suffix = c("_zero_shot", "_static_example")) %>% select(model, starts_with("mean_total")) %>%
mutate_if(
is.numeric,
~ifelse(
. == max(., na.rm = TRUE),
paste0("**", round(., 3), "**"),
round(., 3)
)
) %>%
render_table(
alignment = "lrr",
caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that perform well without or with little context learning",
ref = opts_current$get("label"), dom="t")
super_underperformer %>% select(model, method, mean_total) %>% ungroup() %>% # mutate(model = str_replace_all(model, "_", " ")) %>%
mutate_if(
is.numeric,
~ifelse(
. == max(., na.rm = TRUE),
paste0("**", round(., 3), "**"),
round(., 3)
)
) %>%
render_table(
alignment = "llr",
caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that worse than the regex baselin with 3 or 5 examples for in-context learning",
ref = opts_current$get("label"), dom="t")
df_real_table_extraction %>% group_by(model, method) %>% mutate(mean_total = mean(percentage_correct_total, na.rm = TRUE)) %>%
arrange(desc(mean_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>%
select(model_family, model, method_family, n_examples, mean_total) %>% arrange(desc(mean_total)) %>%
mutate(mean_total = round(mean_total, 3)) %>%
render_table(caption = "Comparing best mean table extraction performance with real 'Aktiva' dataset for each model family", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
df_table_extraction_regex <- readRDS("data_storage/table_extraction_regex.rds") %>%
filter(table_type != "real_tables") %>% mutate(
table_type = str_remove(table_type, "_extended")
)
real_table_extraction_regex_total_performance_median <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(percentage_correct_total) %>% median()
real_table_extraction_regex_num_performance_median <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(percentage_correct_numeric) %>% median(na.rm = TRUE)
real_table_extraction_regex_NA_F1_median <- df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(NA_F1) %>% median()
synth_table_extraction_regex_total_performance_median <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(percentage_correct_total) %>% median()
synth_table_extraction_regex_num_performance_median <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(percentage_correct_numeric) %>% median(na.rm = TRUE)
synth_table_extraction_regex_NA_F1_median <- df_table_extraction_regex %>% filter(table_type == "synth_tables", extraction_backend == "pymupdf") %>% pull(NA_F1) %>% median(na.rm = TRUE)
df_real_table_extraction <- readRDS("data_storage/real_table_extraction_extended_llm.rds") %>%
filter(!model %in% c("deepseek-ai_DeepSeek-R1-Distill-Qwen-32B", 'google_gemma-3n-E4B-it')) %>%
mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_synth <- readRDS("data_storage/real_table_extraction_extended_synth.rds") %>%
mutate(model = gsub("^[^_]+_", "", model))
df_real_table_extraction_azure <- readRDS("data_storage/real_table_extraction_extended_azure.rds") %>%
mutate(model = gsub("^[^_]+_", "", model))
model_by_size <- c(
'gemma-3-4b-it', #'gemma-3n-E4B-it',
"gemma-3-12b-it", "gemma-3-27b-it",
"Llama-3.1-8B-Instruct", "Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct",
"Llama-4-Scout-17B-16E-Instruct", "Llama-4-Maverick-17B-128E-Instruct-FP8",
"Ministral-8B-Instruct-2410", "Mistral-Small-3.1-24B-Instruct-2503",
"Mistral-Large-Instruct-2411", "Qwen2.5-0.5B-Instruct",
"Qwen2.5-1.5B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct",
"Qwen2.5-14B-Instruct", "Qwen2.5-32B-Instruct", "Qwen2.5-72B-Instruct",
"Qwen3-0.6B", "Qwen3-1.7B", "Qwen3-4B",
"Qwen3-8B", "Qwen3-14B", "Qwen3-30B-A3B-Instruct-2507", "Qwen3-32B",
"Qwen3-235B-A22B-Instruct-2507-FP8", "Qwen3-235B-A22B-Instruct-2507",
# "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1",
"Falcon3-10B-Instruct", "phi-4"
)
method_order <- c("top_n_rag_examples", "n_random_examples", "top_n_rag_examples_out_of_sample", "static_example", "zero_shot" )
norm_factors <- read_csv("../benchmark_jobs/page_identification/gpu_benchmark/runtime_factors_real_table_extraction.csv") %>%
mutate(
model_name = model_name %>% str_replace("/", "_")
)
norm_factors_few_examples <- norm_factors %>% filter((str_ends(filename, "binary.yaml") | str_ends(filename, "multi.yaml") | str_ends(filename, "vllm_batched.yaml")))
df_real_table_extraction <- df_real_table_extraction %>% left_join(
norm_factors_few_examples %>% mutate(model_name = gsub("^[^_]+_", "", model_name)) %>% select(model_name, parameter_count, normalization_factor),
by = c("model" = "model_name")
)
df_overview <- bind_rows(df_real_table_extraction, df_real_table_extraction_azure) %>%
filter(out_of_company != TRUE | is.na(out_of_company), n_examples <= 3) %>%
filter(model %in% model_by_size) %>%
mutate(
model = factor(model, levels = model_by_size),
method_family = factor(method_family, levels = method_order),
n_examples = fct_rev(ordered(paste("n =", n_examples)))
)
units_real_tables <- read_csv("../benchmark_truth/real_tables/table_characteristics.csv") %>% mutate(
filepath = paste0('/pvc/benchmark_truth/real_tables/', company, '__', filename),
T_EUR = (T_in_year + T_in_previous_year)>0,
T_EUR_both = (T_in_year + T_in_previous_year)>1
) %>% select(filepath, T_EUR, T_EUR_both)
df_real_table_extraction_synth <- df_real_table_extraction_synth %>% left_join(units_real_tables)
# real
zero_shot_stars <- df_real_table_extraction %>% filter(method == "zero_shot") %>% group_by(model) %>%
reframe(median_total = median(percentage_correct_total, na.rm = TRUE), median_num = median(percentage_correct_numeric, na.rm = TRUE), median_F1 = median(NA_F1, na.rm = TRUE)) %>% filter(median_total>real_table_extraction_regex_total_performance_median, median_num>real_table_extraction_regex_num_performance_median, median_F1>real_table_extraction_regex_NA_F1_median)
static_example_stars <- df_real_table_extraction %>% filter(method == "static_example") %>% group_by(model) %>%
reframe(median_total = median(percentage_correct_total, na.rm = TRUE), median_num = median(percentage_correct_numeric, na.rm = TRUE), median_F1 = median(NA_F1, na.rm = TRUE)) %>% filter(median_total>real_table_extraction_regex_total_performance_median, median_num>real_table_extraction_regex_num_performance_median, median_F1>real_table_extraction_regex_NA_F1_median)
underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example')) %>% group_by(model, method) %>%
reframe(median_total = median(percentage_correct_total, na.rm = TRUE), median_num = median(percentage_correct_numeric, na.rm = TRUE), median_F1 = median(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(median_total<real_table_extraction_regex_total_performance_median, median_num<real_table_extraction_regex_num_performance_median, median_F1<real_table_extraction_regex_NA_F1_median)) %>% arrange(median_total) %>% slice_head(n = 1)
super_underperformer <- df_real_table_extraction %>% filter(!method %in% c('zero_shot', 'static_example'), n_examples>1) %>% group_by(model, method) %>%
reframe(median_total = median(percentage_correct_total, na.rm = TRUE), median_num = median(percentage_correct_numeric, na.rm = TRUE), median_F1 = median(NA_F1, na.rm = TRUE)) %>% group_by(model) %>% filter(any(median_total<real_table_extraction_regex_total_performance_median, median_num<real_table_extraction_regex_num_performance_median, median_F1<real_table_extraction_regex_NA_F1_median)) %>% arrange(median_total) %>% slice_head(n = 1)
zero_shot_stars %>% full_join(static_example_stars, join_by(model), suffix = c("_zero_shot", "_static_example")) %>% select(model, starts_with("median_total")) %>%
mutate_if(
is.numeric,
~ifelse(
. == max(., na.rm = TRUE),
paste0("**", round(., 3), "**"),
round(., 3)
)
) %>%
render_table(
alignment = "lrr",
caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that perform well without or with little context learning",
ref = opts_current$get("label"), dom="t")
super_underperformer %>% select(model, method, median_total) %>% ungroup() %>% # mutate(model = str_replace_all(model, "_", " ")) %>%
mutate_if(
is.numeric,
~ifelse(
. == max(., na.rm = TRUE),
paste0("**", round(., 3), "**"),
round(., 3)
)
) %>%
render_table(
alignment = "llr",
caption = "Comparing table extraction performance with real 'Aktiva' dataset for models that perform worse than the regex baselin with 3 or 5 examples for in-context learning",
ref = opts_current$get("label"), dom="t")
df_real_table_extraction %>% group_by(model, method) %>% filter(parameter_count<17) %>% mutate(median_total = median(percentage_correct_total, na.rm = TRUE)) %>%
arrange(desc(median_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>%
select(model_family, model, method_family, n_examples, median_total) %>% arrange(desc(median_total)) %>%
mutate(median_total = round(median_total, 3)) %>%
render_table(caption = "Comparing best median table extraction performance with real 'Aktiva' dataset for each model family for models with less than 17B parameters", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
df_real_table_extraction %>% group_by(model, method) %>% mutate(median_total = median(percentage_correct_total, na.rm = TRUE)) %>%
arrange(desc(median_total)) %>% group_by(model_family) %>% slice_head(n = 1) %>%
select(model_family, model, method_family, n_examples, median_total) %>% arrange(desc(median_total)) %>%
mutate(median_total = round(median_total, 3)) %>%
render_table(caption = "Comparing best median table extraction performance with real 'Aktiva' dataset for each model family", ref = opts_current$get("label"), alignment = "lllrr", dom="t")
confidence_vs_truth <- df_real_table_extraction %>%
filter(model %in% c("Ministral-8B-Instruct-2410", "Qwen3-8B")) %>%
group_by(method, model) %>% mutate(
mean_percentage_correct_total = mean(percentage_correct_total, na.rm=TRUE), .before = 1
) %>% group_by(model) %>%
arrange(desc(mean_percentage_correct_total)) %>%
slice_max(mean_percentage_correct_total, n = 1, with_ties = TRUE) %>%
mutate(predictions_processed = map(predictions, ~{
.x %>%
select(-"_merge") %>%
mutate(
match = (year_truth == year_result) | (is.na(year_truth) & is.na(year_result)),
confidence = confidence_this_year,
truth_NA = is.na(year_truth),
predicted_NA = is.na(year_result),
.before = 4
) %>% nest(
tuple_year = c(match, confidence, truth_NA, predicted_NA)
) %>%
mutate(
confidence = confidence_previous_year,
match = (previous_year_truth == previous_year_result) | (is.na(previous_year_truth) & is.na(previous_year_result)),
truth_NA = is.na(previous_year_truth),
predicted_NA = is.na(previous_year_result),
.before = 4
) %>% nest(
tuple_previous_year = c(match, confidence, truth_NA, predicted_NA)
) %>% select(
-c(year_truth, previous_year_truth, year_result, previous_year_result,
confidence_this_year, confidence_previous_year)
) %>%
pivot_longer(-c("E1", "E2", "E3")) %>%
unnest(cols = value) %>% mutate(
match = if_else(is.na(match), FALSE, match)
)
})) %>%
unnest(predictions_processed) %>% mutate(
match = factor(match, levels = c(F, T)),
truth_NA = factor(truth_NA, levels = c(F, T))
)
confidence_vs_truth %>% ggplot() +
geom_boxplot(
aes(x = match, y = confidence, fill = truth_NA),
position = position_dodge2(preserve = "single")) +
scale_fill_discrete(drop = FALSE) +
scale_x_discrete(drop = FALSE) +
facet_grid(~ model)
confidence_vs_truth <- df_real_table_extraction %>%
filter(model %in% c("Ministral-8B-Instruct-2410", "Qwen3-8B")) %>%
group_by(method, model) %>% mutate(
median_percentage_correct_total = median(percentage_correct_total, na.rm=TRUE), .before = 1
) %>% group_by(model) %>%
arrange(desc(median_percentage_correct_total)) %>%
slice_max(median_percentage_correct_total, n = 1, with_ties = TRUE) %>%
mutate(predictions_processed = map(predictions, ~{
.x %>%
select(-"_merge") %>%
mutate(
match = (year_truth == year_result) | (is.na(year_truth) & is.na(year_result)),
confidence = confidence_this_year,
truth_NA = is.na(year_truth),
predicted_NA = is.na(year_result),
.before = 4
) %>% nest(
tuple_year = c(match, confidence, truth_NA, predicted_NA)
) %>%
mutate(
confidence = confidence_previous_year,
match = (previous_year_truth == previous_year_result) | (is.na(previous_year_truth) & is.na(previous_year_result)),
truth_NA = is.na(previous_year_truth),
predicted_NA = is.na(previous_year_result),
.before = 4
) %>% nest(
tuple_previous_year = c(match, confidence, truth_NA, predicted_NA)
) %>% select(
-c(year_truth, previous_year_truth, year_result, previous_year_result,
confidence_this_year, confidence_previous_year)
) %>%
pivot_longer(-c("E1", "E2", "E3")) %>%
unnest(cols = value) %>% mutate(
match = if_else(is.na(match), FALSE, match)
)
})) %>%
unnest(predictions_processed) %>% mutate(
match = factor(match, levels = c(F, T)),
truth_NA = factor(truth_NA, levels = c(F, T))
)
confidence_vs_truth %>% ggplot() +
geom_boxplot(
aes(x = match, y = confidence, fill = truth_NA),
position = position_dodge2(preserve = "single")) +
scale_fill_discrete(drop = FALSE) +
scale_x_discrete(drop = FALSE) +
facet_grid(~ model)
data_page_identification_regex <- readRDS("data_storage/page_identification_regex.rds")
metrics <- data_page_identification_regex$metrics
metric_summaries <- data_page_identification_regex$metric_summaries
metrics_by_company_and_type <- data_page_identification_regex$metrics_by_company_and_type
metrics_plot_regex_page_identification <- metrics %>% bind_rows() %>%
pivot_longer(
cols = -c(package, method, classification_type),
names_to = "metric",
values_to = "value"
) %>%
filter(metric %in% c(
# "acc",
"precision", "recall", "F1")) %>%
ggplot() +
geom_jitter(aes(x = method, y = value, color = package), alpha = 0.5, width = 0.2, height = 0) +
facet_grid(metric~classification_type) +
scale_x_discrete(guide = guide_axis(angle = 30)) +
theme(
legend.position = "bottom"
) +
coord_cartesian(ylim = c(0, 1))
# creating combi table
df_list <- list()
for (type in c("Aktiva", "Passiva", "GuV")) {
df_temp <- metric_summaries[type][[1]] %>%
mutate_if(
is.numeric,
~ifelse(
. == max(., na.rm = TRUE),
paste0("**", format(round(., 3), nsmall=3), "**"),
format(round(., 3), nsmall=3)
)
) %>% pivot_wider(names_from = stat, values_from = c(precision, recall, F1)) %>% mutate(
precision = paste(precision_mean, "±", precision_sd),
recall = paste(recall_mean, "±", recall_sd),
F1 = paste(F1_mean, "±", F1_sd),
) %>%
select(-ends_with("_mean")) %>% select(-ends_with("_sd")) %>% mutate_all(~str_remove(., " ± NA")) %>%
arrange(desc(method)) %>% mutate(.before = 2, type = type)
# rownames(df_temp) <- df_temp$method
df_list[[type]] <- df_temp
}
df_combined <- bind_rows(df_list) %>% bold_value_in_table()
