---
editor_options: 
  markdown: 
    wrap: none
---

# Methodology {#methodology}

\ChapFrame[Methodology][bhtblue]

This chapter describes the research design of this thesis. In the subsequent sections it elaborates

## Problem Definition

This thesis aims to evaluate a framework for information extraction from financial reports using advanced computing algorithms, such as \acr{LLM}s, presented by @liExtractingFinancialData2023. We apply this framework on German annual reports of multiple companies and focus on using open-weight \acr{LLM}s. This task requires two problems to be solved:

1.  The information to extract has to be located in the document.
2.  The information has to be extracted correct and a in format that allows further processing in down stream tasks.

We limit the information of interest on the data found in the balance sheet and profit and loss statement. Both are found on separate pages and have a table-like structure. The information extracted should reflect the hierarchy defined in @hgbHandelsgesetzbuchImBundesgesetzblatt2025. The information to extract consists of numeric values.

Since the information of interest is placed on separated pages, the first problem is to find the pages that contain the balance sheet and profit and loss statement. We do not attempt to select a specific part of the page, where the data can be found. Thus, this becomes a classification task, if a page contains the information of interest. Spatial information is not processed.

The second problem is an information extraction task. Potential information has to be identified, its entity has to be recognized and finally its numeric value has to be extracted. In this thesis no special techniques specialized on table extraction are used. We just use a plain text extract.

## Research Design & Philosophy

The research design for this thesis is set up, following the guideline found in @wohlinExperimentationSoftwareEngineering2024. Figure \@ref(fig:reasearch-design-image) shows the decisions made following this guideline. According to @collisBusinessResearchPractical2014 research classification the outcome of this thesis is applied research, focusing on solving a practical problem. Its purpose is evaluation research, comparing different approaches with each other (*benchmarking*). The data collected in our experiments is of quantitative nature and its evaluation uses (semi-)quantitative methods.

(ref:reasearch-design-caption) Showing the decisions made regarding the research design. (The figure is adapted from @wohlinDecisionmakingStructureSelecting2015. The copyright for the original figure is held by Springer Science+Business Media New York 2014.)

```{r reasearch-design-image, fig.cap="(ref:reasearch-design-caption)", echo=FALSE, out.width="100%"}
knitr::include_graphics("images/research_design.png")
```

### Research questions {#research-questions}

For this thesis we formulate two main research questions:

```{r, include=knitr::is_html_output(), results='asis', echo=FALSE}
cat("<ol class='rs-questions' style='--counter: 1;'><li>How can we use LLMs effectively to locate specific information in a financial report?</li><li>How can we use LLMs effectively to extract this information from the document?</li></ol>")
```

\begin{enumerate}[label={\textbf{Q\theenumi}}]
  \item How can we use LLMs effectively to locate specific information in a financial report?
  \item How can we use LLMs effectively to extract this information from the document?
\end{enumerate}

Each of this questions is investigated with its own methods and experiments. In the following we will use the term *page identification* to refer to the first research question and *information extraction* to refer to the second.

Additionally, we formulate a \acr{UX} motivated side research question:

```{r, include=knitr::is_html_output(), results='asis', echo=FALSE}
cat("<ol class='rs-questions' style='--counter: 3;'><li>Can we use additional information from the extraction process to guide the user on which values need to be checked and which can be trusted as they are?</li></ol>")
```

\begin{enumerate}[label={\textbf{Q\theenumi}}]
  \setcounter{enumi}{2}
  \item Can we use additional information from the extraction process to guide the user on which values need to be checked and which can be trusted as they are?
\end{enumerate}

The third question is refereed to using the term *error rate guidance*.

### Hypotheses

Subsequent we formulate our hypotheses for the research questions. Regarding to the *page identification* task we propose:

**H1.1:** \acr{LLM}s can be used to locate specific information in a financial report, achieving a high F1 score.

**H1.2:** \acr{LLM}s can be combined with other approaches to reduce the energy consumption, without lowering the systems recall.

For the *information extraction* task we define two groups of hypotheses. The first group proposes, that \acr{LLM}s can match entities, extract numeric values without mistakes and is not hallucinating if values are not present:

**H2.1a:** \acr{LLM}s can be used to correctly extract multiple numeric values from the assets table.

**H2.1b:** \acr{LLM}s can match row identifiers and place the numeric values in the correct target row.

**H2.1c:** \acr{LLM}s can identify unmatched row identifiers and report, that the value is missing.

The second group of hypotheses proposes, that there are three sources for potential predictors for the information extraction performance: model specific, prompting specific and table specific ones.

**H2.2a:** Model specific features have an effect on the extraction performance.

**H2.2.b:** Prompt strategy specific features have an effect on the extraction performance.

**H2.2c:** Table specific features have an effect on the extraction performance.

Model specific predictors are the model family and the parameter size. Predictors related to the prompting strategies are related to in-context learning decisions, e.g. how many examples should be presented, how should those examples be chosen and is it permitted to use examples from the same company as the current subjects tasks document. Example for table characteristics investigated are the number of columns, visual separation and if there is a currency unit - i.e. *T€* - given.

A list of all investigated predictors and their assumed directions of effect on multiple measures of the information extraction task can be found in the hypotheses evaluation tables in chapter \@ref(feature-effect-analysis).

The hypotheses for our side research question name the *confidence score* as concrete target for our investigations.

**H3.1:** The confidence score can be used to guide the user on which of the identified pages need to be checked and which can be trusted as they are.

**H3.2:** The confidence score can be used to guide the user on which of the predicted values in the information extraction task need to be checked and which can be trusted as they are.

### Evaluation research

We follow the process of evaluation research, in order to investigate our research questions. We compare different approaches to solve the two tasks, searching for the most effective setup, to solve the problems. A setup is considered effective if it achieves good results while being as computationally efficient as possible.

As a baseline for each task a \acrfull{regex} based approach is set up. Regular expressions are chosen as baseline because they are computationally efficient. The results are compared with the authors human performance as well. The results will be used to implement an application that is used by the employees of \acr{RHvB} in future.

## Evaluation Strategy

This section defines, under which conditions we consider a task to be successfully solved and which metric we use to measure the outcome.

### Evaluation framework and metrics

#### Page identification

The page identification task is successful, if a page is correctly classified to contain the information of interest. The balance sheet is composed of the assets (*Aktiva*) and liabilities (*Passiva*) table. Together with the profit an loss statement (*Gewinn- und Verlustrechnung, GuV*) they form the three target classes. The fourth class is called *other*. Subsequently will will use the German terms for the target classes (or table types): **Aktiva**, **Passiva** and **GuV**.

::: paragraph-start
##### Metrics

The distribution of target classes and pages of type *other* is highly imbalanced. At most two pages per target class are found in documents with up to 152 pages. Thus, following @saitoPrecisionRecallPlotMore2015 suggestion, we report measures as precision, recall and F1 score instead of accuracy, to describe the approaches performances.
:::

In a \acr{HITL} application the recall value might be of higher interest than the F1 score. More precisely, in those cases the number of pages to check until the correct page is found is of interest. Thus, the top k recall is reported additionally, if the approach permits to rank the classified pages according to a score.

An alternative measure for the F1 score would be the \acr{AUC} score precision-recall curve.

#### Information extraction

The information extraction task if successful, if the correct numeric value is extracted with the correct entity identifier in the correct \acr{json} format. If a value, defined by the legal text, is not present *null* should be returned with the corresponding entity identifier. The entity identifier can be composed of up to three labels, representing the hierarchy defined in the legal text.

::: paragraph-start
##### Metrics

We use two measures to describe the approaches performances for the information extraction task. First, we check how many of the predicted numeric values are matching the numeric values in the ground truth. The only permitted differences are based on the number of trailing zeros. We do not check for partial correctness, since the real life application requires totally correct extracted numbers.
:::

Second, we report the F1 score for correctly predicting values as missing and thus returning *null*. The distribution of missing values and given numeric values is not imbalanced. Nevertheless, we report the F1 score to establish a comparability with the results of the page identification task.

#### Error rate guidance

An error rate guided result checking process can be implemented, if we can use extraction task related information, to identify a prediction trust worthy. This means, we could white list these values and red flag the remaining ones. Thus, we could guide the users attention in the error checking process on those values, that empirically tend to have a high chance to be faulty.

::: paragraph-start
##### Metrics

In this thesis we focus our attention on a criteria, which we name *confidence*. We calculate the *confidence* score for answers received from \acr{LLM} based on the non-normalized sum of token log probabilities [@boseakEvaluatingLogLikelihoodConfidence2025]:
:::

\begin{equation} 
confidence = \exp \left( \sum logprob(token_i) \right)
(\#eq:confidence)
\end{equation}

For the classification tasks this is equal to the normalized (averaged) sum, since the answer is either containing just one token or the subsequent tokens have a log probability of 0, because the answer is fully determined by the first token.

We are using the non-normalized sum of token log probabilities for the information extraction task too, because we want a single uncertain digit to flag the whole numeric value as unreliable. This means, that shorter answers tend to have higher *confidence* scores. This is especially true for predicting *null*. Thus, we investigate the prediction of numeric values and *null* separated.

We group predictions based on their confidence scores in intervals with a range of 0.05 and calculate the empirical error rate for each interval. We hope to find a error rate (close to) zero for the highest confidence intervals and a noticeable proportion of predictions falling into those intervals.

### Benchmarking

Comparing the performance of different approaches benchmarked in this thesis is possible, because the approaches within a task are performed on a common document base. The task to solve is the same for each approach. The prompts for the different prompting strategies are build systematically and derive from the base prompt formulated for the *zero shot* strategy. Comparing the runtime or energy consumption gets possible with the \acr{GPU} benchmark data (see section \@ref(gpu-benchmark))

## Data Strategy

The population of annual reports of interest for the work at the \acr{RHvB} is composed of all annual reports of companies, where the state of Berlin holds a share. There are often multiple versions of those annual reports: one that is publicly available and targeting share- and stakeholders. The structure and layout of these reports is quite heterogeneous. Often there is a second version that is used internally or for communications with public administrations. They often consist of plain text and tables and show neither diagrams nor photos.

Since the evaluations are run on the \acr{BHT} cluster and partially in the Azure cloud, we work with the publicly available reports, while at \acr{RHvB} the internal documents are more common. The annual reports mostly are downloaded from the companies websites. Some documents are accessed via [Bundesanzeiger](https://www.bundesanzeiger.de) or the [digitale Landesbibliothek Berlin](https://digital.zlb.de).

For the page identification task all kinds of pages from the annual reports are used. For the information extraction only pages with **Aktiva** tables are used. In addition, a set of self-generated synthetic **Aktiva** tables is used for the information extraction task. It is created to systematically investigate potential effects of characteristics financial tables could have.

### Sampling methodology

The reports are selected with the goal of reflecting the heterogeneity of the population of documents within the chosen sample.

::: paragraph-start
##### Page identification

For the page identification task companies from different fields of business are selected and all publicly annual reports downloaded. The companies chosen are found in the first row of Figure \@ref(fig:beteiligungsunternehmen).
:::

The assumption is, that each company within a branch can represent the document style for other representants of the same branch and that the position within a column has no implications for the annual reports styles. Realizing that the *degewo AG* reports would require \acr{OCR} preprocessing we additionally downloaded reports for *GESOBAU AG*. A description of the resulting data set can be found in section \@ref().

::: paragraph-start
##### Information extraction

For the information retrieval task the publicly available reports for all companies are downloaded. A amount of documents chosen for each company is more balanced than for the first task. A description of the resulting data set can be found in section \@ref().
:::

We excluded all companies, that do not report their assets table in the most detailed fashion defined by the legal text, because these asset tables do not match with the strict schema we use. This decision excludes the reports of well known companies as BVG and BSR.

::: paragraph-start
##### Error rate guidiance

We investigate the third research question on the two datasets described above, instead of creating a separate dataset.
:::

### Ground truth creation process

The ground truth for both tasks is created by manual annotation through the authors. The results of early experiments are used to check the ground truth for mistakes or missed items. Found issues get resolved. Details on the ground truth creation processes can be found in section \@ref()

### Preprocessing

We use plain text extracted from the annual reports for all tasks. We do not extract geometric coordinates for the text. @auerDoclingTechnicalReport2024 describes, that available open-source PDF parsing libraries may show issues as poor extraction speed or randomly merged text cells. We tested five PDF extraction libraries, because the results of all subsequent experiments will depend on the text extracts. Section \@ref(text-extraction-benchmark) shows the results.

We perform no manual data cleaning, because this will not be done from the employees of \acr{RHvB} either.

### Data splitting

When we train a machine learning model, we split the data into train and a test set. We do not use a validation set, because we do not compare models using an extended hyper-parameter variation strategy. Instead we just report the performance found for the models build with default settings. We build two random forests for the term frequency approach in the page identification task and more random forests for evaluating the hypotheses for the information extraction task.

Building the term frequency random forest, we face a highly imbalanced dataset. We apply undersampling for the training and evaluate the model on the imbalanced test set.

## Experimental Framework

### LLM overview

```{r llm-overview-data-loading, echo=echo_flag, warning=warning_flag, message=message_flag}
df_llm_overview <- read_csv("data_storage/model_usage_extraction.csv") %>% 
  arrange(tolower(model_family), parameter_count) %>% 
  rename(parameter = parameter_count) %>% 
  mutate_if(is.character, ~if_else(is.na(.), "", .)) %>% 
  mutate_if(is.character, ~if_else(. == "X", "✓", .))
```

Table \@ref(tab:llm-overview) gives an overview on all \acr{LLM}s used for the tasks in this thesis. It shows the passive parameter count in billions for each \acr{LLM} and shows in which specific approach it is used with a tick. Overall `r length(unique(df_llm_overview$model))` models from `r length(unique(df_llm_overview$model_family))` model families are used. If available, we use a instruction fine tuned version of the models.

```{r llm-overview, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
table <- df_llm_overview %>% 
  render_table(
    alignment="lrccccc", 
    caption="Overview of benchmarked LLMs for all tasks. Parameter shows passive parametercount in billions.", 
    ref = opts_current$get("label"),
    colgroups = c(" " = 2, "information extraction" = 3, "page identification" = 2),
    row_group_col = 1,
    force_kable = FALSE
    )
if(knitr::is_latex_output()) {
  table <- table %>% column_spec(1, width = "5.8cm")
}

table
```

### Approaches

#### Page identification

::: paragraph-start
##### Regular expressions

We develop multiple sets of regular expressions and filter out all pages that do not fulfill all regular expressions of a given set. There are different sets for each target type, **Aktiva**, **Passiva** and **GuV**. The sets also differ in how versatile they can cope with additional white space introduced by a imperfect text extraction and how many different words for a given term are accepted. Figure \@ref(fig:regex-filter) shows an example for two sets of regular expressions to identify a **Aktiva** page.
:::

```{r regex-filter, fig.cap="Comparing the prediction of two different sets of regular expressions on dummy pages. The simple one has a lower recall, while the  expended one has a lower precision.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/regex_filtering.png")
```

::: paragraph-start
##### Table of Contents Understanding

We use a \acr{LLM} to extract the \acr{TOC} from the first pages from a document or use the embedded \acr{TOC} and prompt a \acr{LLM} to identify the pages where the **Aktiva**, **Passiva** and **GuV** are located. Figure \@ref(fig:toc-screenshot) shows a screenshot of a annual report with an embedded TOC and its TOC in text form.
:::

```{r toc-screenshot, fig.cap="Showing a screenshot of a annual report with an embedded TOC (left) and its TOC in text form (right). The embeded TOC is not listing all entries from the TOC in text form.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/toc.png")
```

::: paragraph-start
##### Large Language Model Classification

We use \acr{LLM}s to classify if the text extract of a given page is containing a **Aktiva**, **Passiva** or **GuV** table or something else. We test binary classification and a multi-classification approach. The reported confidence scores can be used to form a ranking, which text extract might be most similar to the target type. We use a sampling temperature of zero, because we neither want creative nor variable answers, but just the most probable class and exact copied numbers.
:::

We test a wide range of open-weight models and compare different prompting techniques. Figure \@ref(fig:prompt-setup-classification) shows, how the prompts are composed for the different strategies. Besides a zero shot approach we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing the legal text instead of examples from a annual report.

```{r prompt-setup-classification, fig.cap="Showing the basic structure of the prompts and which strategies are used to pass additional information to the LLM.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/promt_building_classification.png")
```

::: paragraph-start
##### Term frequency Ranking

We use normalized term frequencies and normalized float frequency to as features for a classification using a random forest. The predicted scores are used to build a ranking, which page most probably contains the target pages. Undersampling is used during training, to handle the unbalanced data. Figure \@ref(fig:tf-flowchart) visualizes, how the prediction works in this approach.
:::

```{r tf-flowchart, fig.cap="Visualizing, how term and float frequency get calculated and used to predict, if a page is of the target class.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/tf_flowchart.png")
```

#### Information extraction

::: paragraph-start
##### Regular expressions

We use regular expressions to extract the numeric values for matching row identifiers. The regular expressions handle line breaks between words in the row identifiers, but not within a word. They can handle multiple signs of white space. Besides that, they try to fully match the labels from the legal text with the text extract, ignoring upper case. They extract numbers with "." as thousands separator. Figure \@ref(fig:regex-extractor) is visualizing those capabilities.
:::

```{r regex-extractor, fig.cap="Visualizing the extraction results for different text examples. Texts in green boxes are matching our regular expression. Texts in red boxes do not, because of the red text part.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/regex_extracting.png")
```

::: paragraph-start
##### Real tables

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation. We use a sampling temperature of zero, because we neither want creative nor variable answers, but just the most probable class and exact copied numbers.
:::

The \acr{LLM} has to group row identifiers and corresponding numeric values and match the row identifier with the labels of the schema. If a row identifier is unknown, the values have to be discarded. If a label is not present among the row identifiers, the model predicts *null*. All values are extracted in one pass. We do not include any instruction, how to proceed with currency units, that might be given for certain columns.

We test a wide range of open-weight models and compare different prompting techniques. Figure \@ref(fig:prompt-setup-extraction) shows, how the prompts are composed for the different strategies. Besides a zero shot approach we test few-shot in-context learning with examples that are either chosen randomly or retrieved based on their vector similarity. Finally, we test passing a synthetic **Aktiva** table as example. We test models from OpenAIs GPT family in addition to the open-weight models.

```{r prompt-setup-extraction, fig.cap="Showing the basic structure of the prompts and which strategies are used to pass additional information to the LLM for the information extraction task.", echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/promt_building_extraction.png")
```

::: paragraph-start
##### Synthetic tables

We use \acr{LLM}s to extract the numeric values of synthetic **Aktiva** tables with restricted generation. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

::: paragraph-start
##### Hybrid approach

We use \acr{LLM}s to extract the numeric values of real **Aktiva** tables with restricted generation, providing examples from synthetic **Aktiva** tables. The procedure is identical as with the real **Aktiva** tables. We extract all values with and without an explicit instruction on how to proceed with currency units. We limit our test on the open-weight models.
:::

### Error analysis

To better understand the limitations of the evaluated models and find ways to further improve the system, we will conduct a detailed error analysis. Representative error cases will be documented to illustrate typical failure modes and to inform potential improvements for future work.

#### Page identificaion

Content rot: We will first quantify the types of misclassifications using confusion matrices and error rate statistics.

TF: Additionally, we will manually inspect a sample of erroneous predictions to identify common causes, such as ambiguous table layouts, or model misinterpretations.

[@grandiniMetricsMultiClassClassification2020]

#### Inforamtion extraction

Under construction!

99.5 % or 96 % accuracy for extracting financial data from Annual Comprehensive Financial Reports [@liExtractingFinancialData2023] In the untabulated test, GPT-4 achieved an average accuracy rate of 96.8%, and Claude 2 achieved 93.7%. Gemini had the lowest accuracy rate at 69%. (ebd.)

found error types: including omissions when the LLM was instructed to extract a list of line items, misjudgment of units (such as thousands or millions), and incorrect identification of rows and columns

failed to extract all of list, Too many hallucinated values when it was NA instead [@goughertyTestingReliabilityAIbased2024]

For the information extraction task, we expect to find issues with wrong extracted numeric values due to disrespecting currency units or hallucinated numbers, if a value is absent. We further expect wrong entity recognition and thus wrong row identification matching.

Error rates will be compared among different experimental setups to reveal systematic weaknesses in a stratified analysis.

Finally, we investigate some of the erroneous extracted examples manually, and try to identify the underlying issues.

Tools and criteria

Reporting

Example:

Numeric values are difficult to handle for langauge models in specific tasks. Copying numbers seems not to be a recent problem. How about transforming by multiplication with 1000?

### Evaluation methods

We use a lot of visual representations for the interpretation of the results. Many of those can be found in the technical report sections of the appendix. Bigger graphics with a lot of small multiples can be found in the appendix \@ref(figure-collection). These visuals have a high information density and enable us to compare a lot of results - including distributional information - at once. A 43" screen with 4k resolution is important for this process.

We prefer this process over comparing many rows with measures of location (e.g. mean and median) and measures of distribution shapes (e.g. standard deviation, median absolute deviation, skewness). For presenting the results in chapter \@ref(results) we compress a selection of results in tabular form, because it is easier to grasp. Some interesting observations are discussed in chapter \@ref(discussion). But a lot of information is still kept in the visuals.

::: paragraph-start
##### Boxplots

We use a lot of boxplots to compare the distributions of sample of results. We also inspect violin plots or add a point jitter to the graphics during the inspection, to check for multi-modality. If we find intersting details, we keep those for the presented graphics. If not, we just present the boxplots.
:::

::: paragraph-start
##### SHAP

For interpreting the \acr{SHAP} values, we use bar plots showing the mean absolute shapley values as feature importance, which can be interpreted as an indicator of effect strength. Furthermore, we use bee swarm plots (also called summary plot) and dependency plots, to investigate if we can name a direction for the effect and identify interesting interaction patterns. Further information can be found in chapter 18 of the book "[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/shap.html)" [@molnarInterpretableMachineLearning2025].
:::

::: paragraph-start
##### random forest

We build a random forest classifier in the term frequency approach for the page identification task, to evaluate our hypotheses **H2.2**. We perform no hyper parameter optimization. We want to get a first glimpse on what might influence the extraction instead of getting the best possible performance. We are not modelling causal relationships at all.
:::

We do not use XGBoost for the final analysis, because calculating the \acr{SHAP} values for XGBoost model took to long. Linear and logistic regression models are fitted as well, but not used for the final analysis.
