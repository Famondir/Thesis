---
editor_options: 
  markdown: 
    wrap: none
---

# Discussion {#discussion}

\ChapFrame[Discussion][bhtgray]

## Page identification

### General performance

::: paragraph-start
##### Results

The page identification task is solved with higher F1 scores for every target class than the human reference F1 score. It is solved completely on the created dataset for predicting the class **Aktiva**. In two cases the multi-class classification wit Llama 4 Scout is best. For classifying **GuV** the binary classification with Ministral is even better.
:::

::: paragraph-start
##### Interpretations

To get the best results, a combination of two \acr{LLM}s would be necessary. A more general approach is using Llama 4 Scout for multi-class classification. If there is little VRAM Ministral-8B also does a decent job in multi-class classifiacation.
:::

::: paragraph-start
##### Compare with previous work

We are able to narrow down the page range to five pages without using a \acr{LLM}. With the \acr{LLM} we are guaranteed, to find the correct pages within a range of two pages. Most of the time the first page in the \acr{LLM} ranking is the correct one. @liExtractingFinancialData2023 do not present a concrete number of pages, they have to process after page refinement. The \acr{TOC} does not work as well as expected from their report.
:::

::: paragraph-start
##### Implications

Introducing new areas of application should be easily possible and manageable even from a regular user. For the term frequency approach we can set up a pipeline, where the user just has to enter a list of keywords and then he gets presented a page ranking, based on \acr{TF-IDF} values. The user might adjust the key word list or select correct pages to build a ground truth. If there are more measures of interest (e.g. a float frequency as well) we can automatically train a random forest classifier as well.
:::

Another approach is, that the user provides documents and a list on which page the information of interest is located. This can be the base for a retrieval augmented few-shot classifier, that will improve in the process of classifying more pages.

::: paragraph-start
##### Limitations

The term frequency and \acr{LLM} classification might perform worse, if the information searched for, is just making up a small part of the pages content. If the information is in a table we can use a visual table detection model, to identify all tables. Section \@ref(yolo) shows that this is a promising approach. Then we can use the retrieval augmantag few-shot approach to identify, which table is the correct one.
:::

If the information is not even in a table, but part of a regular sentence, it might get difficult to find the correct page with this approach. Maybe the \acr{TOC} approach could be used for page range refinement in this case.

::: paragraph-start
##### Unexpected results

-   Ministral is performing unexpected well. Is performs better, than other Mistral models, that are newer and have more parameters.
-   \acr{TOC} approach performs not as well as expected by @liExtractingFinancialData2023 results.
:::

::: paragraph-start
##### Recommendations

We recommend, if possible, to refine the page range, using a term frequency approach. Afterwards a \acr{LLM} can be used to perform a multi-class classification those pages. Use the page with the highest score for the information extraction. Keep the ranking.
:::

Do not include a obligatory step, to confirm the selected page, but start the information extraction right away. When the user is checking the results, a wrong page will be noticed immedeatly. Then other pages can be inspected manually, following the order in the ranking.

Save the examples already classified in a vector database and use those in future tasks. Include documents from the same company. Build the database document by document in the beginning, before starting with batch wise processing.

::: paragraph-start
##### Possible improvement

-   Instead of the simple term frequency the \acr{TF-IDF} measure could be implemented.

-   With more expertise and few shot learning the \acr{TOC} approach could perform better.
:::

::: paragraph-start
##### Conclusion


:::

### Energy usage and runtime

The fastest and least energy consuming strategy, using only \acr{LLM}s, is to use a small model as Ministral-8B-Instruct for the multi-class approach. This is more effective than running three binary classifications.

An alternative approach could be to binary predict if the page is of any target type and then perform a classification, which type exactly the page is of. But this would probably consume as much energy as the multi-class approach, because we have to provide a balanced amount of examples for each class. The results of the multi-class strategy are good enough  to run it right away.

In both strategies the k required for perfect recall is three, using the Ministral-8B-Instruct model[^06_discussion-1].

[^06_discussion-1]: Potentially smaller fine tuned models can solve the task even more efficient.

Nevertheless, it is more promising, to reduce the number of pages, to classify with the \acr{LLM} in the first place. This can be achieved, by running the term-frequency approach first to refine the page range, and then use the \acr{LLM} approach.

::: paragraph-start
###### Compare with manual page identification

The manual approach is the slowest. We identified the pages of interest for all target classes in ten random documents for the benchmark. We used the \acr{TOC} and the search function to find key words like **Aktiva** or **Bilanz**. Anyhow, its almost as fast as the multi-classification using Llama 4 Scout, while consuming eight times less energy. Comparing it to Ministral-8B-Instruct it take three times longer but consumes less then half of the energy.
:::

Not taken into account fo this comparison are factors as:

-   costs to buy and maintain hardware (i.e. a GPU cluster).

-   higher costs per runtime if the \acr{LLM} compute is purchased from cloud providers. CLOUD: price if LLM is in the cloud \<-- print tokens used

    -   four classes (3 random examples): 11 k tokens

    -   binary (3 random examples): 6.5 k tokens

-   payment and insurance to pay for a human (e.g . student coworker).

-   the training time and energy consumption for training either

    -   a \acr{LLM} (probably done by the \acr{LLM} provider).

    -   a human (growing up, getting educated).

-   the energy consumed with the food humans eat.

## Information extraction

### General performance

::: paragraph-start
##### Results

The best performing model - Qwen3-235B-A22B-Instruct - almost reaches human performance on real **Aktiva** tables, but is much faster. Both measures, percentage of correct numeric predictions (98.0 %) and F1 score (98.1 %) are not perfect yet and could be improved. It achieves perfect results on synthetic tables provided in \acr{HTML} format.
:::

::: paragraph-start
##### Interpretations

The strong performance observed means, that Qwen3-235B is performing numeric transformations, respecting the currency units, in many cases. Otherwise the upper limit for correct numeric extraction would be 80.8 %, since 19.2 % of all numeric values have *Tâ‚¬* as unit.
:::

Furthermore, we can see that it is possible to achieve perfect output, if the input is perfect structured and without unknown row identifiers. Perfect in, perfect out. Thus, we show that there is no \acr{LLM} approach inherent mechanism, that prevents perfect information extraction of numeric values.

The 0.1 % incorrect predictions on synthetic tables from the PDF documents could be caused by faulty text extracts by *pdfium*. But the Markdown input is without any flaws and resulted in 0.1 % errors as well.

investigate F1 score

::: paragraph-start
##### Compare with previous work

@liExtractingFinancialData2023 achieve a perfect extraction result, after refining their approach, extracting a total of 152 data points from 8 ACFRs reports. Before the prompt adjustments they find 96.1 % correct extracted datapoints.

They included an instruction how to handle missing values but do not report any remarkably higher error rates with missing values.



(probably homogeneous) fuzzy matching instruction 3 values per go, instructions for currency units

98.9 % on ESG reports after refinement, 90 values from 15 documents; heterogeneou (multiple ccompanies)? started at 93.3 % misjudging units between grams, kilograms, and tonnes introduced additional fields to the data points and had LLM extract the units as separate output fields

small in sample extractions =\> 4000 county year ACFRs: 96 % (80_000 data points)
:::

::: paragraph-start
##### Implications

We have not investigated yet, if the 2 % wrong extracted numeric values are caused by not respecting currency units, or if there is another reason. A potential reason may be numeric values, that get stiched together during text extraction.

-   invest more into table extraction?

Checked other extraction libraries as well. Including Azure Document Intelligence and Dpcling, but results not better. In Markdown they get worse.

Checking the extracted values takes up to three minutes. This totals in 300 minutes prediction checking. Thus, selecting a smaller model that is finishing after 2:30 minutes is not speeding up the process a lot. Once we get a sufficient good performance with the big models the prediction checking can be dropped. This would bring th real benefit.
:::

::: paragraph-start
##### Limitations

-   add schemas for different hierarchies
-   test guided decoding instead of restricted (open a new list for unmatched entries?)
:::

::: paragraph-start
##### Unexpected results

-   converts currency units without being prompted explicitly
:::

::: paragraph-start
##### Recommendations

-   invest in supply of machine-readable information and good user experience, instead of 100 % perfect extraction

- building a document extraction database document by document can improve performance taking advantage of same-company rag in-context learning
:::

::: paragraph-start
##### Possible improvement

The extraction performance may get higher, if the in-context learning examples show how to deal with columns that have a currency unit.

-   explicitly instruct, how to handle currency units

-   extend schema or sum rows

-   handle non matched rows explicitly
:::

::: paragraph-start
##### Conclusion
:::

* check if missing value in one col (hand full of those cases) is resulting in hallucinations
* confusion matrix

### Company specific results

predictions for barrierefreie documents of WBM empty, one time because the pages showed **GuV**; also no predictions for Zoo 2024 **Passiva**

WBM und Kassenlotterie komische Textsatzreihenfolge, aber kein negativer Effekt auf text extract

### Error analysis

-   new lines / splitted lines
-   test synthetic hypothesis with pymupdf extract
-   2.4 % wrong gold standard creation

-   errors from wrong formatted numbers
-   errors from wrong / unclear entity mapping

#### Ground truth creation

During the second pass of the ground truth creation we find, that 2.4 % of the values differ among the previously created gold standard and the results of the second pass. In the first pass the values are copied manually, while the results of the second pass are \acr{LLM}s predictions, that we double checked. We find 75 values differing in the 24 documents that are part of both data collections. Table \@ref(tab:error-analysis-gold-standard-extraction) shows the nature of errors and their counts. Most errors are distributed in the *omission* classes.

Errors of this type result from an inconsistent coding process. In one pass a value might have been included, while it is not included in the other pass. Or the value is matched to different row identifiers during the two passes. To resolve this kind of errors a strict and detailed coding manual is necessary. Additionally, the coding should be done from experts of the field instead of the data scientist.

```{r error-analysis-gold-standard-extraction, echo=echo_flag, warning=warning_flag, message=message_flag, results='asis'}
tribble(
  ~error_type, ~count,
  "ommited in first pass", 29,
  "ommited in second pass", 20,
  "multiple differences", 13,
  "missing digit", 10,
  "swapped digits", 2,
  "comma instead of dot", 1,
) %>% render_table(
  alignment = "lr",
  caption = "Showing the nature of errors and their counts. Errors with multiple difference have Levenshtein distance greater one.",
  ref = opts_current$get("label"),
  dom = "t"
)
```

#### Regular expression approach {#regex-synth-backend-discussion}

```{r table-extraction-regex-data-loading-discussion, echo=FALSE, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum('data_storage/table_extraction_regex.rds')}
df_table_extraction_regex <- readRDS("data_storage/table_extraction_regex.rds") %>% 
  filter(table_type != "real_tables") %>% mutate(
    table_type = str_remove(table_type, "_extended")
  )
```

We find, that the regular expression approaches performance on the synthetic dataset is highly influenced by the extraction library used. For the real data we find no difference. Figure \@ref(fig:table-extraction-regex-performance) B shows, that the \acr{regex} approach on text extracted with *pdfium* especially has a wider precision range. The number of wrong extracted numeric values is a little as well.

A reason for this might be incorrect extracted texts. We find, that there are issues with missing (or additional) white space, misplaced line breaks and an extraction of the text column first, followed by the numeric columns. You can find examples for those types of incorrect extracted texts by three different PDF extraction libraries in section \@ref(regex-extraction-mistakes). One of the examples is found with the real table dataset, while two are found with the synthetic tables.

The random white space and line breaks could be handles by adjusting the regular expression for the label matching. Missing white space between numeric values could also be handled by adjusting the \acr{regex}. The misplaced columns would need more advanced reprocessing strategies. But all those error ypes shouel influence the recall and not the precision.

A possible explanation for a spread in precision could be the duplicated row identifier *Geleistete Anzahlungen*. Our simple approach matches this row always with the first occurrence. This means, if the second occurrence is given in the ground truth, but not the first one, our approach would create a false positive result. The percentage of false positive example is then determining the precision value and is linked to the number of total rows in the ground truth.

A reason for wrong numeric values are rows, where the summed value is given in the next column, but same row, as the individual values. In this case the approach selects one individual value and a sum, instead of two individual values.

## Error rate guidance

::: paragraph-start
##### Results

Confidence score can be used to determine empirical error rate for confidence intervals. For well performing models we find most predictions in the highest confidence interval.

Ministral shows good spread over confidence range for page identification task. We can find confidence intervals with zero error rate for the page identification task and the information instruction task on synthetic data.

We do not find confidence intervals with a error rate below 1 % for real **Aktiva** tables. Explicitly instructing to respect currency units, reduces errors.
:::

::: paragraph-start
##### Interpretations

Confidence score can be used to guide attention for page identification task, but hardly for information extraction task.
:::

::: paragraph-start
##### Compare with previous work
:::

::: paragraph-start
##### Implications

Additional segementation could be necessary, to find values that are predicted well enough.

confidence intervals based on company (know which formats are tricky)
:::

::: paragraph-start
##### Limitations


:::

::: paragraph-start
##### Unexpected results

Most models predict high confidence, even for wrong predictions.
:::

::: paragraph-start
##### Recommendations

0 % empirical error rate unstable. Chose cutoff or show error rate continuously?
:::

::: paragraph-start
##### Possible improvement

Testing normalized confidence.
:::

::: paragraph-start
##### Conclusion
:::

Discussion:

An additional feature to narrow down the selection and get a more concrete error rate scores for similar texts could be the the company.

Check perfect text?

Learning benefit of real examples higher for numeric value extraction as for lable matching.

works only well for page identification, where it is not really needed (perfect results, low checks, implicit found in extraction window)

additional segmentation might help

HTML might help =\> document parsing and table extraction

## Feature effect analysis

### General performance

::: paragraph-start
##### Results
:::

::: paragraph-start
##### Interpretations
:::

::: paragraph-start
##### Compare with previous work
:::

::: paragraph-start
##### Implications
:::

::: paragraph-start
##### Limitations
:::

::: paragraph-start
##### Unexpected results
:::

::: paragraph-start
##### Recommendations
:::

::: paragraph-start
##### Possible improvement
:::

::: paragraph-start
##### Conclusion
:::

## Summary

```{r hitl-error-handling, fig.cap="Showing the information extraction process in a HITL application. We propose to include user action only after the information extraction. If a wrong page is selected, this can be fixed and extraction runs again. Wrong extracted values and handling unknown row identifiers should be done in one place.", echo=FALSE, out.width="100%", fig.align='center'}

knitr::include_graphics("images/HITL_flowchart_error_handling.png")
```

## Limitations

#### Regex baseline

-   synthetic tables have been generated with cell lines because this should have improved the performance of a table extraction approach (not conducted)- maybe this is confusing pdfium? Or the zoom level?

### classification

-   Qwen 2.5 hat zweiseitige GuV von IBB entdeckt und zur Anpassung der Ground Truth
-   predictor: n_big_tables (tf or llm relevant?)
-   Why it is important to have a good recall (or top n accuracy)
-   bad performance for Maverick with more models relied to FP8 model version? No. Same reults with FP16

One could build an application that is not asking for a human intervention for reported confidences over 0.9 and then give the possibility to change the page to extract information from later on.

For humans: Easily identifiable if page has a big table with numbers but not so easy to spot the Aktiva / Passiva label.

## Not covered

-   OCR
-   fine-tuning
-   using something smaller (e.g. LSTMs) instead LLMs
-   building application, UX design (ref Ambacher 2024)
-   table extraction (either VLLMs (visual) or classic approaches \<-- tried tabula but was not successful (because of missing visual traits)?) to prevent wrong text flow and have clear cell borders
-   classification oriented models with softmax

in company document next / previous year more helpful than years further away?

### Table detection / extraction

Can be used to narrow down set of possible pages

Can be used to focus only on the table content (measure if correct area was identified would be necessary)

Vision model as baseline

## Outlook

-   ensemble from multiple models or are errors systematic? (e.g. Wohnungsbaugenossenschaften splitting some rows in multiple and none is picked?)
-   check for halluzination vs wrong placed / repeated numbers
-   no perfect score even with synthetic data
-   flexible extraction (name something, find it, get it)
-   UI
    -   checking results / correct errors; col by col; match entities
    -   add unused entries (backlog? extra table?)
    -   possibilities for rerun / flagging the source of issue
-   ml health check / benchmark framework
    -   test new models performance
    -   check if new examples might be harmful (repredicting)

Ad-hocs for monitoring during the year

