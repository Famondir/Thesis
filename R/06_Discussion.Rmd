# Discussion {#discussion}

\ChapFrame

## Interpretations

#### Table extraction

##### Regular expression approach

```{r table-extraction-regex-data-loading-discussion, echo=FALSE, warning=warning_flag, message=message_flag, cache=TRUE, cache.extra = tools::md5sum('data_storage/table_extraction_regex.rds')}
df_table_extraction_regex <- readRDS("data_storage/table_extraction_regex.rds") %>% 
  filter(table_type != "real_tables") %>% mutate(
    table_type = str_remove(table_type, "_extended")
  )
```


Some possible explanations for the different performance on the extracted texts are:

-   a duplicated row name[^06_discussion-1]
-   numeric columns extracted separated from row names by extraction libraries
-   sums in the same row as the single values[^06_discussion-2]
-   with pdfium: missing white space[^06_discussion-3]
-   with pdfium: random line breaks[^06_discussion-4]

[^06_discussion-1]: The row *Geleistete Anzahlungen* can be found in two parts of the table and the simple approach just matches the numbers to the first found entry.

[^06_discussion-2]: In this case the \acr{regex} takes the sum as the value for the previous year.

[^06_discussion-3]: This can form unexpected numeric patterns or prevent the row names to be recognized.

[^06_discussion-4]: The approach takes care of line breaks between words, but not within. This leads to unrecognized row names as well.

You can find some examples for incorrect extracted texts in section \@ref(regex-extraction-mistakes).

The random line breaks result in some missed row names which is reflected by the bigger spread for NA precision with *pdfium* on the synthetic dataset (see Figure \@ref(fig:table-extraction-regex-performance) B). Nevertheless, the NA precision for the majority of the cases is perfect. This is different with the real dataset. The NA precision is found to be at only `r df_table_extraction_regex %>% filter(table_type == "real_tables") %>% pull(NA_precision) %>% mean() %>% round(2)`.

## Limitations

### table extraction

-   found mistakes in gold standard with the llm results; mistakes found by human double check
-   new lines / splitted lines
-   test synthetic hypothesis with pymupdf extract
-   2.4 % wrong gold standard creation
-   confidence intervals based on company (know which formats are tricky)

#### Regex baseline

-   synthetic tables have been generated with cell lines because this should have improved the performance of a table extraction approach (not conducted)- maybe this is confusing pdfium? Or the zoom level?

### classification

-   Qwen 2.5 hat zweiseitige GuV von IBB entdeckt und zur Anpassung der Ground Truth
-   predictor: n_big_tables (tf or llm relevant?)
-   Why it is important to have a good recall (or top n accuracy)
-   bad performance for Maverick with more models relied to FP8 model version? No. Same reults with FP16

One could build an application that is not asking for a human intervention for reported confidences over 0.9 and then give the possibility to change the page to extract information from later on.

For humans: Easily identifiable if page has a big table with numbers but not so easy to spot the Aktiva / Passiva label.

## Not covered

-   OCR
-   fine-tuning
-   using something smaller (e.g. LSTMs) instead LLMs
-   building application, UX design (ref Ambacher 2024)
-   table extraction (either VLLMs (visual) or classic approaches \<-- tried tabula but was not successful (because of missing visual traits)?) to prevent wrong text flow and have clear cell borders
-   classification oriented models with softmax

in company document next / previous year more helpful than years further away?

### Table detection / extraction

Can be used to narrow down set of possible pages

Can be used to focus only on the table content (measure if correct area was identified would be necessary)

Vision model as baseline

## Outlook

-   ensemble from multiple models or are errors systematic? (e.g. Wohnungsbaugenossenschaften splitting some rows in multiple and none is picked?)
-   check for halluzination vs wrong placed / repeated numbers
-   no perfect score even with synthetic data
-   flexible extraction (name something, find it, get it)
-   UI
    -   checking results / correct errors; col by col; match entities
    -   add unused entries (backlog? extra table?)
    -   possibilities for rerun / flagging the source of issue
-   ml health check / benchmark framework
    -   test new models performance
    -   check if new examples might be harmful (repredicting)

Ad-hocs for monitoring during the year

### Vision Model

Yolo

### Docling and Co

### Table extraction

building a document extraction database document by document can improve performance taking advantage of same-company rag in-context learning

predictions for barrierefreie documents of WBM empty, one time because the pages showed **GuV**; also no predictions for Zoo 2024 **Passiva**

### Sophisticated approaches for text extraction

-   docling

not implemented

-   Nougat
-   maker
-   Azure

## Error analysis

-   errors from wrong formatted numbers

-   errors from wrong / unclear entity mapping
