---
title: "Extraction of tabular data from annual reports with LLMs"
author: "Simon Sch√§fer"
date: "September 8, 2025"
site: "bookdown::bookdown_site"
# Change this to FALSE if you'd like to install them manually on your own.
# params:
#   'Install needed packages for {thesisdown}': True

output:
  # bookdown::pdf_book:
  bookdown::pdf_document2:
    # latex_engine: xelatex
    # extra_dependencies: ["float"]
    extra_dependencies: ["flafter"]
    pandoc_args: 
      # - --lua-filter=report_misc/acrfull.lua
      - !expr acronymsdown::add_filter()
    toc: false
    lof: yes 
    lot: yes
    includes:
      in_header:
        - report_misc/bhtThesis.sty
        - report_misc/preamble.tex
  bookdown::gitbook:
    pandoc_args: 
      # - --lua-filter=report_misc/acrfull.lua
      - !expr acronymsdown::add_filter()
    code_folding: hide
    toc_depth: 4
    config:
      toc:
        before: |
          <li class="toc-logo"><a href="./"><img src="images/BHT_Logo_horizontal_Anthrazit_transparent.svg"></a></li>
    includes:
      in_header:
        - report_misc/head.html
      after_body: 
        - report_misc/image_effects.html
        - report_misc/code_folding.html
css: "report_misc/style.css"
documentclass: book
bibliography: ["report_misc/DS 4th: Extracting financial data from annual reports.bib"]
biblio-style: apa
link-citations: yes
csl: report_misc/apa-single-spaced.csl
acronyms:
  fromfile: ./report_misc/acronyms.yml
  loa_title: ""
  insert_loa: false
  insert_links: true
  sorting: usage
  include_unused: false
  style: short-long
# urlcolor: blue
# linkcolor: blue
# citecolor: green

editor_options: 
  markdown: 
    wrap: none
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(kableExtra.latex.load_packages = FALSE)
knitr::opts_chunk$set(fig.align='center') 
# knitr::opts_chunk$set(fig.width=8, fig.height=4) 
# knitr::opts_chunk$set(fig.pos = "H", out.extra = "")

library(jsonlite)
library(tidyverse)
library(reticulate)
use_python("~/anaconda3/envs/llm/bin/python")

library(knitr)
library(kableExtra)
library(DT) # For interactive HTML tables

library(shapviz)
library(h2o)

library(stringr)
library(readr)
library(xml2)
library(knitr)

library(ggplot2)
library(patchwork)
library(ggh4x)
library(PRROC)
library(yardstick)
```

```{r load_layout_functions, include=FALSE}
source("report_misc/layout_functions.R")
source("report_misc/helper_functions.R")

echo_flag <- knitr::is_html_output() ? NA : FALSE
message_flag = FALSE
warning_flag = FALSE
cache_large_figures = TRUE
std_dev <- knitr::is_html_output() ? 'png' : 'pdf'
```

\setDefaultLayout
\pagenumbering{gobble}
\pagestyle{empty}

#  {.unlisted .unnumbered}

## Abstract {.unlisted .unnumbered}

This thesis presents a comprehensive benchmark of information extraction from annual reports of German companies, focusing on the use of large language models (LLMs) to automate the retrieval of financial data from unstructured PDF documents. The study addresses two core challenges: identifying relevant pages containing financial tables and accurately extracting hierarchical tabular data for integration into relational databases.

Multiple open-weight LLMs are evaluated using diverse prompting strategies and compared to alternative methods such as regular expressions and term-frequency approaches. The work builds on and extends previous research by applying these techniques to German-language documents and open-weight models.

Results demonstrate that (even smaller) LLMs can reliably locate and extract financial information, supporting efficient audit processes. The findings highlight the potential of combining LLMs with rule-based methods to improve computational efficiency, and outline a practical workflow for a human-in-the-loop application for information extraction in regulatory and financial domains.

## Zusammenfassung {.unlisted .unnumbered}

Diese Arbeit pr√§sentiert die Ergebnisse eines umfassenden Benchmarks von Methoden zur Extraktion von Informationen aus deutschen Jahresabschlussberichten. Wir legen den Fokus unserer Untersuchung auf die Nutzung von LLMs (*gro√üe Sprachmodelle*) f√ºr die Automatisierung des Auslesens von Finanzdaten aus unstrukturierten \acr{PDF} Dokumenten.

Die Arbeit verfolgt zwei Hauptziele: das Identifizieren relevanter Seiten und das Extrahieren numerischer Werte aus hierarchischen tabellenartigen Strukturen. Dabei wird ein Format der extrahierten Daten sichergestellt, das das Einlesen in eine relationale Datenbank erlaubt.

Die Performanz mehrerer open-weight \acr{LLM}s wird √ºber verschiedene Prompting-Strategien hinweg und mit alternativen Methoden, beispielsweise regul√§re Ausdr√ºcke und Suchwortdichte-Ma√üen, verglichen. Die Arbeit baut auf vorherige Forschungsergebnisse auf und erweitert diese um die Anwendung auf deutschsprachigen Dokumenten und die Nutzung von open-weight Modellen.

Die Ergebnisse zeigen, dass (auch kleinere) Sprachmodelle zuverl√§sslich Kennzahlen aus dem Finanzwesen lokalisieren und extrahieren und so die Pr√ºfungst√§tigkeiten effizient unterst√ºtzen k√∂nnen. Die Ergebnisse zeigen Potential auf, \acr{LLM}s mit regelbasierten Methoden zu kombinieren, um die Effizienz der Informationslokalisation zu erh√∂hen. Wir skizzieren ein \acr{HITL}-basiertes Software-System f√ºr die Informationsextraktion im Bereich der Finanzkontrolle.

## Reading advices {.unlisted .unnumbered}

We recommend to read the thesis in its [digital gitbook version](https://famondir.github.io/Thesis/) instead of the PDF version. Furthermore, the author recommends to read the thesis (any version) on a screen that is larger than 21" and has at least full HD resolution. The more, the merrier.

\huge [üòä]{style="font-size: 1.5em;"} \normalsize

## Declaration of the Use of Artificial Intelligence {.unlisted .unnumbered}

We utilized GitHub Copilot in Visual Studio Code to assist with coding tasks, primarily employing GPT-4.1 and occasionally Claude Sonnet 4.

In addition to traditional literature research methods, such as using Google Scholar and tracking citations in academic papers, we also employed [perplexity.ai](https://www.perplexity.ai/) to support our literature review process.

Additionally, GPT-4.1 was used to enhance the clarity and quality of language throughout the thesis and to generate suggestions for summaries for various sections.

## Personal Goals and Learnings {.unlisted .unnumbered}

Several key goals were accomplished during this thesis. Notably, I succeeded in producing the thesis with bookdown in both PDF and HTML formats‚Äîa milestone I had already set during my previous PhD attempt in 2019. I also became familiar with creating Docker images and orchestrating jobs within a Kubernetes cluster. I gained practical experience in using large language models (LLMs), including leveraging restricted decoding to generate results suitable for downstream tasks.

There were, however, some areas I was interested in exploring further but could not pursue due to time constraints. These include learning to set up and administer a Kubernetes cluster independently, fine-tuning an LLM model, training a small encoder-only model from scratch, and applying visual LLMs for document parsing.

## Dedication {.unlisted .unnumbered}

We would like to express our sincere gratitude to Micha for his valuable support in extending the ground truth dataset for the information extraction task.

We also extend our heartfelt thanks to our family and friends for their unwavering support and patience throughout the past months.

\pagestyle{fancy}

\cleardoublepage
\pagenumbering{roman}
\setcounter{page}{1}
\tableofcontents

\large\bfseries
\addthumbsoverviewtocontents{section}{Thumb marks overview}
\thumbsoverviewback{Table of Thumbs}
\normalsize\mdseries

\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}
