<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Literature review | Extraction of tabular data from annual reports with LLMs</title>
  <meta name="description" content="2 Literature review | Extraction of tabular data from annual reports with LLMs" />
  <meta name="generator" content="bookdown 0.43.2 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Literature review | Extraction of tabular data from annual reports with LLMs" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Literature review | Extraction of tabular data from annual reports with LLMs" />
  
  
  

<meta name="author" content="Simon SchÃ¤fer" />


<meta name="date" content="2025-09-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="methodology.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/codefolding-lua-1.1/codefolding-lua.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/dt-ext-rowgroup-1.13.6/css/rowGroup.dataTables.min.css" rel="stylesheet" />
<script src="libs/dt-ext-rowgroup-1.13.6/js/dataTables.rowGroup.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/choices.js/public/assets/styles/choices.min.css" />
<script src="https://cdn.jsdelivr.net/npm/choices.js/public/assets/scripts/choices.min.js"></script>

<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glider-js@1/glider.min.css">
<script src="https://cdn.jsdelivr.net/npm/glider-js@1/glider.min.js"></script> -->


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="report_misc/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./index.html"><img src="images/BHT_Logo_horizontal_Anthrazit_transparent.svg"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#objectives"><i class="fa fa-check"></i><b>1.2</b> Objectives</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#introduction-methodology"><i class="fa fa-check"></i><b>1.3</b> Methodology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#thesis-outline"><i class="fa fa-check"></i><b>1.4</b> Thesis Outline</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#summary"><i class="fa fa-check"></i><b>1.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="literature-review.html"><a href="literature-review.html"><i class="fa fa-check"></i><b>2</b> Literature review</a>
<ul>
<li class="chapter" data-level="2.1" data-path="literature-review.html"><a href="literature-review.html#computer-science"><i class="fa fa-check"></i><b>2.1</b> Computer science</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="literature-review.html"><a href="literature-review.html#sparse-retrieval"><i class="fa fa-check"></i><b>2.1.1</b> Information retrieval</a></li>
<li class="chapter" data-level="2.1.2" data-path="literature-review.html"><a href="literature-review.html#nlp"><i class="fa fa-check"></i><b>2.1.2</b> Natural language processing</a></li>
<li class="chapter" data-level="2.1.3" data-path="literature-review.html"><a href="literature-review.html#text-processing"><i class="fa fa-check"></i><b>2.1.3</b> Text processing</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="literature-review.html"><a href="literature-review.html#document-layout-analysis"><i class="fa fa-check"></i><b>2.1.3.1</b> Document Layout Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="literature-review.html"><a href="literature-review.html#llm-theory"><i class="fa fa-check"></i><b>2.1.4</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="literature-review.html"><a href="literature-review.html#transformers"><i class="fa fa-check"></i><b>2.1.4.1</b> Transformers</a></li>
<li class="chapter" data-level="2.1.4.2" data-path="literature-review.html"><a href="literature-review.html#attention"><i class="fa fa-check"></i><b>2.1.4.2</b> Attention</a></li>
<li class="chapter" data-level="2.1.4.3" data-path="literature-review.html"><a href="literature-review.html#encoder"><i class="fa fa-check"></i><b>2.1.4.3</b> Encoder</a></li>
<li class="chapter" data-level="2.1.4.4" data-path="literature-review.html"><a href="literature-review.html#decoder"><i class="fa fa-check"></i><b>2.1.4.4</b> Decoder</a></li>
<li class="chapter" data-level="2.1.4.5" data-path="literature-review.html"><a href="literature-review.html#gpt-generative-pretrained-transformers"><i class="fa fa-check"></i><b>2.1.4.5</b> GPT (Generative Pretrained Transformers)</a></li>
<li class="chapter" data-level="2.1.4.6" data-path="literature-review.html"><a href="literature-review.html#mixture-of-experts"><i class="fa fa-check"></i><b>2.1.4.6</b> Mixture of Experts</a></li>
<li class="chapter" data-level="2.1.4.7" data-path="literature-review.html"><a href="literature-review.html#confidence-scores"><i class="fa fa-check"></i><b>2.1.4.7</b> Confidence scores</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="literature-review.html"><a href="literature-review.html#llm-methods"><i class="fa fa-check"></i><b>2.1.5</b> Generation enhancing methods with LLMs</a>
<ul>
<li class="chapter" data-level="2.1.5.1" data-path="literature-review.html"><a href="literature-review.html#in-context-learning"><i class="fa fa-check"></i><b>2.1.5.1</b> In-context Learning</a></li>
<li class="chapter" data-level="2.1.5.2" data-path="literature-review.html"><a href="literature-review.html#rag"><i class="fa fa-check"></i><b>2.1.5.2</b> RAG</a></li>
<li class="chapter" data-level="2.1.5.3" data-path="literature-review.html"><a href="literature-review.html#guided-and-restricted-decoding"><i class="fa fa-check"></i><b>2.1.5.3</b> Guided and restricted decoding</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="literature-review.html"><a href="literature-review.html#other-concepts"><i class="fa fa-check"></i><b>2.2</b> General machine learning and statistics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="literature-review.html"><a href="literature-review.html#sample-distribution-visualization-methods"><i class="fa fa-check"></i><b>2.2.1</b> Sample distribution visualization methods</a></li>
<li class="chapter" data-level="2.2.2" data-path="literature-review.html"><a href="literature-review.html#tree-based-machine-learning-algorithms"><i class="fa fa-check"></i><b>2.2.2</b> Tree based machine learning algorithms</a></li>
<li class="chapter" data-level="2.2.3" data-path="literature-review.html"><a href="literature-review.html#shap"><i class="fa fa-check"></i><b>2.2.3</b> Model agnostic explanation models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="literature-review.html"><a href="literature-review.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methodology.html"><a href="methodology.html"><i class="fa fa-check"></i><b>3</b> Methodology</a>
<ul>
<li class="chapter" data-level="3.1" data-path="methodology.html"><a href="methodology.html#problem-definition"><i class="fa fa-check"></i><b>3.1</b> Problem Definition</a></li>
<li class="chapter" data-level="3.2" data-path="methodology.html"><a href="methodology.html#research-design-philosophy"><i class="fa fa-check"></i><b>3.2</b> Research Design &amp; Philosophy</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="methodology.html"><a href="methodology.html#research-questions"><i class="fa fa-check"></i><b>3.2.1</b> Research questions</a></li>
<li class="chapter" data-level="3.2.2" data-path="methodology.html"><a href="methodology.html#hypotheses"><i class="fa fa-check"></i><b>3.2.2</b> Hypotheses</a></li>
<li class="chapter" data-level="3.2.3" data-path="methodology.html"><a href="methodology.html#evaluation-research"><i class="fa fa-check"></i><b>3.2.3</b> Evaluation research</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="methodology.html"><a href="methodology.html#evaluation-strategy"><i class="fa fa-check"></i><b>3.3</b> Evaluation Strategy</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="methodology.html"><a href="methodology.html#evaluation-framework-and-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Evaluation framework and metrics</a>
<ul>
<li class="chapter" data-level="3.3.1.1" data-path="methodology.html"><a href="methodology.html#page-identification"><i class="fa fa-check"></i><b>3.3.1.1</b> Page identification</a></li>
<li class="chapter" data-level="3.3.1.2" data-path="methodology.html"><a href="methodology.html#information-extraction"><i class="fa fa-check"></i><b>3.3.1.2</b> Information extraction</a></li>
<li class="chapter" data-level="3.3.1.3" data-path="methodology.html"><a href="methodology.html#error-rate-guidance"><i class="fa fa-check"></i><b>3.3.1.3</b> Error rate guidance</a></li>
</ul></li>
<li class="chapter" data-level="3.3.2" data-path="methodology.html"><a href="methodology.html#benchmarking"><i class="fa fa-check"></i><b>3.3.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="methodology.html"><a href="methodology.html#data-strategy"><i class="fa fa-check"></i><b>3.4</b> Data Strategy</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="methodology.html"><a href="methodology.html#sampling-methodology"><i class="fa fa-check"></i><b>3.4.1</b> Sampling methodology</a></li>
<li class="chapter" data-level="3.4.2" data-path="methodology.html"><a href="methodology.html#ground-truth-creation-process"><i class="fa fa-check"></i><b>3.4.2</b> Ground truth creation process</a></li>
<li class="chapter" data-level="3.4.3" data-path="methodology.html"><a href="methodology.html#preprocessing"><i class="fa fa-check"></i><b>3.4.3</b> Preprocessing</a></li>
<li class="chapter" data-level="3.4.4" data-path="methodology.html"><a href="methodology.html#data-splitting"><i class="fa fa-check"></i><b>3.4.4</b> Data splitting</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="methodology.html"><a href="methodology.html#experimental-framework"><i class="fa fa-check"></i><b>3.5</b> Experimental Framework</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="methodology.html"><a href="methodology.html#llm-overview"><i class="fa fa-check"></i><b>3.5.1</b> LLM overview</a></li>
<li class="chapter" data-level="3.5.2" data-path="methodology.html"><a href="methodology.html#approaches"><i class="fa fa-check"></i><b>3.5.2</b> Approaches</a>
<ul>
<li class="chapter" data-level="3.5.2.1" data-path="methodology.html"><a href="methodology.html#page-identification-2"><i class="fa fa-check"></i><b>3.5.2.1</b> Page identification</a></li>
<li class="chapter" data-level="3.5.2.2" data-path="methodology.html"><a href="methodology.html#information-extraction-2"><i class="fa fa-check"></i><b>3.5.2.2</b> Information extraction</a></li>
</ul></li>
<li class="chapter" data-level="3.5.3" data-path="methodology.html"><a href="methodology.html#error-analysis-methodology"><i class="fa fa-check"></i><b>3.5.3</b> Error analysis</a>
<ul>
<li class="chapter" data-level="3.5.3.1" data-path="methodology.html"><a href="methodology.html#page-identificaion"><i class="fa fa-check"></i><b>3.5.3.1</b> Page identificaion</a></li>
<li class="chapter" data-level="3.5.3.2" data-path="methodology.html"><a href="methodology.html#inforamtion-extraction"><i class="fa fa-check"></i><b>3.5.3.2</b> Inforamtion extraction</a></li>
</ul></li>
<li class="chapter" data-level="3.5.4" data-path="methodology.html"><a href="methodology.html#evaluation-methods"><i class="fa fa-check"></i><b>3.5.4</b> Evaluation methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>4</b> Implementation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="implementation.html"><a href="implementation.html#environments"><i class="fa fa-check"></i><b>4.1</b> Environments</a></li>
<li class="chapter" data-level="4.2" data-path="implementation.html"><a href="implementation.html#software-packages"><i class="fa fa-check"></i><b>4.2</b> Software Packages</a></li>
<li class="chapter" data-level="4.3" data-path="implementation.html"><a href="implementation.html#ground-truth-datasets"><i class="fa fa-check"></i><b>4.3</b> Ground truth datasets</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="implementation.html"><a href="implementation.html#ground-truth-creation"><i class="fa fa-check"></i><b>4.3.1</b> Ground truth creation</a></li>
<li class="chapter" data-level="4.3.2" data-path="implementation.html"><a href="implementation.html#ground-truth-database-composition"><i class="fa fa-check"></i><b>4.3.2</b> Ground truth database composition</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="implementation.html"><a href="implementation.html#data-processing"><i class="fa fa-check"></i><b>4.4</b> Data processing</a></li>
<li class="chapter" data-level="4.5" data-path="implementation.html"><a href="implementation.html#evaluation-and-reporting"><i class="fa fa-check"></i><b>4.5</b> Evaluation and Reporting</a></li>
<li class="chapter" data-level="4.6" data-path="implementation.html"><a href="implementation.html#speedup-with-vllm-and-batching"><i class="fa fa-check"></i><b>4.6</b> Speedup with vLLM and batching</a></li>
<li class="chapter" data-level="4.7" data-path="implementation.html"><a href="implementation.html#gpu-benchmark"><i class="fa fa-check"></i><b>4.7</b> Hardware normalization</a></li>
<li class="chapter" data-level="4.8" data-path="implementation.html"><a href="implementation.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>5</b> Results</a>
<ul>
<li class="chapter" data-level="5.1" data-path="results.html"><a href="results.html#page-identification-introduction"><i class="fa fa-check"></i><b>5.1</b> Page identification</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="results.html"><a href="results.html#approaches-1"><i class="fa fa-check"></i><b>5.1.1</b> Approaches</a></li>
<li class="chapter" data-level="5.1.2" data-path="results.html"><a href="results.html#comparison-page-identification"><i class="fa fa-check"></i><b>5.1.2</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="results.html"><a href="results.html#table-extraction-introduction"><i class="fa fa-check"></i><b>5.2</b> Information extraction</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="results.html"><a href="results.html#approaches-2"><i class="fa fa-check"></i><b>5.2.1</b> Approaches</a></li>
<li class="chapter" data-level="5.2.2" data-path="results.html"><a href="results.html#comparing-table-extraction-methods"><i class="fa fa-check"></i><b>5.2.2</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="results.html"><a href="results.html#error-rate-guidance-results"><i class="fa fa-check"></i><b>5.3</b> Error rate guidance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="results.html"><a href="results.html#page-identification-error-rate-results"><i class="fa fa-check"></i><b>5.3.1</b> Page identification</a></li>
<li class="chapter" data-level="5.3.2" data-path="results.html"><a href="results.html#information-extraction-error-rate-results"><i class="fa fa-check"></i><b>5.3.2</b> Information extraction</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="results.html"><a href="results.html#summary-results"><i class="fa fa-check"></i><b>5.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>6</b> Discussion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="discussion.html"><a href="discussion.html#answer-research-questions"><i class="fa fa-check"></i><b>6.1</b> Research questions</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="discussion.html"><a href="discussion.html#page-identification-5"><i class="fa fa-check"></i><b>6.1.1</b> Page identification</a></li>
<li class="chapter" data-level="6.1.2" data-path="discussion.html"><a href="discussion.html#information-extraction-4"><i class="fa fa-check"></i><b>6.1.2</b> Information extraction</a>
<ul>
<li class="chapter" data-level="6.1.2.1" data-path="discussion.html"><a href="discussion.html#extraction-performace-hypotheses-evaluation"><i class="fa fa-check"></i><b>6.1.2.1</b> Core capabilities</a></li>
<li class="chapter" data-level="6.1.2.2" data-path="discussion.html"><a href="discussion.html#extraction-performace-predictor-hypotheses-evaluation"><i class="fa fa-check"></i><b>6.1.2.2</b> Feature effects</a></li>
<li class="chapter" data-level="6.1.2.3" data-path="discussion.html"><a href="discussion.html#error-rate-guidance-1"><i class="fa fa-check"></i><b>6.1.2.3</b> Error rate guidance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="discussion.html"><a href="discussion.html#error-analysis"><i class="fa fa-check"></i><b>6.2</b> Error analysis</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="discussion.html"><a href="discussion.html#page-identification-6"><i class="fa fa-check"></i><b>6.2.1</b> Page identification</a>
<ul>
<li class="chapter" data-level="6.2.1.1" data-path="discussion.html"><a href="discussion.html#llm-approach"><i class="fa fa-check"></i><b>6.2.1.1</b> LLM approach</a></li>
<li class="chapter" data-level="6.2.1.2" data-path="discussion.html"><a href="discussion.html#error-analysis-tf"><i class="fa fa-check"></i><b>6.2.1.2</b> Term frequency approach</a></li>
</ul></li>
<li class="chapter" data-level="6.2.2" data-path="discussion.html"><a href="discussion.html#information-extraction-5"><i class="fa fa-check"></i><b>6.2.2</b> Information extraction</a>
<ul>
<li class="chapter" data-level="6.2.2.1" data-path="discussion.html"><a href="discussion.html#real-tables-3"><i class="fa fa-check"></i><b>6.2.2.1</b> Real tables</a></li>
<li class="chapter" data-level="6.2.2.2" data-path="discussion.html"><a href="discussion.html#same-company-evaluation"><i class="fa fa-check"></i><b>6.2.2.2</b> Company specific results</a></li>
<li class="chapter" data-level="6.2.2.3" data-path="discussion.html"><a href="discussion.html#synthetic-tables-3"><i class="fa fa-check"></i><b>6.2.2.3</b> Synthetic tables</a></li>
<li class="chapter" data-level="6.2.2.4" data-path="discussion.html"><a href="discussion.html#numeric-transformation-discussion"><i class="fa fa-check"></i><b>6.2.2.4</b> Numeric transformations</a></li>
<li class="chapter" data-level="6.2.2.5" data-path="discussion.html"><a href="discussion.html#regex-synth-backend-discussion"><i class="fa fa-check"></i><b>6.2.2.5</b> Regular expression approach</a></li>
<li class="chapter" data-level="6.2.2.6" data-path="discussion.html"><a href="discussion.html#openai-discussion"><i class="fa fa-check"></i><b>6.2.2.6</b> OpenAI models</a></li>
</ul></li>
<li class="chapter" data-level="6.2.3" data-path="discussion.html"><a href="discussion.html#unconsistent-label-matching"><i class="fa fa-check"></i><b>6.2.3</b> Ground truth creation</a></li>
<li class="chapter" data-level="6.2.4" data-path="discussion.html"><a href="discussion.html#context-rot"><i class="fa fa-check"></i><b>6.2.4</b> Context rot</a>
<ul>
<li class="chapter" data-level="6.2.4.1" data-path="discussion.html"><a href="discussion.html#page-identification-7"><i class="fa fa-check"></i><b>6.2.4.1</b> Page identification</a></li>
<li class="chapter" data-level="6.2.4.2" data-path="discussion.html"><a href="discussion.html#information-extraction-6"><i class="fa fa-check"></i><b>6.2.4.2</b> Information extraction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="discussion.html"><a href="discussion.html#general-performance"><i class="fa fa-check"></i><b>6.3</b> Practical Implications and Limitations</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="discussion.html"><a href="discussion.html#page-identification-8"><i class="fa fa-check"></i><b>6.3.1</b> Page identification</a>
<ul>
<li class="chapter" data-level="6.3.1.1" data-path="discussion.html"><a href="discussion.html#transfering-the-system-to-new-problems"><i class="fa fa-check"></i><b>6.3.1.1</b> Transfering the system to new problems</a></li>
<li class="chapter" data-level="6.3.1.2" data-path="discussion.html"><a href="discussion.html#limitations-of-transfer"><i class="fa fa-check"></i><b>6.3.1.2</b> Limitations of transfer</a></li>
<li class="chapter" data-level="6.3.1.3" data-path="discussion.html"><a href="discussion.html#toc-discussion"><i class="fa fa-check"></i><b>6.3.1.3</b> Possible improvement</a></li>
<li class="chapter" data-level="6.3.1.4" data-path="discussion.html"><a href="discussion.html#conclusion-4"><i class="fa fa-check"></i><b>6.3.1.4</b> Conclusion</a></li>
<li class="chapter" data-level="6.3.1.5" data-path="discussion.html"><a href="discussion.html#efficency-discussion"><i class="fa fa-check"></i><b>6.3.1.5</b> Energy usage and runtime</a></li>
</ul></li>
<li class="chapter" data-level="6.3.2" data-path="discussion.html"><a href="discussion.html#alternative-input-formats-extraction"><i class="fa fa-check"></i><b>6.3.2</b> Information extraction</a></li>
<li class="chapter" data-level="6.3.3" data-path="discussion.html"><a href="discussion.html#error-rate-guidance-2"><i class="fa fa-check"></i><b>6.3.3</b> Error rate guidance</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="discussion.html"><a href="discussion.html#not-covered"><i class="fa fa-check"></i><b>6.4</b> Not covered</a></li>
<li class="chapter" data-level="6.5" data-path="discussion.html"><a href="discussion.html#outlook-implementation"><i class="fa fa-check"></i><b>6.5</b> Outlook</a></li>
<li class="chapter" data-level="6.6" data-path="discussion.html"><a href="discussion.html#ethics"><i class="fa fa-check"></i><b>6.6</b> Ethical &amp; Practical Considerations</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="discussion.html"><a href="discussion.html#pdf-extraction-limitations"><i class="fa fa-check"></i><b>6.6.1</b> PDF extraction limitations</a></li>
<li class="chapter" data-level="6.6.2" data-path="discussion.html"><a href="discussion.html#computational-constraints"><i class="fa fa-check"></i><b>6.6.2</b> Computational constraints</a></li>
<li class="chapter" data-level="6.6.3" data-path="discussion.html"><a href="discussion.html#generalizability-scope"><i class="fa fa-check"></i><b>6.6.3</b> Generalizability scope</a></li>
<li class="chapter" data-level="6.6.4" data-path="discussion.html"><a href="discussion.html#ethical-considerations"><i class="fa fa-check"></i><b>6.6.4</b> Ethical considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="discussion.html"><a href="discussion.html#conclusion"><i class="fa fa-check"></i><b>7</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="8" data-path="page-identification-report.html"><a href="page-identification-report.html"><i class="fa fa-check"></i><b>8</b> Appendix A - Page identification report</a>
<ul>
<li class="chapter" data-level="8.1" data-path="page-identification-report.html"><a href="page-identification-report.html#regex-page-identification"><i class="fa fa-check"></i><b>8.1</b> Baseline: Regex</a></li>
<li class="chapter" data-level="8.2" data-path="page-identification-report.html"><a href="page-identification-report.html#toc-understanding"><i class="fa fa-check"></i><b>8.2</b> Table of Contents understanding</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="page-identification-report.html"><a href="page-identification-report.html#text-based-toc-understanding"><i class="fa fa-check"></i><b>8.2.1</b> Details for the approaches</a></li>
<li class="chapter" data-level="8.2.2" data-path="page-identification-report.html"><a href="page-identification-report.html#results-5"><i class="fa fa-check"></i><b>8.2.2</b> Results</a>
<ul>
<li class="chapter" data-level="8.2.2.1" data-path="page-identification-report.html"><a href="page-identification-report.html#comparison-of-the-different-approaches"><i class="fa fa-check"></i><b>8.2.2.1</b> Comparison of the different approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.2.3" data-path="page-identification-report.html"><a href="page-identification-report.html#machine-readable-toc-approach-specific-results"><i class="fa fa-check"></i><b>8.2.3</b> Machine readable TOC approach specific results</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="page-identification-report.html"><a href="page-identification-report.html#llm-page-identification"><i class="fa fa-check"></i><b>8.3</b> Classification with LLMs</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="page-identification-report.html"><a href="page-identification-report.html#binary-classification-1"><i class="fa fa-check"></i><b>8.3.1</b> Binary classification</a></li>
<li class="chapter" data-level="8.3.2" data-path="page-identification-report.html"><a href="page-identification-report.html#multi-class-classification-1"><i class="fa fa-check"></i><b>8.3.2</b> Multi-class classification</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="page-identification-report.html"><a href="page-identification-report.html#tf-classifier"><i class="fa fa-check"></i><b>8.4</b> Term frequency based classifier</a></li>
<li class="chapter" data-level="8.5" data-path="page-identification-report.html"><a href="page-identification-report.html#summary-4"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="information-extraction-report.html"><a href="information-extraction-report.html"><i class="fa fa-check"></i><b>9</b> Appendix B - Information extraction report</a>
<ul>
<li class="chapter" data-level="9.1" data-path="information-extraction-report.html"><a href="information-extraction-report.html#baseline-regex"><i class="fa fa-check"></i><b>9.1</b> Baseline: Regex</a></li>
<li class="chapter" data-level="9.2" data-path="information-extraction-report.html"><a href="information-extraction-report.html#extraction-with-llms-real-tables"><i class="fa fa-check"></i><b>9.2</b> Extraction with LLMs</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="information-extraction-report.html"><a href="information-extraction-report.html#real-table-extraction-results"><i class="fa fa-check"></i><b>9.2.1</b> Real tables</a></li>
<li class="chapter" data-level="9.2.2" data-path="information-extraction-report.html"><a href="information-extraction-report.html#synthetic-table-extraction"><i class="fa fa-check"></i><b>9.2.2</b> Synthetic tables</a></li>
<li class="chapter" data-level="9.2.3" data-path="information-extraction-report.html"><a href="information-extraction-report.html#real-table-extraction-synth-context"><i class="fa fa-check"></i><b>9.2.3</b> Hybrid approach</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="information-extraction-report.html"><a href="information-extraction-report.html#summary-5"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="error-guidance-report.html"><a href="error-guidance-report.html"><i class="fa fa-check"></i><b>10</b> Appendix C - Error rate guidance report</a>
<ul>
<li class="chapter" data-level="10.1" data-path="error-guidance-report.html"><a href="error-guidance-report.html#page-identification-9"><i class="fa fa-check"></i><b>10.1</b> Page identification</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="error-guidance-report.html"><a href="error-guidance-report.html#binary-classification-2"><i class="fa fa-check"></i><b>10.1.1</b> Binary classification</a></li>
<li class="chapter" data-level="10.1.2" data-path="error-guidance-report.html"><a href="error-guidance-report.html#multi-class-classification-2"><i class="fa fa-check"></i><b>10.1.2</b> Multi-class classification</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="error-guidance-report.html"><a href="error-guidance-report.html#extraction-with-llms"><i class="fa fa-check"></i><b>10.2</b> Extraction with LLMs</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="error-guidance-report.html"><a href="error-guidance-report.html#real-tables-4"><i class="fa fa-check"></i><b>10.2.1</b> Real tables</a></li>
<li class="chapter" data-level="10.2.2" data-path="error-guidance-report.html"><a href="error-guidance-report.html#synthetic-tables-4"><i class="fa fa-check"></i><b>10.2.2</b> Synthetic tables</a></li>
<li class="chapter" data-level="10.2.3" data-path="error-guidance-report.html"><a href="error-guidance-report.html#hybrid-respect-units"><i class="fa fa-check"></i><b>10.2.3</b> Hybrid approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="feature-effect-analysis.html"><a href="feature-effect-analysis.html"><i class="fa fa-check"></i><b>11</b> Appendix D - Feature effect analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="feature-effect-analysis.html"><a href="feature-effect-analysis.html#regular-expressions-4"><i class="fa fa-check"></i><b>11.1</b> Regular expressions</a></li>
<li class="chapter" data-level="11.2" data-path="feature-effect-analysis.html"><a href="feature-effect-analysis.html#real-tables-5"><i class="fa fa-check"></i><b>11.2</b> Real tables</a></li>
<li class="chapter" data-level="11.3" data-path="feature-effect-analysis.html"><a href="feature-effect-analysis.html#synthetic-tables-5"><i class="fa fa-check"></i><b>11.3</b> Synthetic tables</a></li>
<li class="chapter" data-level="11.4" data-path="feature-effect-analysis.html"><a href="feature-effect-analysis.html#hybrid-approach-3"><i class="fa fa-check"></i><b>11.4</b> Hybrid approach</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html"><i class="fa fa-check"></i><b>12</b> Appendix E - Miscellaneous</a>
<ul>
<li class="chapter" data-level="12.1" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#hitl"><i class="fa fa-check"></i><b>12.1</b> Human in the loop application</a></li>
<li class="chapter" data-level="12.2" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#local-machine"><i class="fa fa-check"></i><b>12.2</b> Local machine</a></li>
<li class="chapter" data-level="12.3" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#benchmarks"><i class="fa fa-check"></i><b>12.3</b> Benchmarks</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#text-extraction-benchmark"><i class="fa fa-check"></i><b>12.3.1</b> Text extraction</a></li>
<li class="chapter" data-level="12.3.2" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#table-detection-benchmark"><i class="fa fa-check"></i><b>12.3.2</b> Table detection</a>
<ul>
<li class="chapter" data-level="12.3.2.1" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#old-classification-with-llm"><i class="fa fa-check"></i><b>12.3.2.1</b> old classification with llm</a></li>
<li class="chapter" data-level="12.3.2.2" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#yolo"><i class="fa fa-check"></i><b>12.3.2.2</b> yolo benchmark and table transformer</a></li>
</ul></li>
<li class="chapter" data-level="12.3.3" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#vllm-batch-speed"><i class="fa fa-check"></i><b>12.3.3</b> Large language model process speed</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#prompts"><i class="fa fa-check"></i><b>12.4</b> Prompts</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#toc-understanding-promts"><i class="fa fa-check"></i><b>12.4.1</b> TOC understanding</a></li>
<li class="chapter" data-level="12.4.2" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#classification-prompts"><i class="fa fa-check"></i><b>12.4.2</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#regex-page-identification-code"><i class="fa fa-check"></i><b>12.5</b> Regular expressions</a></li>
<li class="chapter" data-level="12.6" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#annual-comprehensive-financial-report-balance-sheet"><i class="fa fa-check"></i><b>12.6</b> Annual Comprehensive Financial Report Balance Sheet</a></li>
<li class="chapter" data-level="12.7" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#regex-extraction-mistakes"><i class="fa fa-check"></i><b>12.7</b> Table extraction with regular expressions</a></li>
<li class="chapter" data-level="12.8" data-path="appendix-e---miscellaneous.html"><a href="appendix-e---miscellaneous.html#tf-missclassifications"><i class="fa fa-check"></i><b>12.8</b> Term frequency missclassifications</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="tables.html"><a href="tables.html"><i class="fa fa-check"></i><b>13</b> Tables</a>
<ul>
<li class="chapter" data-level="13.1" data-path="tables.html"><a href="tables.html#classification"><i class="fa fa-check"></i><b>13.1</b> Classification</a></li>
<li class="chapter" data-level="13.2" data-path="tables.html"><a href="tables.html#table-extraction"><i class="fa fa-check"></i><b>13.2</b> Table extraction</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="tables.html"><a href="tables.html#hybrid-approach-4"><i class="fa fa-check"></i><b>13.2.1</b> Hybrid approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="figure-collection.html"><a href="figure-collection.html"><i class="fa fa-check"></i><b>14</b> Figures</a>
<ul>
<li class="chapter" data-level="14.1" data-path="figure-collection.html"><a href="figure-collection.html#page-identification-10"><i class="fa fa-check"></i><b>14.1</b> Page identification</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="figure-collection.html"><a href="figure-collection.html#regex-baseline"><i class="fa fa-check"></i><b>14.1.1</b> Regex baseline</a></li>
<li class="chapter" data-level="14.1.2" data-path="figure-collection.html"><a href="figure-collection.html#toc-understanding-1"><i class="fa fa-check"></i><b>14.1.2</b> TOC understanding</a></li>
<li class="chapter" data-level="14.1.3" data-path="figure-collection.html"><a href="figure-collection.html#classification-1"><i class="fa fa-check"></i><b>14.1.3</b> Classification</a>
<ul>
<li class="chapter" data-level="14.1.3.1" data-path="figure-collection.html"><a href="figure-collection.html#binary"><i class="fa fa-check"></i><b>14.1.3.1</b> Binary</a></li>
<li class="chapter" data-level="14.1.3.2" data-path="figure-collection.html"><a href="figure-collection.html#multi-class-classification-3"><i class="fa fa-check"></i><b>14.1.3.2</b> Multi-class classification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="figure-collection.html"><a href="figure-collection.html#table-extraction-1"><i class="fa fa-check"></i><b>14.2</b> Table extraction</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="figure-collection.html"><a href="figure-collection.html#regex-approach"><i class="fa fa-check"></i><b>14.2.1</b> Regex approach</a>
<ul>
<li class="chapter" data-level="14.2.1.1" data-path="figure-collection.html"><a href="figure-collection.html#real-tables-6"><i class="fa fa-check"></i><b>14.2.1.1</b> Real tables</a></li>
<li class="chapter" data-level="14.2.1.2" data-path="figure-collection.html"><a href="figure-collection.html#synthetic-tables-6"><i class="fa fa-check"></i><b>14.2.1.2</b> Synthetic tables</a></li>
</ul></li>
<li class="chapter" data-level="14.2.2" data-path="figure-collection.html"><a href="figure-collection.html#real-tables-7"><i class="fa fa-check"></i><b>14.2.2</b> Real tables</a>
<ul>
<li class="chapter" data-level="14.2.2.1" data-path="figure-collection.html"><a href="figure-collection.html#examples-from-same-company"><i class="fa fa-check"></i><b>14.2.2.1</b> Examples from same company</a></li>
<li class="chapter" data-level="14.2.2.2" data-path="figure-collection.html"><a href="figure-collection.html#openai-models-1"><i class="fa fa-check"></i><b>14.2.2.2</b> OpenAI models</a></li>
<li class="chapter" data-level="14.2.2.3" data-path="figure-collection.html"><a href="figure-collection.html#comparing-input-formats-and-text-extration-libraries"><i class="fa fa-check"></i><b>14.2.2.3</b> Comparing input formats and text extration libraries</a></li>
<li class="chapter" data-level="14.2.2.4" data-path="figure-collection.html"><a href="figure-collection.html#hypotheses-6"><i class="fa fa-check"></i><b>14.2.2.4</b> Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="14.2.3" data-path="figure-collection.html"><a href="figure-collection.html#synthetic-tables-7"><i class="fa fa-check"></i><b>14.2.3</b> Synthetic tables</a>
<ul>
<li class="chapter" data-level="14.2.3.1" data-path="figure-collection.html"><a href="figure-collection.html#confidence-4"><i class="fa fa-check"></i><b>14.2.3.1</b> Confidence</a></li>
</ul></li>
<li class="chapter" data-level="14.2.4" data-path="figure-collection.html"><a href="figure-collection.html#hybrid-approach-5"><i class="fa fa-check"></i><b>14.2.4</b> Hybrid approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="layout-testing.html"><a href="layout-testing.html"><i class="fa fa-check"></i><b>15</b> Layout testing</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Extraction of tabular data from annual reports with LLMs</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="literature-review" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Literature review<a href="literature-review.html#literature-review" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter presents all literature relevant to our research questions and provides essential background on the concepts and methods employed in this thesis. As outlined in Chapter <a href="introduction.html#introduction">1</a>, the problem addressed here lies within the field of information retrieval and is approached using <a href="glossary.html#acronyms_NLP">NLP (natural language processing)</a> techniques. Accordingly, Section <a href="literature-review.html#computer-science">2.1</a> reviews techniques for locating information within and extracting it from documents. Subsection <a href="literature-review.html#llm-theory">2.1.4</a> then explains the mechanisms and architectures of modern <a href="glossary.html#acronyms_LLM">LLM (large language model)</a>s, with particular attention to the <a href="glossary.html#acronyms_MoE">MoE (mixture of experts)</a> architecture.</p>
<p>Subsequently, Subsection <a href="literature-review.html#llm-methods">2.1.5</a> discusses the prompting strategy <em>in-context learning</em>, which leverages the âprogramming by exampleâ paradigm, and explores how <a href="glossary.html#acronyms_RAG">RAG (retrieval augmanted generation)</a> integrates into this framework. We also demonstrate how guided decoding can be used to generate structured responses suitable for downstream tasks.</p>
<p>Section <a href="literature-review.html#other-concepts">2.2</a> introduces the <a href="glossary.html#acronyms_SHAP">SHAP (SHapley Additive exPlanations)</a> framework, a unified model for explaining machine learning predictions, applicable to complex models such as deep neural networks and random forests. The latter are briefly described as well. We employ random forests and <a href="glossary.html#acronyms_SHAP">SHAP</a> to test our hypotheses regarding potential predictors for the information extraction task. Our hypotheses are stated in Section <a href="methodology.html#research-questions">3.2.1</a>.</p>
<div id="computer-science" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Computer science<a href="literature-review.html#computer-science" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Information retrieval and <a href="glossary.html#acronyms_NLP">NLP</a> are important areas in computer science, enabling the efficient extraction and utilization of information from vast datasets <span class="citation">(<a href="#ref-rajExploringNewApproaches2025">Raj &amp; Mishra, 2025</a>)</span>. We will describe both in the following subsections.</p>
<div id="sparse-retrieval" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Information retrieval<a href="literature-review.html#sparse-retrieval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Information retrieval deals with finding relevant information within large collections of unstructured text. It is used in search engines, of dialogue, question-answering, and recommender systems <span class="citation">(<a href="#ref-zhuLargeLanguageModels2024">Zhu et al., 2024</a>)</span>.</p>
<p>The term frequency <span class="math inline">\(\mathrm{tf}_{t,d}\)</span> is one of the oldest measures used in retrieval algorithms. It just counts the number of occurrences of a term in a document. Document is an abstraction in this case. It can be a sentence, a page or a file. Since longer documents might have higher term frequency for each term, it is useful to normalize the value by the document length <span class="math inline">\(|d|\)</span>. This measure could be called term rate:</p>
<p><span class="math display" id="eq:term-rate">\[\begin{equation}
\mathrm{tr}_t = \frac{\mathrm{tf}_{t,d}}{|d|}
\tag{2.1}
\end{equation}\]</span></p>
<p>It is part of well established measures as <a href="glossary.html#acronyms_TF-IDF">TF-IDF (Frequency-Inverse Document Frequency)</a> and Okapi <a href="glossary.html#acronyms_BM25">BM25 (best matching 25)</a>. Both are used for ranking, how relevant a document is for a given search query and are widely used in information retrieval systems <span class="citation">(<a href="#ref-robertsonUnderstandingInverseDocument2004">S. Robertson, 2004</a>; <a href="#ref-robertsonProbabilisticRelevanceFramework2009">S. Robertson &amp; Zaragoza, 2009</a>)</span> and thus can be part of a <a href="glossary.html#acronyms_RAG">RAG</a> architecture too. <a href="glossary.html#acronyms_BM25">BM25</a> is one of the âmost successful Web-search and corporate-search algorithmsâ <span class="citation">(<a href="#ref-robertsonProbabilisticRelevanceFramework2009">S. Robertson &amp; Zaragoza, 2009, p. 1</a>)</span>.</p>
<p>The <a href="glossary.html#acronyms_IDF">IDF (Inverse Document Frequency)</a> is often used as a weighting function. If the ranking of possible results of a search query is simply calculated as sum of all term frequencies in a document, that are present in the query as well less informative terms get equal weight.</p>
<p>Looking at the search query: âIs the positron blue?â, helps to illustrate the problem. The terms <em>is</em>, <em>the</em> and <em>blue</em> might be present often in a document for children that is talking about the sky or sea. Such a document could get high score, even though <em>positron</em> is never mentioned. It would be good, if it is most important if the term <em>positron</em> is in the document. We can achieve this by multiplying all term frequencies with the <a href="glossary.html#acronyms_IDF">IDF</a> score <span class="citation">(<a href="#ref-manningIntroductionInformationRetrieval2008">Manning et al., 2008, p. 118</a>)</span>:</p>
<p><span class="math display" id="eq:idf">\[\begin{equation}
\mathrm{idf}_t = \log \frac{N}{\mathrm{df}_t}
\tag{2.2}
\end{equation}\]</span></p>
<p><span class="math inline">\(N\)</span> is the number of documents in the collection of documents and <span class="math inline">\(\mathrm{df}_t\)</span> the number of documents, that contain term <span class="math inline">\(t\)</span>. While the term frequencies <span class="math inline">\(\mathrm{tf}_{t,d}\)</span> are calculated separate for each document, the <a href="glossary.html#acronyms_IDF">IDF</a> score is computed once for the whole collection. The <a href="glossary.html#acronyms_TF-IDF">TF-IDF</a> score is then defined by:</p>
<p><span class="math display" id="eq:tf-idf">\[\begin{equation}
\mathrm{tf\text{-}idf}_t = \mathrm{idf}_t \cdot \mathrm{tf}_{t,d}
\tag{2.3}
\end{equation}\]</span></p>
<p>The more advanced measure <a href="glossary.html#acronyms_BM25">BM25</a> is derived in <span class="citation">Manning et al. (<a href="#ref-manningIntroductionInformationRetrieval2008">2008</a>)</span>. It introduce a saturation term, that prevents a term occurring very often in a document will increasing the score linearly. The 25 in <a href="glossary.html#acronyms_BM25">BM25</a> simply refers to the fact, that it is the 25th version of the Okapi system, that was developed at City University London <span class="citation">(<a href="#ref-robertsonSimpleEffectiveApproximations1994a">S. E. Robertson &amp; Walker, 1994</a>; <a href="#ref-robertsonProbabilisticRelevanceFramework2009">S. Robertson &amp; Zaragoza, 2009</a>)</span>.</p>
<p>Measures as <a href="glossary.html#acronyms_TF-IDF">TF-IDF</a> or <a href="glossary.html#acronyms_BM25">BM25</a> are also used for classification tasks, i.e.Â in the context of sentiment analysis <span class="citation">(<a href="#ref-carvalhoTFIDFCRFNovelSupervised2020">Carvalho &amp; Guedes, 2020</a>)</span> and semantic understanding <span class="citation">(<a href="#ref-rathiImportanceTermWeighting2023">Rathi &amp; Mustafi, 2023</a>)</span>. They can be used for <a href="glossary.html#acronyms_RAG">RAG</a> and are called sparse retrievers in this context.</p>
</div>
<div id="nlp" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Natural language processing<a href="literature-review.html#nlp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="glossary.html#acronyms_NLP">NLP (natural language processing)</a> is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language <span class="citation">(<a href="#ref-mahmudiNaturalLanguageProcessing2024">Mahmudi, 22 Dec 2024 6:46pm</a>)</span>. <a href="glossary.html#acronyms_NLP">NLP</a> encompasses a wide range of tasks, including sentiment analysis, information extraction, table and dialog understanding, as well as summarization, code generation and machine translation <span class="citation">(<a href="#ref-qinLargeLanguageModels2025">Qin et al., 2025</a>)</span>.</p>
<p>Methods of <a href="glossary.html#acronyms_NLP">NLP</a> can enhance the process of information retrieval. Vector embedding based techniques emerged recently and are called dense retrievers <span class="citation">(<a href="#ref-zhuLargeLanguageModels2024">Zhu et al., 2024</a>)</span>.</p>
<p>In the context of this thesis, information retrieval techniques are applied to locate financial information from annual reports. Techniques for information extraction are used to generate responses formatted for downstream tasks.</p>
</div>
<div id="text-processing" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Text processing<a href="literature-review.html#text-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many information extraction tasks target information that is found in natural texts or tables. In order to use <a href="glossary.html#acronyms_NLP">NLP</a> techniques it is a prerequisite to extract those texts. <span class="citation">Adhikari &amp; Agarwal (<a href="#ref-adhikariComparativeStudyPDF2024">2024</a>)</span> show in a benchmark study that <em>pypdfium</em> and <em>PyMuPDF</em> perform best among rule-based systems. But the transformer-based vision and document understanding tool <em>Nougat</em> <span class="citation">(<a href="#ref-blecherNougatNeuralOptical2023">Blecher et al., 2023</a>)</span> performs even better.</p>
<p><span class="citation">Adhikari &amp; Agarwal (<a href="#ref-adhikariComparativeStudyPDF2024">2024</a>)</span> also benchmark different table extraction tools. They find, that the transformer-based <em>Table Transformer</em> <span class="citation">(<a href="#ref-smockPubTables1MComprehensiveTable2022">Smock et al., 2022</a>)</span> extracts tables most accurate in most cases. Rule-based tools are more accurate for table extraction tasks from tenders and manuals.</p>
<p>Sometimes there is only a visual representation of texts, resulting from scanned pages. In this case <a href="glossary.html#acronyms_OCR">OCR (optical character recognition)</a> techniques can be used, to transform the visual information back into a text representation. A comparison study for recent <a href="glossary.html#acronyms_OCR">OCR</a> solutions can be found at <span class="citation">Francis &amp; Sangeetha (<a href="#ref-francisComparisonStudyOptical2025">2025</a>)</span>.</p>
<div id="document-layout-analysis" class="section level4 hasAnchor" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Document Layout Analysis<a href="literature-review.html#document-layout-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An important step in the process of extracting information from complex documents is to recognize the layout of a document <span class="citation">(<a href="#ref-zhongPubLayNetLargestDataset2019">Zhong et al., 2019</a>)</span>. Common tasks are determining the correct order of texts, correctly align captions to tables and figures, identifying headings, tables and figures. This can be used to separate the contents of smaller tables, that contain the information of interest, from surrounding text and thus strip potentially distracting content. <span class="citation">Paul (<a href="#ref-paulStateoftheArtModelArchitectures2025">2025</a>)</span> gives a broad overview for state-of-the-art document layout analysis architectures and presents a comparison of different models performance.</p>
</div>
</div>
<div id="llm-theory" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Large Language Models<a href="literature-review.html#llm-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Minaee et al. (<a href="#ref-minaeeLargeLanguageModels2025">2025</a>)</span> give a broad overview on the field of language models. They discuss, how they are built, used and augmented. They present popular datasets, performance benchmarks and outline challenges and future directions. Their survey is neither limited on decoder-only nor large language models (\acr{LLM}). But their time scope ends before 2024, so recent advancements are not reflected. (not a good introduction survey)</p>
<p>For readers, who want to build understanding for modern <a href="glossary.html#acronyms_LLM">LLM</a> we recommend to work through the book âDive into Deep Learningâ <span class="citation">(<a href="#ref-zhangDiveDeepLearning2023">A. Zhang et al., 2023</a>)</span>.</p>
<div id="transformers" class="section level4 hasAnchor" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> Transformers<a href="literature-review.html#transformers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Wichtig</p>
<p>seit 2017</p>
<p>training on token prediction (base models); masking,</p>
</div>
<div id="attention" class="section level4 hasAnchor" number="2.1.4.2">
<h4><span class="header-section-number">2.1.4.2</span> Attention<a href="literature-review.html#attention" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The most obvious challenge is computational cost. The amount of processing power required scales quadratically with the length of the input <span class="citation">(<a href="#ref-tahirUnderstandingLLMContext2025">Tahir, 2025</a>)</span>.</p>
<p>Sliding window attention (e.g., as popularized by Mistral) or Gemma 2; Sliding window attention is mainly used to improve computational performance <span class="citation">(<a href="#ref-raschkaInstructionPretrainingLLMs2025">Raschka, 2025</a>)</span></p>
<p>Group-query attention (like in Llama 2 and 3)</p>
<p>A key article is âAttention Is All You Needâ that <span class="citation">(<a href="#ref-vaswaniAttentionAllYou2023">Vaswani et al., 2023</a>)</span></p>
</div>
<div id="encoder" class="section level4 hasAnchor" number="2.1.4.3">
<h4><span class="header-section-number">2.1.4.3</span> Encoder<a href="literature-review.html#encoder" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Wichtig</p>
<p>positional encoding important (and distinguishes from tf-idf): dog eats cat</p>
<p>sinusoidal positional encoding, which uses sine and cosine functions of varying frequencies to create unique positional vectors, and Rotary Position Embedding (RoPE), which applies a rotation to the token embeddings based on their position <span class="citation">(<a href="#ref-khowajaAnalysisLlama4s2025">Khowaja, 2025</a>)</span></p>
</div>
<div id="decoder" class="section level4 hasAnchor" number="2.1.4.4">
<h4><span class="header-section-number">2.1.4.4</span> Decoder<a href="literature-review.html#decoder" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For each generated token, the attention mechanism needs to access the key and value vectors of all preceding tokens in the context window. To avoid recomputing these key and value vectors at each step, they are stored in the KV-cache. <span class="citation">(<a href="#ref-khowajaAnalysisLlama4s2025">Khowaja, 2025</a>)</span> However, the memory required to store the KV-cache scales linearly with the size of the context window.</p>
<p>Wichtig</p>
<p>Token sampling, temperature 0</p>
</div>
<div id="gpt-generative-pretrained-transformers" class="section level4 hasAnchor" number="2.1.4.5">
<h4><span class="header-section-number">2.1.4.5</span> GPT (Generative Pretrained Transformers)<a href="literature-review.html#gpt-generative-pretrained-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>hauptsÃ¤chlich decoder (generieren)</p>
<p>Wichtig</p>
</div>
<div id="mixture-of-experts" class="section level4 hasAnchor" number="2.1.4.6">
<h4><span class="header-section-number">2.1.4.6</span> Mixture of Experts<a href="literature-review.html#mixture-of-experts" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recent <a href="glossary.html#acronyms_LLM">LLM (large language model)</a>s often use a <a href="glossary.html#acronyms_MoE">MoE</a> architecture. The models of Llama 4, Qwen3 and GPT-4.1 are prominent examples for this kind of <a href="glossary.html#acronyms_LLM">LLM</a>s. <span class="citation">D. Zhang et al. (<a href="#ref-zhangMixtureExpertsLarge2025">2025</a>)</span> and <span class="citation">Cai et al. (<a href="#ref-caiSurveyMixtureExperts2025a">2025</a>)</span> give an exhaustive overview of different types of <a href="glossary.html#acronyms_MoE">MoE</a> architectures. While <span class="citation">D. Zhang et al. (<a href="#ref-zhangMixtureExpertsLarge2025">2025</a>)</span> lists also models released this year and shows some applications of <a href="glossary.html#acronyms_MoE">MoE</a>, is <span class="citation">Cai et al. (<a href="#ref-caiSurveyMixtureExperts2025a">2025</a>)</span> discussing different architecture types in more detail. <span class="citation">Grootendorst (<a href="#ref-grootendorstVisualGuideMixture2024">2024</a>)</span> gives a guid to <a href="glossary.html#acronyms_MoE">MoE</a> with many helpful illustrations.</p>
<p>The basic idea of <a href="glossary.html#acronyms_MoE">MoE</a> models is to combine multiple smaller, specialized <a href="glossary.html#acronyms_FFN">FFN (feed forward network)</a>s to achieve better predictions overall. The <a href="glossary.html#acronyms_MoE">MoE</a> âparadigm offers a compelling method to significantly expand model capacity while avoiding a corresponding surge in computational demands during training and inference phasesâ <span class="citation">(<a href="#ref-caiSurveyMixtureExperts2025a">Cai et al., 2025, p. 21</a>)</span>.</p>
<p>Figure <a href="literature-review.html#fig:moe-architecture">2.1</a> shows two main differences in the architecture. One one hand there is the dense (a) architecture. Here, each token is fed into every <a href="glossary.html#acronyms_FFN">FFN</a> and all results are pooled. On the other hand, there is the sparse architecture. Here, each token is just fed into a subset of <a href="glossary.html#acronyms_FFN">FFN</a>s. Dense <a href="glossary.html#acronyms_MoE">MoE</a> models often yield higher prediction accuracy, but also significantly increase the computational overhead <span class="citation">(<a href="#ref-caiSurveyMixtureExperts2025a">Cai et al., 2025</a>)</span>.</p>
<p>The gate (also router) takes care of the distribution of tokens to the <a href="glossary.html#acronyms_FFN">FFN</a>s. There is a high diversity of the routing algorithms and its goals are to âensure expert diversity while minimizing redundant computationâ <span class="citation">(<a href="#ref-zhangMixtureExpertsLarge2025">D. Zhang et al., 2025</a>)</span>. There are algorithms that focus on load-balancing, domain specific routing and many more. Traditional <a href="glossary.html#acronyms_MoE">MoE</a> assumes homogeneous experts, where load balancing might be the paramount goal. Recent advances explore more heterogeneous sets of experts and flexible routing strategies, that promise more efficiency <span class="citation">(<a href="#ref-zhangMixtureExpertsLarge2025">D. Zhang et al., 2025</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:moe-architecture"></span>
<img src="images/moe_architecture.png" alt="Showing schemas of the dense and sparse mixture of experts architecture." width="100%" />
<p class="caption">
Figure 2.1: Showing schemas of the dense and sparse mixture of experts architecture.
</p>
</div>
<p>Most of the Qwen3 models have a dense <a href="glossary.html#acronyms_MoE">MoE</a> architecture. Only the two models released in July 2025 have a sparse architecture. These models have two parameter specifications. For example Qwen3-235B-A22B is specifying that the model has 235B token in total. But per token processed it uses (activates) just 22B parameters. In their mixture of experts architecture this means that 8 of 128 experts are participating in processing each token.</p>
<p>The Llama 4 models have a shared expert <a href="glossary.html#acronyms_MoE">MoE</a> architecture. It combines a shared, fixed expert that processes every token and combines those results with results from a sparse <a href="glossary.html#acronyms_MoE">MoE</a> layer.</p>
<p>Googles gemma-3n-E4B uses a selective parameter activation as well. They use the prefix E for effective instead of A for active <span class="citation">(<a href="#ref-googleGemma3nModel">Google, n.d.</a>)</span>. In gemma-3n there are parameters to handle input of different types - text, vision and audio - and they get loaded and activated as necessary. This allows a multi modal functionality. It additionally caches the <a href="glossary.html#acronyms_PLE">PLE (Per-Layer Embedding)</a> in fast storage (RAM) instead of keeping it in the model memory space (VRAM), allowing to run models in low resource environments.</p>
</div>
<div id="confidence-scores" class="section level4 hasAnchor" number="2.1.4.7">
<h4><span class="header-section-number">2.1.4.7</span> Confidence scores<a href="literature-review.html#confidence-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A <a href="glossary.html#acronyms_LLM">LLM</a> can return a log probability score together with each token it predicts. In fact, it can return the top k candidates for the next token with the corresponding log probabilities. The sum of individual log probabilities indicates how likely a sequence of tokens is, according to the <a href="glossary.html#acronyms_LLM">LLM</a>. This sum can be interpreted as a kind of <em>confidence</em> the model has in a prediction <span class="citation">(<a href="#ref-berkovUnderstandingLLMLogprobs2025">Berkov, 2025</a>)</span>.</p>
<p><span class="citation">Boseak (<a href="#ref-boseakEvaluatingLogLikelihoodConfidence2025">2025</a>)</span> investigate the usage of log probabilities to score multiple choice answers. They show that the sum of log probabilities carries valuable information about model confidence, but its proper use is nuanced. The choice to use the raw sum or a length normalized sum of log probabilities can have a noticeable effect on accuracy and they give no recommandation which approach to use in general. <span class="citation">Ma et al. (<a href="#ref-maEstimatingLLMUncertainty2025">2025</a>)</span> argue that normalizing the probability scores is a reason why probability-based methods fail to identify reliability.</p>
<p><span class="citation">Kang et al. (<a href="#ref-kangScalableBestofNSelection2025">2025</a>)</span> compare different potential measures for a <a href="glossary.html#acronyms_LLM">LLM</a>sâ confidence for Best-of-N Selection tasks. The most simplest of these approaches uses the normalized log probabilities of the returned tokens. The negative exponent of this normalized sum is also called perplexity. It is widely used in LLM evaluations, even though it has been shown to fail in capturing a modelâs ability to understand long context. Nevertheless, <span class="citation">Kang et al. (<a href="#ref-kangScalableBestofNSelection2025">2025</a>)</span> show that perplexity can perform equally good as more sophisticated measures of confidence up to 16 choices.</p>
<p><span class="citation">Kauf et al. (<a href="#ref-kaufLogProbabilitiesAre2024">2024</a>)</span> investigated, if log probabilities can be used to measure semantic plausibility of sentences. They compare pairs of sentences that are similar, but where one is describing a plausible scenario, whereas the other describes a unlikely one. They show, that comparing the sum of returned log probabilities of two sentences yields better results for a semantic plausibility comparison than prompting the model to make this decision explicitly.</p>
</div>
</div>
<div id="llm-methods" class="section level3 hasAnchor" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Generation enhancing methods with LLMs<a href="literature-review.html#llm-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="in-context-learning" class="section level4 hasAnchor" number="2.1.5.1">
<h4><span class="header-section-number">2.1.5.1</span> In-context Learning<a href="literature-review.html#in-context-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Few-shot learning is a prompting strategy that enables <a href="glossary.html#acronyms_LLM">LLM</a>s to solve a wide range of tasks, just by presenting task related examples including the solutions. <span class="citation">Brown et al. (<a href="#ref-brownLanguageModelsAre2020">2020</a>)</span> calls the rapid adaption to the provided task and examples as <em>in-context learning</em>. They show that already GPT-3, as general purpose transformer, surpasses the performance of fine-tuned state-of-the-art models in some tasks.</p>
<p>If the performance gain by <em>in-context learning</em> is investigated, the terms <em>zero-shot</em>, <em>one-shot</em> and <em>few-shot</em> learning are often used. In zero-shot learning we only state the task without providing any examples. In one-shot learning a single example is provided. In few-shot learning multiple examples are provided.</p>
<p>The classical alternative to in-context learning is model fine-tuning, in which a modelâs weights are updated via gradient descent, based on a large corpus of examples <span class="citation">(<a href="#ref-brownLanguageModelsAre2020">Brown et al., 2020</a>)</span>. Through fine-tuning, the resulting model can perform the target task independently, without requiring additional context or descriptive tokens at inference time.</p>
<p>In-context learning reduces the need to create large datasets to fine-tune a model on specific tasks. This brings great flexibility and can be resource efficient by sparing the computational costs for adapting the model to new tasks <span class="citation">(<a href="#ref-dongSurveyIncontextLearning2024">Dong et al., 2024</a>)</span>. On the other hand, it can pay off to fine-tune a model, if a specific task has to be solved regularly, because for <em>in-context learning</em> additional tokens have to be processed every time. This can become energy costly and slows down the task processing.</p>
<p>Text classification and information extraction are two tasks that can be solved with few shot-learning <span class="citation">(<a href="#ref-zhaoCalibrateUseImproving2021">Zhao et al., 2021</a>)</span>. Hereby, the order in which the examples are presented is important - at least for GPT-3 <span class="citation">(<a href="#ref-zhaoCalibrateUseImproving2021">Zhao et al., 2021</a>)</span>.</p>
<p>Although tutorials and blogs sometimes suggest that using more examples leads to better results <span class="citation">(<a href="#ref-kukaShotBasedPromptingZeroShot">Kuka", n.d.</a>)</span>, <span class="citation">Brown et al. (<a href="#ref-brownLanguageModelsAre2020">2020</a>)</span> demonstrate that performance may not improve significantly when adding more than a second example.</p>
<p>Furthermore, there is the threat of <em>context rot</em> <span class="citation">(<a href="#ref-kellyhongContextRotHow2025">Kelly Hong &amp; Anton Troynikov, 2025</a>)</span>. This means, the performance can decrease noticeably, when the context gets to long. Thus, the number of examples one should provide is not just constrained by the context width. Additionally, there is a threat of over-fitting on patterns that are not present in the actual task but in the examples if they are to homogeneous.</p>
</div>
<div id="rag" class="section level4 hasAnchor" number="2.1.5.2">
<h4><span class="header-section-number">2.1.5.2</span> RAG<a href="literature-review.html#rag" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Wichtig</p>
</div>
<div id="guided-and-restricted-decoding" class="section level4 hasAnchor" number="2.1.5.3">
<h4><span class="header-section-number">2.1.5.3</span> Guided and restricted decoding<a href="literature-review.html#guided-and-restricted-decoding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Guided decoding is a prompting technique, where a <a href="glossary.html#acronyms_LLM">LLM</a> is instructed to respond in a specific format, e.g.Â as JSON formatted string. It often is combined with <em>in-context</em> learning, to improve the probability, to get the format requested. One can provide examples of pre-structured templates, too. For JSON one can request certain keys and properties for the corresponding values.</p>
<p>Restricted decoding, on the other hand, is manipulating the</p>
<p><span class="citation">Willard &amp; Louf (<a href="#ref-willardEfficientGuidedGeneration2023">2023</a>)</span> are stating, that they use XYZ? to satisfy formatting requirements that are either hard or costly to capture through fine-tuning alone (or through prompting / few-shot learning), summarizing findings of many articles.</p>
<p>âMost implementations of guided generation bias the score values used to determine the probabilities of the tokens in an LLMâs vocabulary. A common and sufficient approach involves repeated evaluations over the entire vocabulary in order to determine which tokens are validâaccording to the constraints and previously sampled tokensâand setting the probabilities of invalid tokens to zero. This approach entails a fixed O(N ) cost for each token generated, where N is the size of the LLMâs vocabulary.</p>
<p>We propose an approach that uses the finite state machine (FSM) for- mulation of regular expressions to both arbitrarily start and stop guided generation and allow the construction of an index with which the set of non- zero-probability tokens can be obtained efficiently at each step. The result is an algorithm that costs O(1) on average.â</p>
<p>generation template strict (closed) vs open</p>
<p>always selecting the most probable response (temp = 0), so numeric values are correct and classification as well</p>
</div>
</div>
</div>
<div id="other-concepts" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> General machine learning and statistics<a href="literature-review.html#other-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="sample-distribution-visualization-methods" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Sample distribution visualization methods<a href="literature-review.html#sample-distribution-visualization-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="boxplots" class="section level5 hasAnchor" number="2.2.1.0.1">
<h5><span class="header-section-number">2.2.1.0.1</span> Boxplots<a href="literature-review.html#boxplots" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="citation">Wickham &amp; Stryjewski (<a href="#ref-wickham40YearsBoxplots2011">2011</a>)</span> describe boxplots as âa compact distributional summary, displaying less detail than a histogram or kernel density, but also taking up less space. Boxplots use robust summary statistics that are always located at actual data points, are quickly computable (originally by hand), and have no tuning parameters. They are particularly useful for comparing distributions across groups.â</p>
<p>Figure <a href="literature-review.html#fig:boxplot-gauss">2.2</a> shows a box and whiskers plot and its components and compares it to a gaussian probability distribution. Half of all observations fall within the box and the median is marked by a thick line. Outliers are defined as observations that are outside the area marked with the (horizontal) lines -called whiskers - that potentially have small bars at their ends. Outliers can be shown by circles or dots.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:boxplot-gauss"></span>
<img src="images/Boxplot_vs_PDF.png" alt="Showing a box and whiskers plot with its components - median, quartiles, whiskers and outliers - and compare it with a gaussian probability distribution. Graphic adjusted from Jhguch (2025)." width="80%" />
<p class="caption">
Figure 2.2: Showing a box and whiskers plot with its components - median, quartiles, whiskers and outliers - and compare it with a gaussian probability distribution. Graphic adjusted from <span class="citation">Jhguch (<a href="#ref-jhguchBoxPlot2025">2025</a>)</span>.
</p>
</div>
<p>The median and quartiles are less sensitive to outliers, than the mean and standard deviation of a sample. Thus, they are more suitable for distributions that are asymmetric or irregularly shaped and for samples with extreme outliers <span class="citation">(<a href="#ref-krzywinskiVisualizingSamplesBox2014">Krzywinski &amp; Altman, 2014</a>)</span>. They can be used with five observations and more. But even for large samples <span class="math inline">\((n \geq 50)\)</span>, whisker positions can vary greatly.</p>
</div>
<div id="violin-plots" class="section level5 hasAnchor" number="2.2.1.0.2">
<h5><span class="header-section-number">2.2.1.0.2</span> Violin plots<a href="literature-review.html#violin-plots" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>There are variations, that try to communicate the sample size of a box plot, either by adjusting the width of the whole box or by introducing notches, that indicate the confidence interval for the median <span class="citation">(<a href="#ref-wickham40YearsBoxplots2011">Wickham &amp; Stryjewski, 2011</a>)</span>. Violin plots <span class="citation">(<a href="#ref-hintzeViolinPlotsBox1998">Hintze &amp; Nelson, 1998</a>)</span> additionally indicate an density estimate, dropping the strict rectangular shape of the box. Figure <a href="literature-review.html#fig:violinplot">2.3</a> shows, that the shapes can be necessary to identify multi-modal distributions, that are invisible with regular boxplots <span class="citation">(<a href="#ref-wickham40YearsBoxplots2011">Wickham &amp; Stryjewski, 2011</a>)</span>. One can tackle this problem by adding a jitter plot layer to the boxplots. Violin plots can also be used for large datasets, preventing to plot a lot of outliers.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:violinplot"></span>
<img src="images/violin_plots_horizontal.png" alt="Comparing boxplots and violinplots, showing that boxplots can not identify multi-modal distributions on their own. Graphic adjusted from Amgen Scholars Program (n.d.)." width="100%" />
<p class="caption">
Figure 2.3: Comparing boxplots and violinplots, showing that boxplots can not identify multi-modal distributions on their own. Graphic adjusted from <span class="citation">Amgen Scholars Program (<a href="#ref-amgenscholarsprogramHowInterpretViolin">n.d.</a>)</span>.
</p>
</div>
</div>
</div>
<div id="tree-based-machine-learning-algorithms" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Tree based machine learning algorithms<a href="literature-review.html#tree-based-machine-learning-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random forests are an ensemble supervised machine learning technique, composed of multiple decision trees <span class="citation">(<a href="#ref-kulkarniRandomForestClassifiers2013">V. Kulkarni &amp; Sinha, 2013</a>)</span>. <span class="citation">Mienye &amp; Jere (<a href="#ref-mienyeSurveyDecisionTrees2024">2024</a>)</span> give a detailed insight into decision trees and their high-performing ensemble algorithms. Tree based machine learning algorithms have gained significant popularity, due to their simplicity and good interpretability <span class="citation">(<a href="#ref-mienyeSurveyDecisionTrees2024">Mienye &amp; Jere, 2024</a>)</span>.</p>
<div id="decision-tree" class="section level5 hasAnchor" number="2.2.2.0.1">
<h5><span class="header-section-number">2.2.2.0.1</span> Decision tree<a href="literature-review.html#decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>âThe basic idea behind decision tree-based algorithms is that they recursively partition the data into subsets based on the values of different attributes until a stopping criterion is metâ <span class="citation">(<a href="#ref-mienyeSurveyDecisionTrees2024">Mienye &amp; Jere, 2024</a>)</span>. Figure <a href="literature-review.html#fig:decision-tree">2.4</a> shows this for artificial data of two continuous features. Popular measures to determine how to split a set of observations are the Gini index, information gain or information gain criteria <span class="citation">(<a href="#ref-mienyeSurveyDecisionTrees2024">Mienye &amp; Jere, 2024</a>)</span>.</p>
<p>The tree shown is used for a regression task and will predict the average of all values of the corresponding terminal node (leaf). To find out which leaf will be the target terminal node for a given set of features, one simply follows the path from the top node (root) downwards, checking the splitting criteria. Thus, the interpretation of decisions made by a decision tree is straightforward and highly transparent.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decision-tree"></span>
<img src="images/tree_simple.png" alt="Visualizing the of partitioning of a two-dimensional continuous feature space based on multiple splitting criteria for decision tree inducing. Graphic adjusted from Molnar (2025)." width="80%" />
<p class="caption">
Figure 2.4: Visualizing the of partitioning of a two-dimensional continuous feature space based on multiple splitting criteria for decision tree inducing. Graphic adjusted from <span class="citation">Molnar (<a href="#ref-molnarInterpretableMachineLearning2025">2025</a>)</span>.
</p>
</div>
<p>Further benefits of decision trees - besides the good interpretability and computational efficiency - are the native capturing of interactions between features <span class="citation">(<a href="#ref-molnarInterpretableMachineLearning2025">Molnar, 2025</a>)</span>, without modeling this explicitly, as it would for example be necessary in a linear regression. Decision trees can be used for classification and regression. They even can incorporate linear functions as leafs, enabling them to better capture linear relationships <span class="citation">(<a href="#ref-raymaekersFastLinearModel2024">Raymaekers et al., 2024</a>)</span>.</p>
<p>Problems of decision trees are, that they lack resilience against data changes and have a tendency to overfitting. A method against overfitting is pruning <span class="citation">(<a href="#ref-mienyeSurveyDecisionTrees2024">Mienye &amp; Jere, 2024</a>)</span>. Building an ensemble of decision trees is another possibility, that results in the random forest algorithm, described in the next paragraph .</p>
<p><span class="citation">Rivera-Lopez et al. (<a href="#ref-rivera-lopezInductionDecisionTrees2022">2022</a>)</span> focus on decision trees and describe several types, including distinctions based on the splitting procedure (see Figure <a href="literature-review.html#fig:advanced-tree-splitting">2.5</a>). In addition to axis-parallel splitting, they present oblique and non-linear splitting criteria, and provide a state-of-the-art review along with a summary analysis of metaheuristics-based approaches for decision tree induction.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:advanced-tree-splitting"></span>
<img src="images/tree_types.jpg" alt="Visualizing the of partitioning of a two-dimensional continuous feature space based on multiple splitting criteria for decision tree inducing. Graphic adjusted from Rivera-Lopez et al. (2022)." width="100%" />
<p class="caption">
Figure 2.5: Visualizing the of partitioning of a two-dimensional continuous feature space based on multiple splitting criteria for decision tree inducing. Graphic adjusted from <span class="citation">Rivera-Lopez et al. (<a href="#ref-rivera-lopezInductionDecisionTrees2022">2022</a>)</span>.
</p>
</div>
</div>
<div id="random-forest" class="section level5 hasAnchor" number="2.2.2.0.2">
<h5><span class="header-section-number">2.2.2.0.2</span> Random forest<a href="literature-review.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A random forest is using the principle of bagging and applies it on the level of features and observations. This means, it starts, creating basic decision trees with differing subsets of features and uses bootstrapping to select a randomized set of observations to train the tree with. The final prediction is then determined by voting (for classification) or averaging (for regression) the predictions of all trees in the ensemble.</p>
<p>The induction of the trees can be be parallelized, making it efficient on modern hardware. Random forests can cope with thousands of features and can be applied to large datasets <span class="citation">(<a href="#ref-breimanRandomForests2001">Breiman, 2001</a>)</span>. There are methods that address the problems of imbalanced datasets too. As there are methods to prune a decision tree to fight overfitting, there are methods to prune a random forest by removing whole trees, to improve the learning and classification performance, too <span class="citation">(<a href="#ref-kulkarniPruningRandomForest2012">V. Y. Kulkarni &amp; Sinha, 2012</a>)</span>.</p>
<p>Random forests are âpowerful learning ensemble[s] given [their] predictive performance, flexibility, and ease of useâ <span class="citation">(<a href="#ref-haddouchiSurveyTaxonomyMethods2024">Haddouchi &amp; Berrado, 2024</a>)</span>. While it is based on decision trees, that are considered to be <em>white boxes</em>, because of their easy interpretability, random forests are seen as <em>black boxes</em>. The decision could be tracked without complicated math, but is tedious, because it would require propagating through many decision trees, noting their predictions and then averaging those.</p>
<p>The fact that the random forest model is categorized as a black-box model restricts its deployment in many fields of application <span class="citation">(<a href="#ref-haddouchiSurveyTaxonomyMethods2024">Haddouchi &amp; Berrado, 2024</a>)</span>. One feature oriented tool for explainability is the <a href="glossary.html#acronyms_SHAP">SHAP</a> framework, presented in section <a href="literature-review.html#shap">2.2.3</a>. It allows local explanation, a global overview and pattern discovery for random forests <span class="citation">(<a href="#ref-haddouchiSurveyTaxonomyMethods2024">Haddouchi &amp; Berrado, 2024</a>)</span>.</p>
</div>
<div id="gradient-boosted-decision-trees" class="section level5 hasAnchor" number="2.2.2.0.3">
<h5><span class="header-section-number">2.2.2.0.3</span> Gradient boosted decision trees<a href="literature-review.html#gradient-boosted-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Another highly effective and widely used advancement to decision trees are gradient boosted decision trees <span class="citation">(<a href="#ref-chenXGBoostScalableTree2016">Chen &amp; Guestrin, 2016</a>)</span>. Instead of the principle of bagging it applies the principle of boosting. It is sequentially building decision trees, where the later ones correct the errors in predictions made by the former trees. It uses gradient descent to minimize those errors <span class="citation">(<a href="#ref-mienyeSurveyDecisionTrees2024">Mienye &amp; Jere, 2024</a>)</span>.</p>
<p>The <a href="glossary.html#acronyms_XGBoost">XGBoost (Extreme Gradient Boosting)</a> algorithm is a famous member of this family, with âan outstanding performance recordâ <span class="citation">(<a href="#ref-burnwalComprehensiveSurveyPrediction2023">Burnwal &amp; Jaiswal, 2023</a>)</span>. âAmong the 29 challenge winning solutions published at Kaggleâs blog during 2015, 17 solutions used XGBoostâ <span class="citation">(<a href="#ref-chenXGBoostScalableTree2016">Chen &amp; Guestrin, 2016</a>)</span>.</p>
<p>In the following we will emphasize some of its benefits as described by <span class="citation">Burnwal &amp; Jaiswal (<a href="#ref-burnwalComprehensiveSurveyPrediction2023">2023</a>)</span>:</p>
<ul>
<li><p><a href="glossary.html#acronyms_XGBoost">XGBoost</a> employs both L1 (Lasso) and L2 (Ridge) regularization in its objective function to penalize model complexity, mitigating overfitting. However, overfitting can occur, especially if hyperparameters are not adjusted properly.</p></li>
<li><p><a href="glossary.html#acronyms_XGBoost">XGBoost</a> provides feature importance metrics to facilitate model interpretation, facilitating feature selection and improving the understanding of the modelâs decision-making process.</p></li>
<li><p><a href="glossary.html#acronyms_XGBoost">XGBoost</a> âruns more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited settingsâ <span class="citation">(<a href="#ref-chenXGBoostScalableTree2016">Chen &amp; Guestrin, 2016</a>)</span> using parallelization techniques.</p></li>
</ul>
<p>But there are some challenges, that are to investigate in future research. E.g. finding methods to handle imbalanced data and automate the hyperparameter tuning process.</p>
</div>
</div>
<div id="shap" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Model agnostic explanation models<a href="literature-review.html#shap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="shapley-values" class="section level5 hasAnchor" number="2.2.3.0.1">
<h5><span class="header-section-number">2.2.3.0.1</span> Shapley values<a href="literature-review.html#shapley-values" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Shapley values are introduced by <span class="citation">Shapley (<a href="#ref-shapley17ValueNPerson2016">2016</a>)</span> originally in 1952 in the field of game theory. He defined three axioms that a fair allocation of rewards must fulfill:</p>
<ol style="list-style-type: decimal">
<li><p>Symmetry: If two players contribute the same amount, they are interchangeable and should gain equal reward.</p></li>
<li><p>Efficiency: The whole value of the game is distributed among the players.</p></li>
<li><p>Law of aggregation: If a player contributes to multiple independent games, his contribution in total should be the sum of contributions in each game.</p></li>
</ol>
<p>From the third axiom a fourth property derives, that is sometimes named independently. If a player is not contributing to a game, he gets no share. <span class="citation">OâSullivan (<a href="#ref-osullivanMathematicsShapleyValues2023">2023</a>)</span> calls this the <em>null player</em> property.</p>
<p>The formula for a single shapley values is given by <span class="citation">(<a href="#ref-lundbergUnifiedApproachInterpreting2017">S. Lundberg &amp; Lee, 2017</a>)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<p><span class="math display" id="eq:shapley">\[\begin{equation}
\phi_{i} = \sum_{S \subseteq P \setminus \{i\}}{\frac{|S|! \left( |P|-|S|-1 \right) !}{|P|!} \left[ val \left( S \cup \{i\} \right) - val\left(S\right) \right]}
\tag{2.4}
\end{equation}\]</span></p>
<p><span class="citation">Molnar (<a href="#ref-molnarInterpretableMachineLearning2025">2025</a>)</span> bridges the game theory terms to the field of machine learning as follows: âThe <em>game</em> is the prediction task for a single instance of the dataset. The <em>gain</em> is the actual prediction for this instance minus the average prediction for all instances. The <em>players</em> are the feature values of the instance that collaborate to receive the gain (= predict a certain value).â</p>
</div>
<div id="shap-framework" class="section level5 hasAnchor" number="2.2.3.0.2">
<h5><span class="header-section-number">2.2.3.0.2</span> SHAP framework<a href="literature-review.html#shap-framework" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In <span class="citation">S. Lundberg &amp; Lee (<a href="#ref-lundbergUnifiedApproachInterpreting2017">2017</a>)</span> they present âA Unified Approach to Interpreting Model Predictionsâ based on shapley values, called <a href="glossary.html#acronyms_SHAP">SHAP (SHapley Additive exPlanations)</a>. It assigns each feature an importance value for every observation. This allows to inspect, why a specific prediction is made and might explain, why a model makes a mistake for specific observations. Inspecting the predictions for all observations can show generalized effects of features.</p>
<p><span class="citation">S. Lundberg &amp; Lee (<a href="#ref-lundbergUnifiedApproachInterpreting2017">2017</a>)</span> show that their approach is the only possible explanation model for the class of additive feature attribution methods, that has three desirable characteristics: local accuracy, missingness and consistency. Shapely values can be computed for any machine learning model, but its exact calculation is computationally extremely expensive <span class="citation">(<a href="#ref-huComputingSHAPEfficiently2023">Hu &amp; Wang, 2023</a>)</span>, since it is of exponential complexity <span class="math inline">\(\mathcal{O}(2^p)\)</span> regarding the number of features (or predictors) <span class="math inline">\(p = |P|\)</span>.</p>
<p>Even with the approximation of the shapley values, introduced in <span class="citation">S. Lundberg &amp; Lee (<a href="#ref-lundbergUnifiedApproachInterpreting2017">2017</a>)</span> as Kernel SHAP, the complexity for tree based algorithms is <span class="math inline">\(\mathcal{O}(MTL2^p)\)</span>, with the number of samples <span class="math inline">\(M\)</span>, number of trees <span class="math inline">\(T\)</span> and the number of leaves <span class="math inline">\(L\)</span>. The tree based optimization of the algorithm, TreeSHAP, allows an approximation in <span class="math inline">\(\mathcal{O}(MTLD^2)\)</span> <span class="citation">(<a href="#ref-lundbergExplainableAITrees2019">S. M. Lundberg et al., 2019</a>)</span>, with the maximum tree depth <span class="math inline">\(D\)</span>. Depending on the number of observations to calculate shapley values for (<span class="math inline">\(M\)</span>), the Fast TreeSHAP algorithm has a even lower time complexity of <span class="math inline">\(\mathcal{O}(TLD2^D+MTLD)\)</span> <span class="citation">(<a href="#ref-yangFastTreeSHAPAccelerating2022">Yang, 2022</a>)</span>.</p>
<p>To quantify the overall effect of a feature on the model, we compute the mean of the absolute Shapley values for that feature across all observations. Here is the formula, which we adapted from <span class="citation">Molnar (<a href="#ref-molnarInterpretableMachineLearning2025">2025</a>)</span> by adjusting the notation and variable names to match Equation <a href="literature-review.html#eq:shapley">(2.4)</a>:</p>
<p><span class="math display" id="eq:meanSHAP">\[\begin{equation}
mean\left(|SHAP|\right) = \frac{1}{n} \sum_{k=1}^n | \phi_{i}^{\left( k \right)} |
\tag{2.5}
\end{equation}\]</span></p>
<p>This value is called <a href="glossary.html#acronyms_SHAP">SHAP</a> feature importance. It can be interpreted similar to standardized beta values for a linear regression. In some cases it would be possible to calculate an effect direction for the feature importance. But it is not common practice. Instead visual representations presented in section @ref() are used for such interpretations.</p>
<p><span class="citation">S. Lundberg &amp; Lee (<a href="#ref-lundbergUnifiedApproachInterpreting2017">2017</a>)</span> also showed that SHAP values are more consistent with human intuition than preceding local explainable models. <span class="citation">Z. Li et al. (<a href="#ref-liBuildingTrustMachine2024">2024</a>)</span> mention, that explainability of machine learning models is not only important for researches but also for practitioners, to demonstrate their reliability to potential users and build trust. Regardless of the popularity of <a href="glossary.html#acronyms_SHAP">SHAP</a> scores, there are claims that they can be inadequate as a measure of feature importance <span class="citation">(<a href="#ref-huangFailingsShapleyValues2024">Huang &amp; Marques-Silva, 2024</a>)</span>. The approximated as well as exact <a href="glossary.html#acronyms_SHAP">SHAP</a> scores can assign higher value to unimportant features than to important ones.</p>
<p>However, the need for a high explainability of machine learning algorithms is more urgent than ever, since the EUâs regulatory ecosystem is emphasizing the importance of <a href="glossary.html#acronyms_XAI">XAI (explainable artificial intelligence)</a> <span class="citation">(<a href="#ref-nanniniOperationalizingExplainableArtificial2024">Nannini et al., 2024</a>)</span>.</p>
</div>
</div>
</div>
<div id="summary-1" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Summary<a href="literature-review.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter highlighted recent architectures for <a href="glossary.html#acronyms_LLM">LLM</a>s, such as Mixture-of-Experts, and provided references for readers to explore foundational concepts and the history of <a href="glossary.html#acronyms_NLP">NLP</a> in greater detail. it presented <em>in-context learning</em> and retriever augmentation as advanced prompting strategies benchmarked in our experiments.</p>
<p>The review underscored the importance of explainability in machine learning, particularly through frameworks like SHAP, and discussed visualization techniques such as boxplots and violin plots for understanding data distributions.</p>
<p>The discussed methods and concepts directly inform the thesisâs approach to extracting financial information from unstructured documents. By presenting advanced LLM architectures and explainability tools, the chapter establishes a foundation for the experimental strategies and evaluation criteria used throughout the thesis.</p>
<p>Building on these insights, the following chapter details the methodology adopted for applying and evaluating these techniques in the context of financial report analysis, setting the stage for the experimental investigations and results.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-adhikariComparativeStudyPDF2024" class="csl-entry">
Adhikari, N. S., &amp; Agarwal, S. (2024). <em>A <span>Comparative Study</span> of <span>PDF Parsing Tools Across Diverse Document Categories</span></em> (arXiv:2410.09871). arXiv. <a href="https://doi.org/10.48550/arXiv.2410.09871">https://doi.org/10.48550/arXiv.2410.09871</a>
</div>
<div id="ref-amgenscholarsprogramHowInterpretViolin" class="csl-entry">
Amgen Scholars Program. (n.d.). <em>How to <span>Interpret Violin Charts</span></em>. https://www.labxchange.org/library/items/lb:LabXchange:46f64d7a:html:1.
</div>
<div id="ref-berkovUnderstandingLLMLogprobs2025" class="csl-entry">
Berkov, M. (2025). Understanding <span>LLM Logprobs</span>. In <em>Thinking Sand</em>.
</div>
<div id="ref-blecherNougatNeuralOptical2023" class="csl-entry">
Blecher, L., Cucurull, G., Scialom, T., &amp; Stojnic, R. (2023). <em>Nougat: <span>Neural Optical Understanding</span> for <span>Academic Documents</span></em> (arXiv:2308.13418). arXiv. <a href="https://doi.org/10.48550/arXiv.2308.13418">https://doi.org/10.48550/arXiv.2308.13418</a>
</div>
<div id="ref-boseakEvaluatingLogLikelihoodConfidence2025" class="csl-entry">
Boseak, C. (2025). Evaluating <span>Log-Likelihood</span> for <span>Confidence Estimation</span> in <span>LLM-Based Multiple-Choice Question Answering</span>. <em>Innovative Journal of Applied Science</em>, <em>02</em>(04). <a href="https://doi.org/10.70844/ijas.2025.2.29">https://doi.org/10.70844/ijas.2025.2.29</a>
</div>
<div id="ref-breimanRandomForests2001" class="csl-entry">
Breiman, L. (2001). Random <span>Forests</span>. <em>Machine Learning</em>, <em>45</em>(1), 5â32. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry">
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., â¦ Amodei, D. (2020). <em>Language <span>Models</span> are <span>Few-Shot Learners</span></em> (arXiv:2005.14165). arXiv. <a href="https://doi.org/10.48550/arXiv.2005.14165">https://doi.org/10.48550/arXiv.2005.14165</a>
</div>
<div id="ref-burnwalComprehensiveSurveyPrediction2023" class="csl-entry">
Burnwal, Y., &amp; Jaiswal, Dr. R. C. (2023). A <span>Comprehensive Survey</span> on <span>Prediction Models</span> and the <span>Impact</span> of <span>XGBoost</span>. <em>International Journal for Research in Applied Science and Engineering Technology</em>, <em>11</em>(12), 1552â1556. <a href="https://doi.org/10.22214/ijraset.2023.57625">https://doi.org/10.22214/ijraset.2023.57625</a>
</div>
<div id="ref-caiSurveyMixtureExperts2025a" class="csl-entry">
Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S., &amp; Huang, J. (2025). A <span>Survey</span> on <span>Mixture</span> of <span>Experts</span> in <span>Large Language Models</span>. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 1â20. <a href="https://doi.org/10.1109/TKDE.2025.3554028">https://doi.org/10.1109/TKDE.2025.3554028</a>
</div>
<div id="ref-carvalhoTFIDFCRFNovelSupervised2020" class="csl-entry">
Carvalho, F., &amp; Guedes, G. P. (2020). <em><span>TF-IDFC-RF</span>: <span>A Novel Supervised Term Weighting Scheme</span></em> (arXiv:2003.07193). arXiv. <a href="https://doi.org/10.48550/arXiv.2003.07193">https://doi.org/10.48550/arXiv.2003.07193</a>
</div>
<div id="ref-chenXGBoostScalableTree2016" class="csl-entry">
Chen, T., &amp; Guestrin, C. (2016). <span>XGBoost</span>: <span>A Scalable Tree Boosting System</span>. <em>Proceedings of the 22nd <span>ACM SIGKDD International Conference</span> on <span>Knowledge Discovery</span> and <span>Data Mining</span></em>, 785â794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>
</div>
<div id="ref-dongSurveyIncontextLearning2024" class="csl-entry">
Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., Sun, X., Li, L., &amp; Sui, Z. (2024). A <span>Survey</span> on <span class="nocase">In-context Learning</span>. In Y. Al-Onaizan, M. Bansal, &amp; Y.-N. Chen (Eds.), <em>Proceedings of the 2024 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span></em> (pp. 1107â1128). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.emnlp-main.64">https://doi.org/10.18653/v1/2024.emnlp-main.64</a>
</div>
<div id="ref-francisComparisonStudyOptical2025" class="csl-entry">
Francis, Sofi. A., &amp; Sangeetha, M. (2025). A comparison study on optical character recognition models in mathematical equations and in any language. <em>Results in Control and Optimization</em>, <em>18</em>, 100532. <a href="https://doi.org/10.1016/j.rico.2025.100532">https://doi.org/10.1016/j.rico.2025.100532</a>
</div>
<div id="ref-googleGemma3nModel" class="csl-entry">
Google. (n.d.). Gemma 3n model overview. In <em>Google AI for Developers</em>. https://ai.google.dev/gemma/docs/gemma-3n.
</div>
<div id="ref-grootendorstVisualGuideMixture2024" class="csl-entry">
Grootendorst, M. (2024). <em>A <span>Visual Guide</span> to <span>Mixture</span> of <span>Experts</span> (<span>MoE</span>)</em>. https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts.
</div>
<div id="ref-haddouchiSurveyTaxonomyMethods2024" class="csl-entry">
Haddouchi, M., &amp; Berrado, A. (2024). <em>A survey and taxonomy of methods interpreting random forest models</em> (arXiv:2407.12759). arXiv. <a href="https://doi.org/10.48550/arXiv.2407.12759">https://doi.org/10.48550/arXiv.2407.12759</a>
</div>
<div id="ref-hintzeViolinPlotsBox1998" class="csl-entry">
Hintze, J. L., &amp; Nelson, R. D. (1998). Violin <span>Plots</span>: <span>A Box Plot-Density Trace Synergism</span>. <em>The American Statistician</em>, <em>52</em>(2), 181â184. <a href="https://doi.org/10.1080/00031305.1998.10480559">https://doi.org/10.1080/00031305.1998.10480559</a>
</div>
<div id="ref-huComputingSHAPEfficiently2023" class="csl-entry">
Hu, L., &amp; Wang, K. (2023). <em>Computing <span>SHAP Efficiently Using Model Structure Information</span></em> (arXiv:2309.02417). arXiv. <a href="https://doi.org/10.48550/arXiv.2309.02417">https://doi.org/10.48550/arXiv.2309.02417</a>
</div>
<div id="ref-huangFailingsShapleyValues2024" class="csl-entry">
Huang, X., &amp; Marques-Silva, J. (2024). On the failings of <span>Shapley</span> values for explainability. <em>International Journal of Approximate Reasoning</em>, <em>171</em>, 109112. <a href="https://doi.org/10.1016/j.ijar.2023.109112">https://doi.org/10.1016/j.ijar.2023.109112</a>
</div>
<div id="ref-jhguchBoxPlot2025" class="csl-entry">
Jhguch. (2025). Box plot. <em>Wikipedia</em>.
</div>
<div id="ref-kangScalableBestofNSelection2025" class="csl-entry">
Kang, Z., Zhao, X., &amp; Song, D. (2025). <em>Scalable <span class="nocase">Best-of-N Selection</span> for <span>Large Language Models</span> via <span>Self-Certainty</span></em> (arXiv:2502.18581). arXiv. <a href="https://doi.org/10.48550/arXiv.2502.18581">https://doi.org/10.48550/arXiv.2502.18581</a>
</div>
<div id="ref-kaufLogProbabilitiesAre2024" class="csl-entry">
Kauf, C., Chersoni, E., Lenci, A., Fedorenko, E., &amp; Ivanova, A. A. (2024). <em>Log <span>Probabilities Are</span> a <span>Reliable Estimate</span> of <span>Semantic Plausibility</span> in <span>Base</span> and <span>Instruction-Tuned Language Models</span></em> (arXiv:2403.14859). arXiv. <a href="https://doi.org/10.48550/arXiv.2403.14859">https://doi.org/10.48550/arXiv.2403.14859</a>
</div>
<div id="ref-kellyhongContextRotHow2025" class="csl-entry">
Kelly Hong, &amp; Anton Troynikov. (2025). <em>Context <span>Rot</span>: <span>How Increasing Input Tokens Impacts LLM Performance</span></em>. https://research.trychroma.com/context-rot.
</div>
<div id="ref-khowajaAnalysisLlama4s2025" class="csl-entry">
Khowaja, S. A. (2025). Analysis of <span>Llama</span> 4âs 10 <span>Million Token Context Window Claim</span>. In <em>Medium</em>.
</div>
<div id="ref-krzywinskiVisualizingSamplesBox2014" class="csl-entry">
Krzywinski, M., &amp; Altman, N. (2014). Visualizing samples with box plots. <em>Nature Methods</em>, <em>11</em>(2), 119â120. <a href="https://doi.org/10.1038/nmeth.2813">https://doi.org/10.1038/nmeth.2813</a>
</div>
<div id="ref-kukaShotBasedPromptingZeroShot" class="csl-entry">
Kuka", "Valeriia. (n.d.). <em>Shot-<span>Based Prompting</span>: <span>Zero-Shot</span>, <span>One-Shot</span>, and <span>Few-Shot Prompting</span></em>. https://learnprompting.org/docs/basics/few_shot.
</div>
<div id="ref-kulkarniPruningRandomForest2012" class="csl-entry">
Kulkarni, V. Y., &amp; Sinha, P. K. (2012). Pruning of <span>Random Forest</span> classifiers: <span>A</span> survey and future directions. <em>2012 <span>International Conference</span> on <span>Data Science</span> &amp; <span>Engineering</span> (<span>ICDSE</span>)</em>, 64â68. <a href="https://doi.org/10.1109/ICDSE.2012.6282329">https://doi.org/10.1109/ICDSE.2012.6282329</a>
</div>
<div id="ref-kulkarniRandomForestClassifiers2013" class="csl-entry">
Kulkarni, V., &amp; Sinha, P. (2013). Random forest classifiers: <span>A</span> survey and future research directions. <em>International Journal of Advanced Computing</em>, <em>36</em>, 1144â1153.
</div>
<div id="ref-liBuildingTrustMachine2024" class="csl-entry">
Li, Z., Bouazizi, M., Ohtsuki, T., Ishii, M., &amp; Nakahara, E. (2024). Toward <span>Building Trust</span> in <span>Machine Learning Models</span>: <span>Quantifying</span> the <span>Explainability</span> by <span>SHAP</span> and <span>References</span> to <span>Human Strategy</span>. <em>IEEE Access</em>, <em>12</em>, 11010â11023. <a href="https://doi.org/10.1109/ACCESS.2023.3347796">https://doi.org/10.1109/ACCESS.2023.3347796</a>
</div>
<div id="ref-lundbergExplainableAITrees2019" class="csl-entry">
Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., &amp; Lee, S.-I. (2019). <em>Explainable <span>AI</span> for <span>Trees</span>: <span>From Local Explanations</span> to <span>Global Understanding</span></em>. arXiv. <a href="https://doi.org/10.48550/ARXIV.1905.04610">https://doi.org/10.48550/ARXIV.1905.04610</a>
</div>
<div id="ref-lundbergUnifiedApproachInterpreting2017" class="csl-entry">
Lundberg, S., &amp; Lee, S.-I. (2017). <em>A <span>Unified Approach</span> to <span>Interpreting Model Predictions</span></em> (arXiv:1705.07874). arXiv. <a href="https://doi.org/10.48550/arXiv.1705.07874">https://doi.org/10.48550/arXiv.1705.07874</a>
</div>
<div id="ref-maEstimatingLLMUncertainty2025" class="csl-entry">
Ma, H., Chen, J., Zhou, J. T., Wang, G., &amp; Zhang, C. (2025). <em>Estimating <span>LLM Uncertainty</span> with <span>Evidence</span></em> (arXiv:2502.00290). arXiv. <a href="https://doi.org/10.48550/arXiv.2502.00290">https://doi.org/10.48550/arXiv.2502.00290</a>
</div>
<div id="ref-mahmudiNaturalLanguageProcessing2024" class="csl-entry">
Mahmudi, A. (22 Dec 2024 6:46pm). Natural <span>Language Processing</span>. In <em>School of Computing and Information Systems</em>. https://cis.unimelb.edu.au/research/artificial-intelligence/research/Natural-Language-Processing.
</div>
<div id="ref-manningIntroductionInformationRetrieval2008" class="csl-entry">
Manning, C. D., Raghavan, P., &amp; SchÃ¼tze, H. (2008). Introduction to <span>Information Retrieval</span>. In <em>Cambridge Aspire website</em>. https://www.cambridge.org/highereducation/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C; Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511809071">https://doi.org/10.1017/CBO9780511809071</a>
</div>
<div id="ref-mienyeSurveyDecisionTrees2024" class="csl-entry">
Mienye, I. D., &amp; Jere, N. (2024). A <span>Survey</span> of <span>Decision Trees</span>: <span>Concepts</span>, <span>Algorithms</span>, and <span>Applications</span>. <em>IEEE Access</em>, <em>12</em>, 86716â86727. <a href="https://doi.org/10.1109/ACCESS.2024.3416838">https://doi.org/10.1109/ACCESS.2024.3416838</a>
</div>
<div id="ref-minaeeLargeLanguageModels2025" class="csl-entry">
Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., &amp; Gao, J. (2025). <em>Large <span>Language Models</span>: <span>A Survey</span></em> (arXiv:2402.06196). arXiv. <a href="https://doi.org/10.48550/arXiv.2402.06196">https://doi.org/10.48550/arXiv.2402.06196</a>
</div>
<div id="ref-molnarInterpretableMachineLearning2025" class="csl-entry">
Molnar, C. (2025). <em>Interpretable machine learning: A guide for making black box models explainable</em> (Third edition). Christoph Molnar.
</div>
<div id="ref-nanniniOperationalizingExplainableArtificial2024" class="csl-entry">
Nannini, L., Alonso-Moral, J. M., CatalÃ¡, A., Lama, M., &amp; Barro, S. (2024). Operationalizing <span>Explainable Artificial Intelligence</span> in the <span>European Union Regulatory Ecosystem</span>. <em>IEEE Intelligent Systems</em>, <em>39</em>(4), 37â48. <a href="https://doi.org/10.1109/MIS.2024.3383155">https://doi.org/10.1109/MIS.2024.3383155</a>
</div>
<div id="ref-osullivanMathematicsShapleyValues2023" class="csl-entry">
OâSullivan, C. (2023). <em>The mathematics behind <span>Shapley Values</span></em>.
</div>
<div id="ref-paulStateoftheArtModelArchitectures2025" class="csl-entry">
Paul, R. (2025). <em>State-of-the-<span>Art Model Architectures</span> for <span>Document Layout Analysis</span></em>. https://www.rohan-paul.com/p/state-of-the-art-model-architectures.
</div>
<div id="ref-qinLargeLanguageModels2025" class="csl-entry">
Qin, L., Chen, Q., Feng, X., Wu, Y., Zhang, Y., Li, Y., Li, M., Che, W., &amp; Yu, P. S. (2025). <em>Large <span>Language Models Meet NLP</span>: <span>A Survey</span></em> (arXiv:2405.12819). arXiv. <a href="https://doi.org/10.48550/arXiv.2405.12819">https://doi.org/10.48550/arXiv.2405.12819</a>
</div>
<div id="ref-rajExploringNewApproaches2025" class="csl-entry">
Raj, M., &amp; Mishra, N. (2025). <em>Exploring new <span>Approaches</span> for <span>Information Retrieval</span> through <span>Natural Language Processing</span></em> (arXiv:2505.02199). arXiv. <a href="https://doi.org/10.48550/arXiv.2505.02199">https://doi.org/10.48550/arXiv.2505.02199</a>
</div>
<div id="ref-raschkaInstructionPretrainingLLMs2025" class="csl-entry">
Raschka, S. (2025). <em>Instruction <span>Pretraining LLMs</span></em>. https://magazine.sebastianraschka.com/p/instruction-pretraining-llms.
</div>
<div id="ref-rathiImportanceTermWeighting2023" class="csl-entry">
Rathi, R. N., &amp; Mustafi, A. (2023). The importance of <span>Term Weighting</span> in semantic understanding of text: <span>A</span> review of techniques. <em>Multimedia Tools and Applications</em>, <em>82</em>(7), 9761â9783. <a href="https://doi.org/10.1007/s11042-022-12538-3">https://doi.org/10.1007/s11042-022-12538-3</a>
</div>
<div id="ref-raymaekersFastLinearModel2024" class="csl-entry">
Raymaekers, J., Rousseeuw, P. J., Verdonck, T., &amp; Yao, R. (2024). Fast <span>Linear Model Trees</span> by <span>PILOT</span>. <em>Machine Learning</em>, <em>113</em>(9), 6561â6610. <a href="https://doi.org/10.1007/s10994-024-06590-3">https://doi.org/10.1007/s10994-024-06590-3</a>
</div>
<div id="ref-rivera-lopezInductionDecisionTrees2022" class="csl-entry">
Rivera-Lopez, R., Canul-Reich, J., Mezura-Montes, E., &amp; Cruz-ChÃ¡vez, M. A. (2022). Induction of decision trees as classification models through metaheuristics. <em>Swarm and Evolutionary Computation</em>, <em>69</em>, 101006. <a href="https://doi.org/10.1016/j.swevo.2021.101006">https://doi.org/10.1016/j.swevo.2021.101006</a>
</div>
<div id="ref-robertsonUnderstandingInverseDocument2004" class="csl-entry">
Robertson, S. (2004). Understanding inverse document frequency: On theoretical arguments for <span>IDF</span>. <em>Journal of Documentation</em>, <em>60</em>(5), 503â520. <a href="https://doi.org/10.1108/00220410410560582">https://doi.org/10.1108/00220410410560582</a>
</div>
<div id="ref-robertsonSimpleEffectiveApproximations1994a" class="csl-entry">
Robertson, S. E., &amp; Walker, S. (1994). Some <span>Simple Effective Approximations</span> to the 2-<span>Poisson Model</span> for <span>Probabilistic Weighted Retrieval</span>. In B. W. Croft &amp; C. J. Van Rijsbergen (Eds.), <em><span>SIGIR</span> â94</em> (pp. 232â241). Springer London. <a href="https://doi.org/10.1007/978-1-4471-2099-5_24">https://doi.org/10.1007/978-1-4471-2099-5_24</a>
</div>
<div id="ref-robertsonProbabilisticRelevanceFramework2009" class="csl-entry">
Robertson, S., &amp; Zaragoza, H. (2009). The <span>Probabilistic Relevance Framework</span>: <span>BM25</span> and <span>Beyond</span>. <em>Foundations and Trends in Information Retrieval</em>, <em>3</em>, 333â389. <a href="https://doi.org/10.1561/1500000019">https://doi.org/10.1561/1500000019</a>
</div>
<div id="ref-shapley17ValueNPerson2016" class="csl-entry">
Shapley, L. S. (2016). 17. <span>A Value</span> for n-<span>Person Games</span>. In H. W. Kuhn &amp; A. W. Tucker (Eds.), <em>Contributions to the <span>Theory</span> of <span>Games</span>, <span>Volume II</span></em> (pp. 307â318). Princeton University Press.
</div>
<div id="ref-smockPubTables1MComprehensiveTable2022" class="csl-entry">
Smock, B., Pesala, R., &amp; Abraham, R. (2022). <span>PubTables-1M</span>: <span>Towards</span> comprehensive table extraction from unstructured documents. <em>2022 <span>IEEE</span>/<span>CVF Conference</span> on <span>Computer Vision</span> and <span>Pattern Recognition</span> (<span>CVPR</span>)</em>, 4624â4632. <a href="https://doi.org/10.1109/CVPR52688.2022.00459">https://doi.org/10.1109/CVPR52688.2022.00459</a>
</div>
<div id="ref-tahirUnderstandingLLMContext2025" class="csl-entry">
Tahir. (2025). ð§ <span>Understanding LLM Context Windows</span>: <span>Tokens</span>, <span>Attention</span>, and <span>Challenges</span>. In <em>Medium</em>.
</div>
<div id="ref-vaswaniAttentionAllYou2023" class="csl-entry">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2023). <em>Attention <span>Is All You Need</span></em> (arXiv:1706.03762). arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>
</div>
<div id="ref-wickham40YearsBoxplots2011" class="csl-entry">
Wickham, H., &amp; Stryjewski, L. (2011). <em>40 years of boxplots</em>.
</div>
<div id="ref-willardEfficientGuidedGeneration2023" class="csl-entry">
Willard, B. T., &amp; Louf, R. (2023). <em>Efficient <span>Guided Generation</span> for <span>Large Language Models</span></em> (arXiv:2307.09702). arXiv. <a href="https://doi.org/10.48550/arXiv.2307.09702">https://doi.org/10.48550/arXiv.2307.09702</a>
</div>
<div id="ref-yangFastTreeSHAPAccelerating2022" class="csl-entry">
Yang, J. (2022). <em>Fast <span>TreeSHAP</span>: <span>Accelerating SHAP Value Computation</span> for <span>Trees</span></em> (arXiv:2109.09847). arXiv. <a href="https://doi.org/10.48550/arXiv.2109.09847">https://doi.org/10.48550/arXiv.2109.09847</a>
</div>
<div id="ref-zhangDiveDeepLearning2023" class="csl-entry">
Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2023). <em>Dive into <span>Deep Learning</span></em> (arXiv:2106.11342). arXiv. <a href="https://doi.org/10.48550/arXiv.2106.11342">https://doi.org/10.48550/arXiv.2106.11342</a>
</div>
<div id="ref-zhangMixtureExpertsLarge2025" class="csl-entry">
Zhang, D., Song, J., Bi, Z., Yuan, Y., Wang, T., Yeong, J., &amp; Hao, J. (2025). <em>Mixture of <span>Experts</span> in <span>Large Language Models</span></em> (arXiv:2507.11181). arXiv. <a href="https://doi.org/10.48550/arXiv.2507.11181">https://doi.org/10.48550/arXiv.2507.11181</a>
</div>
<div id="ref-zhaoCalibrateUseImproving2021" class="csl-entry">
Zhao, T. Z., Wallace, E., Feng, S., Klein, D., &amp; Singh, S. (2021). <em>Calibrate <span>Before Use</span>: <span>Improving Few-Shot Performance</span> of <span>Language Models</span></em> (arXiv:2102.09690). arXiv. <a href="https://doi.org/10.48550/arXiv.2102.09690">https://doi.org/10.48550/arXiv.2102.09690</a>
</div>
<div id="ref-zhongPubLayNetLargestDataset2019" class="csl-entry">
Zhong, X., Tang, J., &amp; Yepes, A. J. (2019). <em><span>PubLayNet</span>: Largest dataset ever for document layout analysis</em> (arXiv:1908.07836). arXiv. <a href="https://doi.org/10.48550/arXiv.1908.07836">https://doi.org/10.48550/arXiv.1908.07836</a>
</div>
<div id="ref-zhuLargeLanguageModels2024" class="csl-entry">
Zhu, Y., Yuan, H., Wang, S., Liu, J., Liu, W., Deng, C., Chen, H., Dou, Z., &amp; Wen, J.-R. (2024). <em>Large <span>Language Models</span> for <span>Information Retrieval</span>: <span>A Survey</span></em> (arXiv:2308.07107). arXiv. <a href="https://doi.org/10.48550/arXiv.2308.07107">https://doi.org/10.48550/arXiv.2308.07107</a>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>We replaced <span class="math inline">\(F\)</span> by <span class="math inline">\(P\)</span> to speak in the terms of players instead of features. We also replcaed <span class="math inline">\(f\)</span> by <span class="math inline">\(val\)</span>, because it better fits the story, that this is the value gain in a game, as explained by <span class="citation">OâSullivan (<a href="#ref-osullivanMathematicsShapleyValues2023">2023</a>)</span>.<a href="literature-review.html#fnref1" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>
<script>
// Lightbox functionality for displaying images in a modal view
document.addEventListener("DOMContentLoaded", function () {
  // Create a lightbox container
  const lightbox = document.createElement("div");
  lightbox.id = "lightbox";
  lightbox.style.position = "fixed";
  lightbox.style.top = "0";
  lightbox.style.left = "0";
  lightbox.style.width = "100%";
  lightbox.style.height = "100%";
  lightbox.style.backgroundColor = "rgba(0, 0, 0, 0.8)";
  lightbox.style.display = "none";
  lightbox.style.justifyContent = "center";
  lightbox.style.alignItems = "center";
  lightbox.style.zIndex = "1000";
  document.body.appendChild(lightbox);

  // Add an image element to the lightbox
  const lightboxImage = document.createElement("img");
  lightboxImage.id = "lightbox-image";
  // lightboxImage.style.maxWidth = "90%";
  // lightboxImage.style.maxHeight = "90%";
  lightbox.appendChild(lightboxImage);

  // Close lightbox on click
  lightbox.addEventListener("click", function () {
    lightbox.style.display = "none";
  });

  // Add onclick event to all images except the excluded one
  const images = document.querySelectorAll("img");
  images.forEach((img) => {
    if (img.src.includes("images/BHT_Logo_horizontal_Anthrazit_transparent.svg")) {
      return; // Skip the excluded image
    }
    img.style.cursor = "pointer"; // Change cursor to indicate clickability
    img.addEventListener("click", function () {
      lightboxImage.src = img.src; // Set the lightbox image source
      lightbox.style.display = "flex"; // Show the lightbox
    });
  });
});
</script>

<script>
// Image slider/carousel functionality for divs with class "image-slider"
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll('.image-slider').forEach(function (slider) {
    const images = slider.querySelectorAll('img');
    if (images.length < 2) return; // No need for slider if only one image

    // Hide all images except the first
    images.forEach((img, i) => img.style.display = i === 0 ? 'inline' : 'none');
    let current = 0;

    // Create navigation buttons
    const prevBtn = document.createElement('button');
    prevBtn.textContent = 'â¨ Prev';
    prevBtn.style.marginRight = '10px';
    const nextBtn = document.createElement('button');
    nextBtn.textContent = 'Next â©';
    nextBtn.style.marginLeft = '10px';

    // Create a caption container
    const captionContainer = document.createElement('div');
    captionContainer.style.fontStyle = 'italic';
    captionContainer.style.color = '#555';

    // Create a flex container for buttons and caption
    const controlsContainer = document.createElement('div');
    controlsContainer.style.display = 'flex';
    controlsContainer.style.justifyContent = 'space-between';
    controlsContainer.style.alignItems = 'center';
    controlsContainer.style.marginTop = '10px';
    controlsContainer.style.width = '100%';

    // Place buttons and caption in flex container
    controlsContainer.appendChild(prevBtn);
    controlsContainer.appendChild(captionContainer);
    controlsContainer.appendChild(nextBtn);

    // Function to update the caption
    const updateCaption = () => {
      const caption = images[current].nextElementSibling;
      if (caption && caption.classList.contains('image-caption')) {
        captionContainer.textContent = caption.textContent;
      } else {
        captionContainer.textContent = ''; // Clear caption if none exists
      }
    };

    // Function to show the next image
    const showNextImage = () => {
      images[current].style.display = 'none';
      current = (current + 1) % images.length;
      images[current].style.display = 'inline';
      updateCaption();
    };

    // Function to show the previous image
    const showPrevImage = () => {
      images[current].style.display = 'none';
      current = (current - 1 + images.length) % images.length;
      images[current].style.display = 'inline';
      updateCaption();
    };

    // Button click handlers
    prevBtn.onclick = function () {
      stopAutoSlide();
      showPrevImage();
    };
    nextBtn.onclick = function () {
      stopAutoSlide();
      showNextImage();
    };

    // Insert controls container below images
    slider.appendChild(controlsContainer);

    // Automatic sliding
    let autoSlideInterval = setInterval(showNextImage, 2000);

    // Stop automatic sliding on user interaction
    const stopAutoSlide = () => {
      clearInterval(autoSlideInterval);
    };

    // Stop auto-slide when the user interacts with the slider
    prevBtn.addEventListener('click', stopAutoSlide);
    nextBtn.addEventListener('click', stopAutoSlide);
    images.forEach((img) => {
      img.addEventListener('click', stopAutoSlide);
    });

    // Initialize the caption
    updateCaption();
  });
});
</script>

<script>
// Dropdown filter functionality for images with lightbox
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll('.image-selector').forEach(function (selector) {
    const images = selector.querySelectorAll('img');
    if (images.length === 0) return;

    // Create a dropdown filter container
    const filterContainer = document.createElement('div');
    filterContainer.classList.add('filter-container');

    const dropdown = document.createElement('select');
    dropdown.multiple = true;
    dropdown.classList.add('filter-dropdown');

    // Add options to the dropdown
    const uniqueCategories = [];
    images.forEach((img) => {
      const caption = img.nextElementSibling;
      if (caption && caption.classList.contains('image-caption')) {
        const category = caption.textContent.trim();
        if (!uniqueCategories.includes(category)) uniqueCategories.push(category);
      }
    });

    uniqueCategories.forEach((category) => {
      const option = document.createElement('option');
      option.value = category;
      option.textContent = category;
      // Do NOT set option.selected here!
      dropdown.appendChild(option);
    });

    filterContainer.appendChild(dropdown);
    selector.appendChild(filterContainer);

    // Add a grid container for filtered images
    const gridContainer = document.createElement('div');
    gridContainer.classList.add('image-grid');
    selector.appendChild(gridContainer);

    // Choices.js initialization
    const choices = new Choices(dropdown, {
      removeItemButton: true,
      shouldSort: false,
      searchEnabled: false,
      placeholder: true,
      placeholderValue: 'Filter categories',
    });

    // Clear any existing selection first, then set initial selection
    choices.removeActiveItems(); // This clears existing selections
    choices.setChoiceByValue(uniqueCategories[0]); // This sets only the first category

    // Filtering function
    const filterImages = () => {
      const selectedCategories = Array.from(dropdown.selectedOptions).map(opt => opt.value);
      gridContainer.innerHTML = '';
      let shownWrappers = [];
      images.forEach((img) => {
        const caption = img.nextElementSibling;
        if (caption && caption.classList.contains('image-caption')) {
          const category = caption.textContent.trim();
          if (selectedCategories.includes(category)) {
            const imgWrapper = document.createElement('div');
            imgWrapper.classList.add('image-wrapper');
            const imgClone = img.cloneNode(true);
            imgClone.style.cursor = 'pointer';
            imgClone.addEventListener('click', function () {
              const lightboxImage = document.querySelector("#lightbox img");
              const lightbox = document.querySelector("#lightbox");
              lightboxImage.src = imgClone.src;
              lightbox.style.display = 'flex';
            });
            imgWrapper.appendChild(imgClone);
            const captionClone = caption.cloneNode(true);
            imgWrapper.appendChild(captionClone);
            gridContainer.appendChild(imgWrapper);
            shownWrappers.push(imgWrapper);
          }
        }
      });
      // If only one image is shown, make it span both columns
      if (shownWrappers.length === 1) {
        shownWrappers[0].style.gridColumn = "span 2";
      }
    };

    // Listen for changes from Choices.js
    dropdown.addEventListener('change', filterImages);

    // Initial grid (only first category shown)
    filterImages();
  });
});
</script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  var codes = document.querySelectorAll('.hideme');
  var code, i, d, s, p;
  for (i = 0; i < codes.length; i++) {
    code = codes[i];
    p = code.parentNode;
    d = document.createElement('details');
    s = document.createElement('summary');
    s.innerText = 'Details';
    // <details><summary>Details</summary></details>
    d.appendChild(s);
    // move the code into <details>
    p.replaceChild(d, code);
    d.appendChild(code);
  }
});
</script>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methodology.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
