apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-deployment-openai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-deployment-openai
  template:
    metadata:
      labels:
        app: vllm-deployment-openai
    spec:
      containers:
      - name: vllm-deployment-openai
        image: vllm/vllm-openai:latest  # Use the official vLLM Docker image
        ports:
        - containerPort: 8000  # vLLM exposes the API on port 8000
        command: ["/bin/sh", "-c"]
        args: [
          # "vllm serve Qwen/Qwen2.5-7B --port 8000 --host 0.0.0.0 --tensor-parallel-size 1 --api-key sk-1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef --dtype auto"
          "vllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 4  --port 8000 --host 0.0.0.0 --api-key sk-1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef --dtype auto"
        ]
        # args: [
        #   "vllm serve Qwen/Qwen2.5-7B --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024 --port 8000 --host 0.0.0.0"
        # ]
        # command: ["python3", "-m", "vllm.entrypoints.api_server"] # mal ausprobieren statt command
        # args:
        #   - --model
        #   - "Qwen/Qwen2.5-7B"  # Specify the model name, e.g., "gpt2"
        #   - --host
        #   - 0.0.0.0
        #   - --port
        #   - "8000"
        #   - --tensor-parallel-size
        #   - "1"
        resources:
          requests:
              cpu: "2"
              memory: "64Gi"
              nvidia.com/gpu: "4"
          limits:
            nvidia.com/gpu: "4"
            memory: 128Gi
            cpu: "32"
        env:
          - name: LANG
            value: 'C.UTF-8'
          - name: HF_HOME
            value: "/pvc/hub_cache"
          - name: OMP_NUM_THREADS
            value: "1"
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: token
        volumeMounts:
          - name: vllm-pvc
            mountPath: /pvc
          - name: dshm
            mountPath: /dev/shm
      nodeSelector:
        # kubernetes.io/hostname: cl-worker29
        gpu: a100
      imagePullSecrets:
        - name: private-registry-auth
      volumes:
        - name: vllm-pvc
          persistentVolumeClaim:
            claimName: vllm-pvc
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
            
